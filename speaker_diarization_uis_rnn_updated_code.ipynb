{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sulimankhan587/speaker-diarization/blob/main/speaker_diarization_uis_rnn_updated_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "guhScHfwcN_b",
        "outputId": "cc521ba0-36ec-43ad-f218-1dd95b5db525"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c08783e8-5321-407a-8d2f-90a5373f5f15\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c08783e8-5321-407a-8d2f-90a5373f5f15\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"khsulemani\",\"key\":\"18955b5dbcea35546f1e6a1760b5e9ce\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Run this cell and select the kaggle.json file downloaded\n",
        "# from the Kaggle account settings page.\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "# this is my kaggle .json you can upload your kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTRET41LcaQs",
        "outputId": "9ce4de22-7da4-40ca-836c-6aae2fc41d54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# this cell is important to execute as the toy dataset in your drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vXFIL_lcX5E",
        "outputId": "8115256c-b9ce-49e7-a58c-7be5d956b7b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 66 Dec 21 06:24 kaggle.json\n"
          ]
        }
      ],
      "source": [
        "# Let's make sure the kaggle.json file is present.\n",
        "!ls -lha kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ci7T91r7cYB-",
        "outputId": "5e9cbb45-ac50-4f66-e200-f2fa08047e24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ref                                                             title                                                size  lastUpdated          downloadCount  voteCount  usabilityRating  \n",
            "--------------------------------------------------------------  --------------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n",
            "thedrcat/daigt-v2-train-dataset                                 DAIGT V2 Train Dataset                               29MB  2023-11-16 01:38:36           2266        212  1.0              \n",
            "muhammadbinimran/housing-price-prediction-data                  Housing Price Prediction Data                       763KB  2023-11-21 17:56:32          10488        175  1.0              \n",
            "thedrcat/daigt-proper-train-dataset                             DAIGT Proper Train Dataset                          119MB  2023-11-05 14:03:25           1983        159  1.0              \n",
            "thedevastator/netflix-imdb-scores                               Netflix IMDB Scores                                 699KB  2023-12-03 14:10:34           4433         81  1.0              \n",
            "rish59/financial-statements-of-major-companies2009-2023         Financial Statements of Major Companies(2009-2023)   11KB  2023-11-30 07:01:09           1049         30  0.88235295       \n",
            "henryshan/starbucks                                             Starbucks                                             5KB  2023-12-06 03:07:49           3054         67  1.0              \n",
            "nani123456789/taxi-trip-fare-prediction                         Taxi Trip Fare Prediction                             3MB  2023-12-15 16:46:09            508         26  1.0              \n",
            "jocelyndumlao/cardiovascular-disease-dataset                    Cardiovascular_Disease_Dataset                      411KB  2023-12-09 06:51:28           1893         65  1.0              \n",
            "everydaycodings/produce-prices-dataset                          Fruits and Vegetables Prices Dataset                232KB  2023-12-11 13:40:33           2044         40  1.0              \n",
            "thedevastator/global-shark-attack-incidents                     Global Shark Attack Incidents                       593KB  2023-12-04 21:30:45            742         23  1.0              \n",
            "thedrcat/daigt-external-train-dataset                           DAIGT External Train Dataset                        435MB  2023-11-06 17:10:37            340         45  1.0              \n",
            "thedevastator/hate-speech-and-offensive-language-detection      Hate Speech and Offensive Language Detection        963KB  2023-12-02 11:38:15           1005         24  1.0              \n",
            "jacksondivakarr/student-classification-dataset                  Student Classification Dataset                       15KB  2023-12-02 16:23:43           2839         47  1.0              \n",
            "rafsunahmad/world-polluted-country-report                       World Polluted Country Report                         5KB  2023-12-18 13:35:28            793         24  1.0              \n",
            "dansbecker/melbourne-housing-snapshot                           Melbourne Housing Snapshot                          451KB  2018-06-05 12:52:24         133565       1403  0.7058824        \n",
            "carlmcbrideellis/llm-7-prompt-training-dataset                  LLM: 7 prompt training dataset                       41MB  2023-11-15 07:32:56           1789        134  1.0              \n",
            "thedevastator/spotify-tracks-genre-dataset                      Spotify Tracks Genre                                  8MB  2023-11-30 04:25:48           3038         72  1.0              \n",
            "gabrielluizone/high-school-alcoholism-and-academic-performance  High School Alcoholism and Academic Performance     207KB  2023-12-02 17:56:36           1937         45  1.0              \n",
            "nelgiriyewithana/australian-vehicle-prices                      Australian Vehicle Prices                           582KB  2023-11-27 04:51:30           2321         72  1.0              \n",
            "joebeachcapital/30000-spotify-songs                             30000 Spotify Songs                                   3MB  2023-11-01 06:06:43          12807        256  1.0              \n"
          ]
        }
      ],
      "source": [
        "# Next, install the Kaggle API client.\n",
        "!pip install -q kaggle\n",
        "# The Kaggle API client expects this file to be in ~/.kaggle,\n",
        "# so move it there.\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "# This permissions change avoids a warning on Kaggle tool startup.\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "# List available datasets.\n",
        "!kaggle datasets list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_ijJyG4cYFV",
        "outputId": "87828f55-f006-4e4a-ede0-1eba7cfec92e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "  inflating: data/TRAIN/DR6/MTXS0/SI1060.WRD  \n",
            "  inflating: data/TRAIN/DR6/MTXS0/SI1690.PHN  \n",
            "  inflating: data/TRAIN/DR6/MTXS0/SI1690.TXT  \n",
            "  inflating: data/TRAIN/DR6/MTXS0/SI1690.WAV  \n",
            "  inflating: data/TRAIN/DR6/MTXS0/SI1690.WAV.wav  \n",
            "  inflating: data/TRAIN/DR6/MTXS0/SI1690.WRD  \n",
            "  inflating: data/TRAIN/DR6/MTXS0/SI2320.PHN  \n",
            "  inflating: data/TRAIN/DR6/MTXS0/SI2320.TXT  \n",
            "  inflating: data/TRAIN/DR6/MTXS0/SI2320.WAV  \n",
            "  inflating: data/TRAIN/DR6/MTXS0/SI2320.WAV.wav  \n",
            "  inflating: data/TRAIN/DR6/MTXS0/SI2320.WRD  \n",
            "  inflating: data/TRAIN/DR6/MTXS0/SX160.PHN  \n",
            "  inflating: data/TRAIN/DR6/MTXS0/SX160.TXT  \n",
            "  inflating: data/TRAIN/DR6/MTXS0/SX160.WAV  \n",
            "  inflating: data/TRAIN/DR6/MTXS0/SX160.WAV.wav  \n",
            "  inflating: data/TRAIN/DR6/MTXS0/SX160.WRD  \n",
            "  inflating: data/TRAIN/DR6/MTXS0/SX250.PHN  \n",
            "  inflating: data/TRAIN/DR6/MTXS0/SX250.TXT  \n",
            "  inflating: data/TRAIN/DR6/MTXS0/SX250.WAV  \n",
            "  inflating: data/TRAIN/DR6/MTXS0/SX250.WAV.wav  \n",
            "  inflating: data/TRAIN/DR6/MTXS0/SX250.WRD  \n",
            "  inflating: data/TRAIN/DR6/MTXS0/SX340.PHN  \n",
            "  inflating: data/TRAIN/DR6/MTXS0/SX340.TXT  \n",
            "  inflating: data/TRAIN/DR6/MTXS0/SX340.WAV  \n",
            "  inflating: data/TRAIN/DR6/MTXS0/SX340.WAV.wav  \n",
            "  inflating: data/TRAIN/DR6/MTXS0/SX340.WRD  \n",
            "  inflating: data/TRAIN/DR6/MTXS0/SX430.PHN  \n",
            "  inflating: data/TRAIN/DR6/MTXS0/SX430.TXT  \n",
            "  inflating: data/TRAIN/DR6/MTXS0/SX430.WAV  \n",
            "  inflating: data/TRAIN/DR6/MTXS0/SX430.WAV.wav  \n",
            "  inflating: data/TRAIN/DR6/MTXS0/SX430.WRD  \n",
            "  inflating: data/TRAIN/DR6/MTXS0/SX70.PHN  \n",
            "  inflating: data/TRAIN/DR6/MTXS0/SX70.TXT  \n",
            "  inflating: data/TRAIN/DR6/MTXS0/SX70.WAV  \n",
            "  inflating: data/TRAIN/DR6/MTXS0/SX70.WAV.wav  \n",
            "  inflating: data/TRAIN/DR6/MTXS0/SX70.WRD  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SI1058.PHN  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SI1058.TXT  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SI1058.WAV  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SI1058.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SI1058.WRD  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SI1688.PHN  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SI1688.TXT  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SI1688.WAV  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SI1688.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SI1688.WRD  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SI2318.PHN  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SI2318.TXT  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SI2318.WAV  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SI2318.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SI2318.WRD  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SX158.PHN  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SX158.TXT  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SX158.WAV  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SX158.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SX158.WRD  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SX248.PHN  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SX248.TXT  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SX248.WAV  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SX248.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SX248.WRD  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SX338.PHN  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SX338.TXT  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SX338.WAV  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SX338.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SX338.WRD  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SX428.PHN  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SX428.TXT  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SX428.WAV  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SX428.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SX428.WRD  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SX68.PHN  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SX68.TXT  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SX68.WAV  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SX68.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FBLV0/SX68.WRD  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SI1607.PHN  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SI1607.TXT  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SI1607.WAV  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SI1607.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SI1607.WRD  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SI2237.PHN  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SI2237.TXT  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SI2237.WAV  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SI2237.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SI2237.WRD  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SI977.PHN  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SI977.TXT  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SI977.WAV  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SI977.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SI977.WRD  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SX167.PHN  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SX167.TXT  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SX167.WAV  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SX167.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SX167.WRD  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SX257.PHN  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SX257.TXT  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SX257.WAV  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SX257.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SX257.WRD  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SX347.PHN  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SX347.TXT  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SX347.WAV  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SX347.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SX347.WRD  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SX437.PHN  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SX437.TXT  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SX437.WAV  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SX437.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SX437.WRD  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SX77.PHN  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SX77.TXT  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SX77.WAV  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SX77.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FCJS0/SX77.WRD  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SI1913.PHN  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SI1913.TXT  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SI1913.WAV  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SI1913.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SI1913.WRD  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SI2053.PHN  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SI2053.TXT  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SI2053.WAV  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SI2053.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SI2053.WRD  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SI793.PHN  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SI793.TXT  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SI793.WAV  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SI793.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SI793.WRD  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SX163.PHN  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SX163.TXT  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SX163.WAV  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SX163.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SX163.WRD  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SX253.PHN  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SX253.TXT  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SX253.WAV  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SX253.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SX253.WRD  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SX343.PHN  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SX343.TXT  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SX343.WAV  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SX343.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SX343.WRD  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SX433.PHN  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SX433.TXT  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SX433.WAV  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SX433.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SX433.WRD  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SX73.PHN  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SX73.TXT  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SX73.WAV  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SX73.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FCRZ0/SX73.WRD  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SI1047.PHN  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SI1047.TXT  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SI1047.WAV  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SI1047.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SI1047.WRD  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SI1677.PHN  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SI1677.TXT  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SI1677.WAV  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SI1677.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SI1677.WRD  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SI2307.PHN  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SI2307.TXT  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SI2307.WAV  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SI2307.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SI2307.WRD  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SX147.PHN  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SX147.TXT  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SX147.WAV  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SX147.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SX147.WRD  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SX237.PHN  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SX237.TXT  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SX237.WAV  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SX237.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SX237.WRD  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SX327.PHN  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SX327.TXT  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SX327.WAV  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SX327.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SX327.WRD  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SX417.PHN  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SX417.TXT  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SX417.WAV  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SX417.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SX417.WRD  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SX57.PHN  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SX57.TXT  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SX57.WAV  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SX57.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FJEN0/SX57.WRD  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SI1022.PHN  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SI1022.TXT  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SI1022.WAV  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SI1022.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SI1022.WRD  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SI1652.PHN  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SI1652.TXT  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SI1652.WAV  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SI1652.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SI1652.WRD  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SI2282.PHN  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SI2282.TXT  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SI2282.WAV  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SI2282.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SI2282.WRD  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SX122.PHN  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SX122.TXT  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SX122.WAV  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SX122.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SX122.WRD  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SX212.PHN  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SX212.TXT  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SX212.WAV  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SX212.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SX212.WRD  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SX302.PHN  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SX302.TXT  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SX302.WAV  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SX302.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SX302.WRD  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SX32.PHN  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SX32.TXT  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SX32.WAV  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SX32.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SX32.WRD  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SX392.PHN  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SX392.TXT  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SX392.WAV  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SX392.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FJHK0/SX392.WRD  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SI1432.PHN  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SI1432.TXT  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SI1432.WAV  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SI1432.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SI1432.WRD  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SI2062.PHN  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SI2062.TXT  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SI2062.WAV  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SI2062.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SI2062.WRD  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SI802.PHN  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SI802.TXT  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SI802.WAV  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SI802.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SI802.WRD  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SX172.PHN  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SX172.TXT  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SX172.WAV  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SX172.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SX172.WRD  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SX262.PHN  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SX262.TXT  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SX262.WAV  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SX262.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SX262.WRD  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SX352.PHN  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SX352.TXT  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SX352.WAV  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SX352.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SX352.WRD  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SX442.PHN  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SX442.TXT  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SX442.WAV  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SX442.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SX442.WRD  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SX82.PHN  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SX82.TXT  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SX82.WAV  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SX82.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FJRP1/SX82.WRD  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SI1052.PHN  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SI1052.TXT  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SI1052.WAV  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SI1052.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SI1052.WRD  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SI1682.PHN  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SI1682.TXT  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SI1682.WAV  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SI1682.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SI1682.WRD  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SI2312.PHN  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SI2312.TXT  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SI2312.WAV  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SI2312.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SI2312.WRD  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SX152.PHN  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SX152.TXT  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SX152.WAV  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SX152.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SX152.WRD  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SX242.PHN  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SX242.TXT  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SX242.WAV  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SX242.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SX242.WRD  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SX332.PHN  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SX332.TXT  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SX332.WAV  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SX332.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SX332.WRD  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SX422.PHN  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SX422.TXT  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SX422.WAV  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SX422.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SX422.WRD  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SX62.PHN  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SX62.TXT  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SX62.WAV  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SX62.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FJSK0/SX62.WRD  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SI1141.PHN  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SI1141.TXT  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SI1141.WAV  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SI1141.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SI1141.WRD  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SI1771.PHN  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SI1771.TXT  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SI1771.WAV  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SI1771.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SI1771.WRD  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SI2221.PHN  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SI2221.TXT  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SI2221.WAV  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SI2221.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SI2221.WRD  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SX151.PHN  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SX151.TXT  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SX151.WAV  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SX151.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SX151.WRD  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SX241.PHN  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SX241.TXT  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SX241.WAV  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SX241.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SX241.WRD  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SX331.PHN  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SX331.TXT  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SX331.WAV  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SX331.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SX331.WRD  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SX421.PHN  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SX421.TXT  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SX421.WAV  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SX421.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SX421.WRD  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SX61.PHN  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SX61.TXT  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SX61.WAV  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SX61.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FKDE0/SX61.WRD  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SI1117.PHN  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SI1117.TXT  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SI1117.WAV  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SI1117.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SI1117.WRD  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SI1747.PHN  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SI1747.TXT  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SI1747.WAV  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SI1747.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SI1747.WRD  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SI487.PHN  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SI487.TXT  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SI487.WAV  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SI487.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SI487.WRD  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SX161.PHN  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SX161.TXT  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SX161.WAV  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SX161.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SX161.WRD  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SX217.PHN  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SX217.TXT  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SX217.WAV  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SX217.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SX217.WRD  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SX366.PHN  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SX366.TXT  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SX366.WAV  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SX366.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SX366.WRD  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SX37.PHN  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SX37.TXT  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SX37.WAV  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SX37.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SX37.WRD  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SX397.PHN  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SX397.TXT  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SX397.WAV  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SX397.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FKSR0/SX397.WRD  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SI1051.PHN  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SI1051.TXT  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SI1051.WAV  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SI1051.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SI1051.WRD  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SI1681.PHN  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SI1681.TXT  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SI1681.WAV  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SI1681.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SI1681.WRD  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SI2311.PHN  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SI2311.TXT  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SI2311.WAV  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SI2311.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SI2311.WRD  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SX151.PHN  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SX151.TXT  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SX151.WAV  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SX151.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SX151.WRD  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SX241.PHN  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SX241.TXT  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SX241.WAV  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SX241.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SX241.WRD  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SX331.PHN  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SX331.TXT  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SX331.WAV  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SX331.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SX331.WRD  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SX421.PHN  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SX421.TXT  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SX421.WAV  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SX421.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SX421.WRD  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SX61.PHN  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SX61.TXT  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SX61.WAV  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SX61.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FLEH0/SX61.WRD  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SI1137.PHN  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SI1137.TXT  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SI1137.WAV  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SI1137.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SI1137.WRD  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SI1767.PHN  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SI1767.TXT  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SI1767.WAV  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SI1767.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SI1767.WRD  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SI507.PHN  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SI507.TXT  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SI507.WAV  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SI507.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SI507.WRD  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SX147.PHN  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SX147.TXT  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SX147.WAV  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SX147.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SX147.WRD  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SX237.PHN  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SX237.TXT  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SX237.WAV  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SX237.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SX237.WRD  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SX277.PHN  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SX277.TXT  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SX277.WAV  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SX277.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SX277.WRD  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SX417.PHN  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SX417.TXT  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SX417.WAV  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SX417.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SX417.WRD  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SX57.PHN  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SX57.TXT  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SX57.WAV  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SX57.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FLET0/SX57.WRD  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SI1509.PHN  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SI1509.TXT  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SI1509.WAV  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SI1509.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SI1509.WRD  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SI2139.PHN  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SI2139.TXT  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SI2139.WAV  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SI2139.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SI2139.WRD  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SI879.PHN  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SI879.TXT  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SI879.WAV  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SI879.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SI879.WRD  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SX159.PHN  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SX159.TXT  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SX159.WAV  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SX159.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SX159.WRD  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SX249.PHN  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SX249.TXT  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SX249.WAV  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SX249.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SX249.WRD  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SX339.PHN  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SX339.TXT  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SX339.WAV  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SX339.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SX339.WRD  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SX429.PHN  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SX429.TXT  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SX429.WAV  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SX429.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SX429.WRD  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SX69.PHN  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SX69.TXT  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SX69.WAV  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SX69.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FMAH1/SX69.WRD  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SI1041.PHN  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SI1041.TXT  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SI1041.WAV  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SI1041.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SI1041.WRD  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SI1072.PHN  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SI1072.TXT  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SI1072.WAV  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SI1072.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SI1072.WRD  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SI1702.PHN  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SI1702.TXT  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SI1702.WAV  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SI1702.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SI1702.WRD  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SX172.PHN  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SX172.TXT  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SX172.WAV  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SX172.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SX172.WRD  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SX262.PHN  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SX262.TXT  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SX262.WAV  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SX262.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SX262.WRD  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SX352.PHN  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SX352.TXT  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SX352.WAV  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SX352.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SX352.WRD  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SX442.PHN  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SX442.TXT  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SX442.WAV  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SX442.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SX442.WRD  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SX82.PHN  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SX82.TXT  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SX82.WAV  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SX82.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FMKC0/SX82.WRD  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SI1471.PHN  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SI1471.TXT  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SI1471.WAV  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SI1471.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SI1471.WRD  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SI2101.PHN  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SI2101.TXT  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SI2101.WAV  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SI2101.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SI2101.WRD  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SI841.PHN  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SI841.TXT  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SI841.WAV  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SI841.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SI841.WRD  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SX121.PHN  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SX121.TXT  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SX121.WAV  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SX121.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SX121.WRD  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SX211.PHN  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SX211.TXT  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SX211.WAV  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SX211.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SX211.WRD  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SX301.PHN  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SX301.TXT  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SX301.WAV  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SX301.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SX301.WRD  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SX31.PHN  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SX31.TXT  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SX31.WAV  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SX31.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SX31.WRD  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SX391.PHN  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SX391.TXT  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SX391.WAV  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SX391.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FPAB1/SX391.WRD  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SI1921.PHN  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SI1921.TXT  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SI1921.WAV  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SI1921.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SI1921.WRD  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SI2011.PHN  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SI2011.TXT  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SI2011.WAV  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SI2011.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SI2011.WRD  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SI661.PHN  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SI661.TXT  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SI661.WAV  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SI661.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SI661.WRD  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SX121.PHN  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SX121.TXT  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SX121.WAV  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SX121.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SX121.WRD  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SX211.PHN  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SX211.TXT  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SX211.WAV  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SX211.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SX211.WRD  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SX301.PHN  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SX301.TXT  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SX301.WAV  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SX301.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SX301.WRD  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SX31.PHN  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SX31.TXT  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SX31.WAV  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SX31.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SX31.WRD  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SX391.PHN  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SX391.TXT  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SX391.WAV  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SX391.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FPAC0/SX391.WRD  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SI1315.PHN  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SI1315.TXT  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SI1315.WAV  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SI1315.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SI1315.WRD  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SI1945.PHN  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SI1945.TXT  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SI1945.WAV  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SI1945.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SI1945.WRD  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SI685.PHN  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SI685.TXT  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SI685.WAV  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SI685.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SI685.WRD  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SX145.PHN  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SX145.TXT  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SX145.WAV  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SX145.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SX145.WRD  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SX235.PHN  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SX235.TXT  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SX235.WAV  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SX235.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SX235.WRD  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SX325.PHN  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SX325.TXT  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SX325.WAV  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SX325.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SX325.WRD  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SX415.PHN  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SX415.TXT  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SX415.WAV  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SX415.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SX415.WRD  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SX55.PHN  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SX55.TXT  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SX55.WAV  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SX55.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FREH0/SX55.WRD  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SI1241.PHN  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SI1241.TXT  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SI1241.WAV  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SI1241.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SI1241.WRD  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SI1871.PHN  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SI1871.TXT  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SI1871.WAV  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SI1871.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SI1871.WRD  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SI611.PHN  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SI611.TXT  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SI611.WAV  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SI611.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SI611.WRD  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SX161.PHN  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SX161.TXT  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SX161.WAV  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SX161.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SX161.WRD  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SX251.PHN  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SX251.TXT  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SX251.WAV  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SX251.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SX251.WRD  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SX341.PHN  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SX341.TXT  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SX341.WAV  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SX341.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SX341.WRD  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SX431.PHN  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SX431.TXT  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SX431.WAV  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SX431.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SX431.WRD  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SX71.PHN  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SX71.TXT  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SX71.WAV  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SX71.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FSPM0/SX71.WRD  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SI1159.PHN  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SI1159.TXT  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SI1159.WAV  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SI1159.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SI1159.WRD  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SI1789.PHN  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SI1789.TXT  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SI1789.WAV  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SI1789.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SI1789.WRD  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SI529.PHN  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SI529.TXT  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SI529.WAV  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SI529.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SI529.WRD  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SX169.PHN  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SX169.TXT  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SX169.WAV  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SX169.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SX169.WRD  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SX259.PHN  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SX259.TXT  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SX259.WAV  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SX259.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SX259.WRD  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SX349.PHN  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SX349.TXT  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SX349.WAV  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SX349.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SX349.WRD  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SX439.PHN  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SX439.TXT  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SX439.WAV  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SX439.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SX439.WRD  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SX79.PHN  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SX79.TXT  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SX79.WAV  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SX79.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/FVKB0/SX79.WRD  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SI1295.PHN  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SI1295.TXT  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SI1295.WAV  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SI1295.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SI1295.WRD  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SI1798.PHN  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SI1798.TXT  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SI1798.WAV  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SI1798.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SI1798.WRD  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SI538.PHN  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SI538.TXT  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SI538.WAV  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SI538.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SI538.WRD  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SX178.PHN  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SX178.TXT  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SX178.WAV  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SX178.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SX178.WRD  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SX268.PHN  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SX268.TXT  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SX268.WAV  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SX268.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SX268.WRD  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SX358.PHN  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SX358.TXT  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SX358.WAV  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SX358.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SX358.WRD  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SX448.PHN  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SX448.TXT  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SX448.WAV  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SX448.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SX448.WRD  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SX88.PHN  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SX88.TXT  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SX88.WAV  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SX88.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MADD0/SX88.WRD  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SI1326.PHN  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SI1326.TXT  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SI1326.WAV  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SI1326.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SI1326.WRD  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SI1655.PHN  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SI1655.TXT  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SI1655.WAV  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SI1655.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SI1655.WRD  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SI1956.PHN  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SI1956.TXT  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SI1956.WAV  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SI1956.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SI1956.WRD  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SX156.PHN  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SX156.TXT  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SX156.WAV  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SX156.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SX156.WRD  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SX246.PHN  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SX246.TXT  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SX246.WAV  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SX246.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SX246.WRD  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SX336.PHN  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SX336.TXT  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SX336.WAV  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SX336.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SX336.WRD  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SX426.PHN  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SX426.TXT  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SX426.WAV  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SX426.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SX426.WRD  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SX66.PHN  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SX66.TXT  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SX66.WAV  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SX66.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MAEO0/SX66.WRD  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SI1569.PHN  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SI1569.TXT  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SI1569.WAV  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SI1569.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SI1569.WRD  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SI2199.PHN  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SI2199.TXT  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SI2199.WAV  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SI2199.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SI2199.WRD  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SI939.PHN  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SI939.TXT  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SI939.WAV  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SI939.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SI939.WRD  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SX129.PHN  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SX129.TXT  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SX129.WAV  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SX129.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SX129.WRD  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SX219.PHN  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SX219.TXT  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SX219.WAV  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SX219.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SX219.WRD  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SX309.PHN  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SX309.TXT  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SX309.WAV  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SX309.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SX309.WRD  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SX39.PHN  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SX39.TXT  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SX39.WAV  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SX39.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SX39.WRD  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SX399.PHN  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SX399.TXT  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SX399.WAV  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SX399.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MAFM0/SX399.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SI1319.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SI1319.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SI1319.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SI1319.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SI1319.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SI1949.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SI1949.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SI1949.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SI1949.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SI1949.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SI689.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SI689.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SI689.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SI689.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SI689.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SX149.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SX149.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SX149.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SX149.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SX149.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SX239.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SX239.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SX239.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SX239.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SX239.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SX329.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SX329.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SX329.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SX329.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SX329.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SX419.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SX419.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SX419.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SX419.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SX419.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SX59.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SX59.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SX59.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SX59.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBAR0/SX59.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SI1055.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SI1055.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SI1055.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SI1055.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SI1055.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SI1685.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SI1685.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SI1685.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SI1685.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SI1685.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SI2315.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SI2315.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SI2315.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SI2315.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SI2315.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SX155.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SX155.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SX155.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SX155.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SX155.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SX245.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SX245.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SX245.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SX245.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SX245.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SX335.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SX335.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SX335.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SX335.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SX335.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SX425.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SX425.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SX425.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SX425.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SX425.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SX65.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SX65.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SX65.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SX65.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBBR0/SX65.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SI1169.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SI1169.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SI1169.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SI1169.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SI1169.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SI1799.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SI1799.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SI1799.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SI1799.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SI1799.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SI539.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SI539.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SI539.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SI539.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SI539.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SX179.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SX179.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SX179.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SX179.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SX179.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SX269.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SX269.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SX269.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SX269.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SX269.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SX359.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SX359.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SX359.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SX359.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SX359.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SX449.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SX449.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SX449.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SX449.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SX449.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SX89.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SX89.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SX89.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SX89.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBML0/SX89.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SI1014.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SI1014.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SI1014.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SI1014.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SI1014.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SI1644.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SI1644.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SI1644.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SI1644.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SI1644.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SI2274.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SI2274.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SI2274.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SI2274.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SI2274.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SX114.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SX114.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SX114.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SX114.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SX114.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SX204.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SX204.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SX204.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SX204.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SX204.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SX294.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SX294.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SX294.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SX294.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SX294.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SX311.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SX311.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SX311.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SX311.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SX311.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SX384.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SX384.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SX384.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SX384.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBOM0/SX384.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SI2102.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SI2102.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SI2102.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SI2102.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SI2102.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SI505.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SI505.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SI505.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SI505.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SI505.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SI757.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SI757.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SI757.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SI757.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SI757.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SX122.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SX122.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SX122.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SX122.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SX122.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SX212.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SX212.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SX212.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SX212.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SX212.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SX302.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SX302.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SX302.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SX302.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SX302.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SX32.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SX32.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SX32.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SX32.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SX32.WRD  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SX392.PHN  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SX392.TXT  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SX392.WAV  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SX392.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MBTH0/SX392.WRD  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SI1660.PHN  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SI1660.TXT  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SI1660.WAV  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SI1660.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SI1660.WRD  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SI2290.PHN  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SI2290.TXT  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SI2290.WAV  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SI2290.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SI2290.WRD  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SI650.PHN  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SI650.TXT  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SI650.WAV  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SI650.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SI650.WRD  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SX130.PHN  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SX130.TXT  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SX130.WAV  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SX130.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SX130.WRD  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SX220.PHN  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SX220.TXT  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SX220.WAV  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SX220.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SX220.WRD  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SX310.PHN  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SX310.TXT  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SX310.WAV  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SX310.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SX310.WRD  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SX40.PHN  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SX40.TXT  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SX40.WAV  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SX40.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SX40.WRD  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SX400.PHN  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SX400.TXT  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SX400.WAV  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SX400.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MCLK0/SX400.WRD  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SI1121.PHN  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SI1121.TXT  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SI1121.WAV  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SI1121.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SI1121.WRD  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SI1725.PHN  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SI1725.TXT  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SI1725.WAV  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SI1725.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SI1725.WRD  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SI1751.PHN  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SI1751.TXT  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SI1751.WAV  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SI1751.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SI1751.WRD  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SX131.PHN  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SX131.TXT  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SX131.WAV  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SX131.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SX131.WRD  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SX221.PHN  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SX221.TXT  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SX221.WAV  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SX221.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SX221.WRD  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SX24.PHN  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SX24.TXT  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SX24.WAV  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SX24.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SX24.WRD  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SX401.PHN  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SX401.TXT  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SX401.WAV  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SX401.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SX401.WRD  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SX41.PHN  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SX41.TXT  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SX41.WAV  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SX41.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MCRE0/SX41.WRD  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SI1209.PHN  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SI1209.TXT  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SI1209.WAV  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SI1209.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SI1209.WRD  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SI1839.PHN  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SI1839.TXT  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SI1839.WAV  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SI1839.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SI1839.WRD  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SI579.PHN  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SI579.TXT  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SI579.WAV  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SI579.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SI579.WRD  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SX129.PHN  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SX129.TXT  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SX129.WAV  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SX129.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SX129.WRD  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SX219.PHN  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SX219.TXT  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SX219.WAV  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SX219.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SX219.WRD  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SX309.PHN  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SX309.TXT  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SX309.WAV  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SX309.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SX309.WRD  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SX39.PHN  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SX39.TXT  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SX39.WAV  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SX39.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SX39.WRD  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SX399.PHN  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SX399.TXT  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SX399.WAV  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SX399.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MCTH0/SX399.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SI1480.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SI1480.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SI1480.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SI1480.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SI1480.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SI2110.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SI2110.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SI2110.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SI2110.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SI2110.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SI850.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SI850.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SI850.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SI850.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SI850.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SX130.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SX130.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SX130.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SX130.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SX130.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SX220.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SX220.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SX220.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SX220.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SX220.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SX310.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SX310.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SX310.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SX310.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SX310.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SX40.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SX40.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SX40.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SX40.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SX40.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SX400.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SX400.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SX400.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SX400.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDCM0/SX400.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SI1170.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SI1170.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SI1170.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SI1170.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SI1170.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SI1800.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SI1800.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SI1800.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SI1800.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SI1800.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SI540.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SI540.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SI540.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SI540.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SI540.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SX180.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SX180.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SX180.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SX180.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SX180.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SX270.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SX270.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SX270.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SX270.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SX270.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SX360.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SX360.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SX360.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SX360.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SX360.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SX450.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SX450.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SX450.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SX450.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SX450.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SX90.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SX90.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SX90.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SX90.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDED0/SX90.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SI1066.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SI1066.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SI1066.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SI1066.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SI1066.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SI1696.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SI1696.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SI1696.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SI1696.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SI1696.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SI2326.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SI2326.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SI2326.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SI2326.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SI2326.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SX166.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SX166.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SX166.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SX166.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SX166.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SX256.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SX256.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SX256.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SX256.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SX256.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SX346.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SX346.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SX346.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SX346.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SX346.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SX436.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SX436.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SX436.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SX436.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SX436.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SX76.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SX76.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SX76.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SX76.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDKS0/SX76.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SI1435.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SI1435.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SI1435.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SI1435.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SI1435.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SI2065.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SI2065.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SI2065.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SI2065.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SI2065.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SI2144.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SI2144.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SI2144.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SI2144.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SI2144.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SX175.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SX175.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SX175.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SX175.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SX175.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SX265.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SX265.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SX265.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SX265.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SX265.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SX355.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SX355.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SX355.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SX355.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SX355.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SX445.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SX445.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SX445.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SX445.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SX445.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SX85.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SX85.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SX85.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SX85.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDLC1/SX85.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SI1234.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SI1234.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SI1234.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SI1234.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SI1234.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SI1864.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SI1864.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SI1864.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SI1864.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SI1864.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SI604.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SI604.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SI604.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SI604.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SI604.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SX154.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SX154.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SX154.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SX154.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SX154.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SX244.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SX244.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SX244.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SX244.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SX244.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SX334.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SX334.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SX334.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SX334.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SX334.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SX424.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SX424.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SX424.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SX424.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SX424.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SX64.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SX64.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SX64.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SX64.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDLM0/SX64.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SI1233.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SI1233.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SI1233.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SI1233.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SI1233.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SI1863.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SI1863.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SI1863.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SI1863.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SI1863.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SI603.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SI603.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SI603.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SI603.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SI603.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SX153.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SX153.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SX153.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SX153.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SX153.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SX243.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SX243.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SX243.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SX243.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SX243.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SX333.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SX333.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SX333.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SX333.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SX333.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SX423.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SX423.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SX423.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SX423.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SX423.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SX63.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SX63.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SX63.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SX63.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDLR0/SX63.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SI1299.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SI1299.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SI1299.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SI1299.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SI1299.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SI1929.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SI1929.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SI1929.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SI1929.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SI1929.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SI669.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SI669.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SI669.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SI669.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SI669.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SX129.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SX129.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SX129.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SX129.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SX129.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SX219.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SX219.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SX219.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SX219.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SX219.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SX309.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SX309.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SX309.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SX309.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SX309.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SX39.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SX39.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SX39.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SX39.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SX39.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SX399.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SX399.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SX399.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SX399.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDLR1/SX399.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SI1760.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SI1760.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SI1760.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SI1760.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SI1760.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SI2126.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SI2126.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SI2126.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SI2126.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SI2126.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SI866.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SI866.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SI866.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SI866.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SI866.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SX146.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SX146.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SX146.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SX146.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SX146.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SX236.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SX236.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SX236.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SX236.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SX236.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SX326.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SX326.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SX326.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SX326.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SX326.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SX416.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SX416.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SX416.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SX416.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SX416.WRD  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SX56.PHN  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SX56.TXT  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SX56.WAV  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SX56.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MDPB0/SX56.WRD  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SI1674.PHN  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SI1674.TXT  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SI1674.WAV  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SI1674.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SI1674.WRD  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SI2225.PHN  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SI2225.TXT  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SI2225.WAV  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SI2225.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SI2225.WRD  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SI2304.PHN  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SI2304.TXT  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SI2304.WAV  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SI2304.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SI2304.WRD  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SX144.PHN  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SX144.TXT  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SX144.WAV  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SX144.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SX144.WRD  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SX234.PHN  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SX234.TXT  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SX234.WAV  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SX234.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SX234.WRD  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SX324.PHN  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SX324.TXT  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SX324.WAV  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SX324.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SX324.WRD  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SX414.PHN  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SX414.TXT  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SX414.WAV  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SX414.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SX414.WRD  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SX54.PHN  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SX54.TXT  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SX54.WAV  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SX54.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MFXS0/SX54.WRD  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SI1005.PHN  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SI1005.TXT  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SI1005.WAV  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SI1005.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SI1005.WRD  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SI1342.PHN  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SI1342.TXT  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SI1342.WAV  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SI1342.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SI1342.WRD  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SI1635.PHN  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SI1635.TXT  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SI1635.WAV  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SI1635.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SI1635.WRD  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SX105.PHN  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SX105.TXT  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SX105.WAV  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SX105.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SX105.WRD  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SX15.PHN  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SX15.TXT  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SX15.WAV  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SX15.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SX15.WRD  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SX195.PHN  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SX195.TXT  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SX195.WAV  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SX195.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SX195.WRD  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SX285.PHN  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SX285.TXT  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SX285.WAV  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SX285.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SX285.WRD  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SX375.PHN  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SX375.TXT  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SX375.WAV  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SX375.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MFXV0/SX375.WRD  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SI1036.PHN  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SI1036.TXT  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SI1036.WAV  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SI1036.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SI1036.WRD  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SI1666.PHN  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SI1666.TXT  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SI1666.WAV  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SI1666.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SI1666.WRD  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SI2296.PHN  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SI2296.TXT  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SI2296.WAV  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SI2296.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SI2296.WRD  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SX136.PHN  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SX136.TXT  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SX136.WAV  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SX136.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SX136.WRD  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SX226.PHN  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SX226.TXT  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SX226.WAV  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SX226.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SX226.WRD  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SX316.PHN  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SX316.TXT  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SX316.WAV  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SX316.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SX316.WRD  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SX406.PHN  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SX406.TXT  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SX406.WAV  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SX406.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SX406.WRD  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SX46.PHN  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SX46.TXT  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SX46.WAV  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SX46.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MGAK0/SX46.WRD  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SI1212.PHN  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SI1212.TXT  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SI1212.WAV  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SI1212.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SI1212.WRD  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SI1694.PHN  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SI1694.TXT  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SI1694.WAV  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SI1694.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SI1694.WRD  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SI1842.PHN  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SI1842.TXT  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SI1842.WAV  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SI1842.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SI1842.WRD  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SX132.PHN  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SX132.TXT  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SX132.WAV  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SX132.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SX132.WRD  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SX222.PHN  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SX222.TXT  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SX222.WAV  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SX222.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SX222.WRD  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SX312.PHN  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SX312.TXT  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SX312.WAV  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SX312.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SX312.WRD  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SX402.PHN  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SX402.TXT  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SX402.WAV  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SX402.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SX402.WRD  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SX42.PHN  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SX42.TXT  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SX42.WAV  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SX42.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MGAR0/SX42.WRD  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SI1165.PHN  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SI1165.TXT  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SI1165.WAV  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SI1165.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SI1165.WRD  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SI1802.PHN  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SI1802.TXT  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SI1802.WAV  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SI1802.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SI1802.WRD  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SI535.PHN  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SI535.TXT  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SI535.WAV  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SI535.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SI535.WRD  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SX175.PHN  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SX175.TXT  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SX175.WAV  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SX175.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SX175.WRD  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SX265.PHN  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SX265.TXT  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SX265.WAV  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SX265.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SX265.WRD  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SX355.PHN  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SX355.TXT  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SX355.WAV  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SX355.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SX355.WRD  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SX445.PHN  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SX445.TXT  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SX445.WAV  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SX445.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SX445.WRD  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SX85.PHN  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SX85.TXT  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SX85.WAV  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SX85.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MGAW0/SX85.WRD  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SI1164.PHN  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SI1164.TXT  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SI1164.WAV  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SI1164.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SI1164.WRD  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SI534.PHN  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SI534.TXT  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SI534.WAV  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SI534.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SI534.WRD  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SI797.PHN  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SI797.TXT  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SI797.WAV  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SI797.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SI797.WRD  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SX174.PHN  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SX174.TXT  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SX174.WAV  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SX174.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SX174.WRD  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SX264.PHN  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SX264.TXT  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SX264.WAV  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SX264.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SX264.WRD  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SX354.PHN  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SX354.TXT  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SX354.WAV  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SX354.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SX354.WRD  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SX444.PHN  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SX444.TXT  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SX444.WAV  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SX444.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SX444.WRD  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SX84.PHN  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SX84.TXT  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SX84.WAV  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SX84.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MGSL0/SX84.WRD  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SI1575.PHN  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SI1575.TXT  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SI1575.WAV  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SI1575.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SI1575.WRD  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SI2205.PHN  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SI2205.TXT  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SI2205.WAV  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SI2205.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SI2205.WRD  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SI945.PHN  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SI945.TXT  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SI945.WAV  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SI945.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SI945.WRD  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SX135.PHN  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SX135.TXT  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SX135.WAV  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SX135.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SX135.WRD  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SX225.PHN  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SX225.TXT  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SX225.WAV  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SX225.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SX225.WRD  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SX315.PHN  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SX315.TXT  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SX315.WAV  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SX315.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SX315.WRD  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SX405.PHN  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SX405.TXT  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SX405.WAV  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SX405.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SX405.WRD  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SX45.PHN  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SX45.TXT  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SX45.WAV  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SX45.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MHBS0/SX45.WRD  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SI1772.PHN  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SI1772.TXT  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SI1772.WAV  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SI1772.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SI1772.WRD  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SI512.PHN  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SI512.TXT  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SI512.WAV  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SI512.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SI512.WRD  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SI612.PHN  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SI612.TXT  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SI612.WAV  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SI612.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SI612.WRD  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SX152.PHN  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SX152.TXT  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SX152.WAV  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SX152.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SX152.WRD  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SX242.PHN  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SX242.TXT  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SX242.WAV  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SX242.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SX242.WRD  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SX332.PHN  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SX332.TXT  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SX332.WAV  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SX332.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SX332.WRD  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SX422.PHN  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SX422.TXT  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SX422.WAV  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SX422.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SX422.WRD  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SX62.PHN  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SX62.TXT  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SX62.WAV  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SX62.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MHXL0/SX62.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SI1604.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SI1604.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SI1604.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SI1604.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SI1604.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SI682.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SI682.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SI682.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SI682.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SI682.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SI710.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SI710.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SI710.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SI710.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SI710.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SX164.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SX164.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SX164.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SX164.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SX164.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SX254.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SX254.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SX254.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SX254.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SX254.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SX344.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SX344.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SX344.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SX344.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SX344.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SX434.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SX434.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SX434.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SX434.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SX434.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SX74.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SX74.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SX74.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SX74.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJAI0/SX74.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SI1042.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SI1042.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SI1042.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SI1042.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SI1042.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SI1672.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SI1672.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SI1672.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SI1672.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SI1672.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SI1705.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SI1705.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SI1705.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SI1705.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SI1705.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SX142.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SX142.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SX142.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SX142.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SX142.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SX232.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SX232.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SX232.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SX232.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SX232.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SX322.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SX322.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SX322.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SX322.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SX322.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SX412.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SX412.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SX412.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SX412.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SX412.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SX52.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SX52.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SX52.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SX52.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJDG0/SX52.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SI1605.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SI1605.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SI1605.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SI1605.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SI1605.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SI2235.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SI2235.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SI2235.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SI2235.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SI2235.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SI975.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SI975.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SI975.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SI975.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SI975.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SX165.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SX165.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SX165.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SX165.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SX165.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SX255.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SX255.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SX255.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SX255.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SX255.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SX345.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SX345.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SX345.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SX345.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SX345.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SX435.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SX435.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SX435.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SX435.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SX435.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SX75.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SX75.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SX75.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SX75.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJFR0/SX75.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SI1251.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SI1251.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SI1251.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SI1251.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SI1251.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SI1457.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SI1457.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SI1457.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SI1457.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SI1457.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SI827.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SI827.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SI827.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SI827.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SI827.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SX107.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SX107.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SX107.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SX107.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SX107.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SX17.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SX17.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SX17.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SX17.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SX17.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SX197.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SX197.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SX197.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SX197.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SX197.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SX287.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SX287.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SX287.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SX287.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SX287.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SX377.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SX377.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SX377.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SX377.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJJM0/SX377.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SI1236.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SI1236.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SI1236.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SI1236.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SI1236.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SI1866.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SI1866.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SI1866.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SI1866.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SI1866.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SI606.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SI606.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SI606.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SI606.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SI606.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SX156.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SX156.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SX156.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SX156.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SX156.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SX246.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SX246.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SX246.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SX246.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SX246.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SX336.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SX336.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SX336.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SX336.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SX336.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SX426.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SX426.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SX426.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SX426.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SX426.WRD  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SX66.PHN  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SX66.TXT  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SX66.WAV  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SX66.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MJRA0/SX66.WRD  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SI1609.PHN  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SI1609.TXT  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SI1609.WAV  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SI1609.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SI1609.WRD  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SI2239.PHN  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SI2239.TXT  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SI2239.WAV  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SI2239.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SI2239.WRD  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SI979.PHN  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SI979.TXT  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SI979.WAV  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SI979.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SI979.WRD  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SX169.PHN  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SX169.TXT  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SX169.WAV  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SX169.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SX169.WRD  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SX259.PHN  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SX259.TXT  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SX259.WAV  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SX259.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SX259.WRD  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SX30.PHN  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SX30.TXT  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SX30.WAV  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SX30.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SX30.WRD  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SX439.PHN  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SX439.TXT  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SX439.WAV  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SX439.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SX439.WRD  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SX79.PHN  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SX79.TXT  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SX79.WAV  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SX79.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MKAG0/SX79.WRD  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SI2132.PHN  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SI2132.TXT  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SI2132.WAV  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SI2132.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SI2132.WRD  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SI588.PHN  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SI588.TXT  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SI588.WAV  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SI588.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SI588.WRD  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SI872.PHN  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SI872.TXT  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SI872.WAV  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SI872.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SI872.WRD  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SX152.PHN  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SX152.TXT  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SX152.WAV  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SX152.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SX152.WRD  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SX242.PHN  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SX242.TXT  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SX242.WAV  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SX242.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SX242.WRD  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SX332.PHN  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SX332.TXT  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SX332.WAV  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SX332.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SX332.WRD  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SX422.PHN  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SX422.TXT  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SX422.WAV  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SX422.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SX422.WRD  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SX62.PHN  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SX62.TXT  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SX62.WAV  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SX62.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MKDB0/SX62.WRD  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SI1059.PHN  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SI1059.TXT  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SI1059.WAV  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SI1059.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SI1059.WRD  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SI1689.PHN  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SI1689.TXT  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SI1689.WAV  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SI1689.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SI1689.WRD  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SI2319.PHN  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SI2319.TXT  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SI2319.WAV  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SI2319.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SI2319.WRD  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SX159.PHN  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SX159.TXT  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SX159.WAV  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SX159.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SX159.WRD  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SX249.PHN  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SX249.TXT  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SX249.WAV  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SX249.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SX249.WRD  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SX339.PHN  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SX339.TXT  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SX339.WAV  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SX339.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SX339.WRD  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SX429.PHN  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SX429.TXT  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SX429.WAV  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SX429.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SX429.WRD  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SX69.PHN  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SX69.TXT  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SX69.WAV  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SX69.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MKLR0/SX69.WRD  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SI1780.PHN  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SI1780.TXT  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SI1780.WAV  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SI1780.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SI1780.WRD  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SI2035.PHN  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SI2035.TXT  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SI2035.WAV  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SI2035.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SI2035.WRD  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SI520.PHN  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SI520.TXT  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SI520.WAV  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SI520.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SI520.WRD  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SX160.PHN  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SX160.TXT  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SX160.WAV  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SX160.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SX160.WRD  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SX250.PHN  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SX250.TXT  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SX250.WAV  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SX250.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SX250.WRD  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SX340.PHN  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SX340.TXT  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SX340.WAV  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SX340.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SX340.WRD  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SX430.PHN  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SX430.TXT  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SX430.WAV  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SX430.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SX430.WRD  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SX70.PHN  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SX70.TXT  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SX70.WAV  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SX70.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MMDG0/SX70.WRD  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SI1071.PHN  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SI1071.TXT  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SI1071.WAV  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SI1071.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SI1071.WRD  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SI1701.PHN  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SI1701.TXT  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SI1701.WAV  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SI1701.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SI1701.WRD  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SI2331.PHN  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SI2331.TXT  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SI2331.WAV  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SI2331.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SI2331.WRD  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SX261.PHN  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SX261.TXT  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SX261.WAV  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SX261.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SX261.WRD  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SX27.PHN  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SX27.TXT  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SX27.WAV  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SX27.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SX27.WRD  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SX351.PHN  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SX351.TXT  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SX351.WAV  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SX351.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SX351.WRD  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SX441.PHN  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SX441.TXT  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SX441.WAV  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SX441.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SX441.WRD  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SX81.PHN  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SX81.TXT  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SX81.WAV  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SX81.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MMWS1/SX81.WRD  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SI1068.PHN  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SI1068.TXT  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SI1068.WAV  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SI1068.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SI1068.WRD  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SI1698.PHN  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SI1698.TXT  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SI1698.WAV  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SI1698.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SI1698.WRD  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SI2328.PHN  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SI2328.TXT  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SI2328.WAV  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SI2328.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SI2328.WRD  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SX168.PHN  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SX168.TXT  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SX168.WAV  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SX168.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SX168.WRD  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SX202.PHN  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SX202.TXT  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SX202.WAV  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SX202.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SX202.WRD  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SX258.PHN  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SX258.TXT  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SX258.WAV  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SX258.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SX258.WRD  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SX348.PHN  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SX348.TXT  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SX348.WAV  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SX348.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SX348.WRD  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SX78.PHN  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SX78.TXT  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SX78.WAV  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SX78.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MNTW0/SX78.WRD  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SI1576.PHN  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SI1576.TXT  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SI1576.WAV  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SI1576.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SI1576.WRD  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SI2206.PHN  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SI2206.TXT  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SI2206.WAV  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SI2206.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SI2206.WRD  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SI946.PHN  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SI946.TXT  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SI946.WAV  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SI946.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SI946.WRD  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SX136.PHN  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SX136.TXT  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SX136.WAV  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SX136.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SX136.WRD  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SX226.PHN  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SX226.TXT  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SX226.WAV  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SX226.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SX226.WRD  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SX316.PHN  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SX316.TXT  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SX316.WAV  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SX316.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SX316.WRD  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SX406.PHN  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SX406.TXT  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SX406.WAV  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SX406.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SX406.WRD  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SX46.PHN  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SX46.TXT  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SX46.WAV  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SX46.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MPAR0/SX46.WRD  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SI1258.PHN  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SI1258.TXT  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SI1258.WAV  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SI1258.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SI1258.WRD  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SI1888.PHN  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SI1888.TXT  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SI1888.WAV  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SI1888.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SI1888.WRD  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SI628.PHN  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SI628.TXT  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SI628.WAV  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SI628.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SI628.WRD  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SX178.PHN  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SX178.TXT  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SX178.WAV  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SX178.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SX178.WRD  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SX268.PHN  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SX268.TXT  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SX268.WAV  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SX268.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SX268.WRD  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SX358.PHN  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SX358.TXT  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SX358.WAV  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SX358.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SX358.WRD  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SX448.PHN  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SX448.TXT  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SX448.WAV  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SX448.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SX448.WRD  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SX88.PHN  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SX88.TXT  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SX88.WAV  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SX88.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MPFU0/SX88.WRD  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SI1591.PHN  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SI1591.TXT  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SI1591.WAV  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SI1591.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SI1591.WRD  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SI511.PHN  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SI511.TXT  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SI511.WAV  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SI511.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SI511.WRD  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SI961.PHN  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SI961.TXT  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SI961.WAV  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SI961.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SI961.WRD  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SX151.PHN  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SX151.TXT  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SX151.WAV  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SX151.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SX151.WRD  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SX241.PHN  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SX241.TXT  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SX241.WAV  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SX241.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SX241.WRD  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SX331.PHN  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SX331.TXT  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SX331.WAV  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SX331.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SX331.WRD  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SX421.PHN  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SX421.TXT  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SX421.WAV  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SX421.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SX421.WRD  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SX61.PHN  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SX61.TXT  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SX61.WAV  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SX61.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MREM0/SX61.WRD  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SI1671.PHN  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SI1671.TXT  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SI1671.WAV  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SI1671.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SI1671.WRD  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SI2301.PHN  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SI2301.TXT  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SI2301.WAV  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SI2301.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SI2301.WRD  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SI2332.PHN  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SI2332.TXT  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SI2332.WAV  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SI2332.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SI2332.WRD  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SX141.PHN  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SX141.TXT  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SX141.WAV  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SX141.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SX141.WRD  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SX231.PHN  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SX231.TXT  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SX231.WAV  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SX231.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SX231.WRD  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SX321.PHN  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SX321.TXT  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SX321.WAV  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SX321.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SX321.WRD  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SX411.PHN  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SX411.TXT  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SX411.WAV  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SX411.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SX411.WRD  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SX51.PHN  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SX51.TXT  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SX51.WAV  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SX51.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MRLJ1/SX51.WRD  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SI1080.PHN  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SI1080.TXT  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SI1080.WAV  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SI1080.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SI1080.WRD  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SI1710.PHN  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SI1710.TXT  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SI1710.WAV  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SI1710.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SI1710.WRD  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SI2340.PHN  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SI2340.TXT  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SI2340.WAV  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SI2340.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SI2340.WRD  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SX180.PHN  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SX180.TXT  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SX180.WAV  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SX180.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SX180.WRD  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SX270.PHN  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SX270.TXT  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SX270.WAV  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SX270.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SX270.WRD  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SX360.PHN  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SX360.TXT  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SX360.WAV  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SX360.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SX360.WRD  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SX450.PHN  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SX450.TXT  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SX450.WAV  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SX450.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SX450.WRD  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SX90.PHN  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SX90.TXT  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SX90.WAV  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SX90.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MRMG0/SX90.WRD  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SI1021.PHN  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SI1021.TXT  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SI1021.WAV  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SI1021.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SI1021.WRD  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SI1349.PHN  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SI1349.TXT  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SI1349.WAV  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SI1349.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SI1349.WRD  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SI2281.PHN  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SI2281.TXT  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SI2281.WAV  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SI2281.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SI2281.WRD  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SX121.PHN  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SX121.TXT  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SX121.WAV  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SX121.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SX121.WRD  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SX211.PHN  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SX211.TXT  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SX211.WAV  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SX211.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SX211.WRD  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SX301.PHN  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SX301.TXT  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SX301.WAV  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SX301.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SX301.WRD  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SX31.PHN  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SX31.TXT  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SX31.WAV  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SX31.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SX31.WRD  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SX391.PHN  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SX391.TXT  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SX391.WAV  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SX391.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MRMH0/SX391.WRD  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SI1482.PHN  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SI1482.TXT  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SI1482.WAV  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SI1482.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SI1482.WRD  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SI2026.PHN  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SI2026.TXT  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SI2026.WAV  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SI2026.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SI2026.WRD  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SI2112.PHN  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SI2112.TXT  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SI2112.WAV  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SI2112.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SI2112.WRD  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SX132.PHN  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SX132.TXT  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SX132.WAV  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SX132.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SX132.WRD  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SX222.PHN  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SX222.TXT  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SX222.WAV  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SX222.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SX222.WRD  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SX312.PHN  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SX312.TXT  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SX312.WAV  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SX312.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SX312.WRD  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SX402.PHN  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SX402.TXT  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SX402.WAV  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SX402.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SX402.WRD  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SX42.PHN  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SX42.TXT  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SX42.WAV  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SX42.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MRPC1/SX42.WRD  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SI1049.PHN  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SI1049.TXT  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SI1049.WAV  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SI1049.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SI1049.WRD  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SI1679.PHN  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SI1679.TXT  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SI1679.WAV  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SI1679.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SI1679.WRD  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SI2309.PHN  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SI2309.TXT  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SI2309.WAV  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SI2309.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SI2309.WRD  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SX149.PHN  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SX149.TXT  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SX149.WAV  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SX149.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SX149.WRD  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SX239.PHN  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SX239.TXT  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SX239.WAV  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SX239.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SX239.WRD  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SX329.PHN  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SX329.TXT  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SX329.WAV  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SX329.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SX329.WRD  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SX419.PHN  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SX419.TXT  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SX419.WAV  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SX419.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SX419.WRD  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SX59.PHN  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SX59.TXT  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SX59.WAV  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SX59.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MSAH1/SX59.WRD  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SI1007.PHN  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SI1007.TXT  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SI1007.WAV  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SI1007.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SI1007.WRD  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SI1637.PHN  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SI1637.TXT  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SI1637.WAV  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SI1637.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SI1637.WRD  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SI2267.PHN  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SI2267.TXT  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SI2267.WAV  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SI2267.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SI2267.WRD  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SX107.PHN  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SX107.TXT  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SX107.WAV  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SX107.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SX107.WRD  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SX17.PHN  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SX17.TXT  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SX17.WAV  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SX17.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SX17.WRD  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SX197.PHN  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SX197.TXT  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SX197.WAV  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SX197.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SX197.WRD  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SX287.PHN  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SX287.TXT  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SX287.WAV  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SX287.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SX287.WRD  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SX377.PHN  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SX377.TXT  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SX377.WAV  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SX377.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MSDB0/SX377.WRD  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SI1589.PHN  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SI1589.TXT  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SI1589.WAV  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SI1589.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SI1589.WRD  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SI2216.PHN  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SI2216.TXT  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SI2216.WAV  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SI2216.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SI2216.WRD  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SI2219.PHN  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SI2219.TXT  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SI2219.WAV  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SI2219.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SI2219.WRD  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SX149.PHN  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SX149.TXT  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SX149.WAV  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SX149.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SX149.WRD  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SX239.PHN  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SX239.TXT  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SX239.WAV  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SX239.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SX239.WRD  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SX329.PHN  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SX329.TXT  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SX329.WAV  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SX329.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SX329.WRD  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SX419.PHN  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SX419.TXT  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SX419.WAV  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SX419.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SX419.WRD  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SX59.PHN  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SX59.TXT  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SX59.WAV  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SX59.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MSES0/SX59.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SI1572.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SI1572.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SI1572.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SI1572.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SI1572.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SI2202.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SI2202.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SI2202.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SI2202.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SI2202.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SI942.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SI942.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SI942.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SI942.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SI942.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SX132.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SX132.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SX132.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SX132.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SX132.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SX222.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SX222.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SX222.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SX222.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SX222.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SX312.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SX312.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SX312.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SX312.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SX312.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SX402.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SX402.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SX402.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SX402.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SX402.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SX42.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SX42.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SX42.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SX42.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTAB0/SX42.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SI1157.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SI1157.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SI1157.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SI1157.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SI1157.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SI1787.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SI1787.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SI1787.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SI1787.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SI1787.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SI527.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SI527.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SI527.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SI527.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SI527.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SX167.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SX167.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SX167.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SX167.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SX167.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SX17.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SX17.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SX17.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SX17.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SX17.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SX257.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SX257.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SX257.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SX257.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SX257.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SX437.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SX437.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SX437.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SX437.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SX437.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SX77.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SX77.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SX77.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SX77.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTER0/SX77.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SI1187.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SI1187.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SI1187.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SI1187.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SI1187.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SI1817.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SI1817.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SI1817.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SI1817.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SI1817.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SI630.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SI630.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SI630.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SI630.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SI630.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SX107.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SX107.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SX107.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SX107.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SX107.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SX17.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SX17.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SX17.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SX17.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SX17.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SX197.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SX197.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SX197.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SX197.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SX197.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SX287.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SX287.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SX287.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SX287.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SX287.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SX377.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SX377.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SX377.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SX377.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTKD0/SX377.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SI1313.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SI1313.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SI1313.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SI1313.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SI1313.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SI1477.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SI1477.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SI1477.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SI1477.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SI1477.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SI847.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SI847.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SI847.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SI847.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SI847.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SX127.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SX127.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SX127.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SX127.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SX127.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SX217.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SX217.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SX217.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SX217.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SX217.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SX307.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SX307.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SX307.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SX307.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SX307.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SX37.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SX37.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SX37.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SX37.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SX37.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SX397.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SX397.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SX397.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SX397.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTLC0/SX397.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SI1065.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SI1065.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SI1065.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SI1065.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SI1065.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SI1695.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SI1695.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SI1695.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SI1695.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SI1695.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SI2325.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SI2325.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SI2325.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SI2325.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SI2325.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SX165.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SX165.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SX165.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SX165.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SX165.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SX255.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SX255.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SX255.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SX255.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SX255.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SX345.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SX345.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SX345.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SX345.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SX345.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SX435.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SX435.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SX435.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SX435.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SX435.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SX75.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SX75.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SX75.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SX75.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTML0/SX75.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SI1064.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SI1064.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SI1064.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SI1064.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SI1064.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SI2324.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SI2324.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SI2324.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SI2324.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SI2324.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SI582.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SI582.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SI582.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SI582.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SI582.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SX164.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SX164.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SX164.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SX164.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SX164.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SX254.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SX254.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SX254.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SX254.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SX254.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SX344.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SX344.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SX344.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SX344.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SX344.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SX434.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SX434.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SX434.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SX434.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SX434.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SX74.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SX74.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SX74.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SX74.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTMN0/SX74.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SI1600.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SI1600.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SI1600.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SI1600.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SI1600.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SI2230.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SI2230.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SI2230.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SI2230.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SI2230.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SI506.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SI506.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SI506.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SI506.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SI506.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SX160.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SX160.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SX160.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SX160.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SX160.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SX250.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SX250.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SX250.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SX250.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SX250.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SX340.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SX340.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SX340.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SX340.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SX340.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SX430.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SX430.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SX430.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SX430.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SX430.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SX70.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SX70.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SX70.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SX70.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTPR0/SX70.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SI1512.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SI1512.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SI1512.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SI1512.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SI1512.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SI2142.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SI2142.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SI2142.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SI2142.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SI2142.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SI882.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SI882.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SI882.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SI882.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SI882.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SX162.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SX162.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SX162.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SX162.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SX162.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SX252.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SX252.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SX252.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SX252.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SX252.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SX342.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SX342.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SX342.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SX342.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SX342.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SX432.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SX432.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SX432.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SX432.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SX432.WRD  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SX72.PHN  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SX72.TXT  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SX72.WAV  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SX72.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MTWH1/SX72.WRD  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SI1485.PHN  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SI1485.TXT  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SI1485.WAV  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SI1485.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SI1485.WRD  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SI2115.PHN  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SI2115.TXT  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SI2115.WAV  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SI2115.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SI2115.WRD  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SI855.PHN  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SI855.TXT  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SI855.WAV  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SI855.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SI855.WRD  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SX135.PHN  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SX135.TXT  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SX135.WAV  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SX135.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SX135.WRD  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SX225.PHN  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SX225.TXT  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SX225.WAV  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SX225.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SX225.WRD  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SX315.PHN  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SX315.TXT  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SX315.WAV  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SX315.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SX315.WRD  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SX405.PHN  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SX405.TXT  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SX405.WAV  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SX405.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SX405.WRD  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SX45.PHN  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SX45.TXT  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SX45.WAV  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SX45.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MVRW0/SX45.WRD  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SI1057.PHN  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SI1057.TXT  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SI1057.WAV  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SI1057.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SI1057.WRD  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SI1687.PHN  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SI1687.TXT  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SI1687.WAV  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SI1687.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SI1687.WRD  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SI2317.PHN  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SI2317.TXT  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SI2317.WAV  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SI2317.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SI2317.WRD  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SX157.PHN  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SX157.TXT  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SX157.WAV  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SX157.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SX157.WRD  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SX247.PHN  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SX247.TXT  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SX247.WAV  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SX247.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SX247.WRD  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SX337.PHN  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SX337.TXT  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SX337.WAV  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SX337.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SX337.WRD  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SX427.PHN  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SX427.TXT  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SX427.WAV  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SX427.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SX427.WRD  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SX67.PHN  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SX67.TXT  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SX67.WAV  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SX67.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MWRE0/SX67.WRD  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SI1443.PHN  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SI1443.TXT  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SI1443.WAV  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SI1443.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SI1443.WRD  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SI1525.PHN  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SI1525.TXT  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SI1525.WAV  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SI1525.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SI1525.WRD  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SI2073.PHN  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SI2073.TXT  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SI2073.WAV  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SI2073.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SI2073.WRD  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SX183.PHN  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SX183.TXT  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SX183.WAV  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SX183.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SX183.WRD  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SX273.PHN  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SX273.TXT  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SX273.WAV  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SX273.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SX273.WRD  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SX3.PHN  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SX3.TXT  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SX3.WAV  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SX3.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SX3.WRD  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SX363.PHN  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SX363.TXT  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SX363.WAV  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SX363.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SX363.WRD  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SX93.PHN  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SX93.TXT  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SX93.WAV  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SX93.WAV.wav  \n",
            "  inflating: data/TRAIN/DR7/MWRP0/SX93.WRD  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SI1612.PHN  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SI1612.TXT  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SI1612.WAV  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SI1612.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SI1612.WRD  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SI2242.PHN  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SI2242.TXT  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SI2242.WAV  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SI2242.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SI2242.WRD  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SI982.PHN  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SI982.TXT  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SI982.WAV  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SI982.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SI982.WRD  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SX172.PHN  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SX172.TXT  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SX172.WAV  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SX172.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SX172.WRD  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SX262.PHN  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SX262.TXT  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SX262.WAV  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SX262.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SX262.WRD  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SX352.PHN  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SX352.TXT  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SX352.WAV  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SX352.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SX352.WRD  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SX442.PHN  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SX442.TXT  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SX442.WAV  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SX442.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SX442.WRD  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SX82.PHN  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SX82.TXT  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SX82.WAV  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SX82.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FBCG1/SX82.WRD  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SI1248.PHN  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SI1248.TXT  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SI1248.WAV  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SI1248.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SI1248.WRD  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SI1878.PHN  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SI1878.TXT  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SI1878.WAV  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SI1878.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SI1878.WRD  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SI618.PHN  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SI618.TXT  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SI618.WAV  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SI618.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SI618.WRD  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SX168.PHN  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SX168.TXT  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SX168.WAV  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SX168.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SX168.WRD  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SX258.PHN  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SX258.TXT  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SX258.WAV  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SX258.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SX258.WRD  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SX348.PHN  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SX348.TXT  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SX348.WAV  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SX348.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SX348.WRD  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SX438.PHN  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SX438.TXT  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SX438.WAV  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SX438.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SX438.WRD  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SX78.PHN  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SX78.TXT  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SX78.WAV  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SX78.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FCEG0/SX78.WRD  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SI1438.PHN  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SI1438.TXT  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SI1438.WAV  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SI1438.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SI1438.WRD  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SI2068.PHN  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SI2068.TXT  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SI2068.WAV  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SI2068.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SI2068.WRD  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SI808.PHN  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SI808.TXT  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SI808.WAV  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SI808.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SI808.WRD  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SX178.PHN  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SX178.TXT  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SX178.WAV  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SX178.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SX178.WRD  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SX268.PHN  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SX268.TXT  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SX268.WAV  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SX268.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SX268.WRD  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SX358.PHN  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SX358.TXT  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SX358.WAV  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SX358.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SX358.WRD  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SX448.PHN  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SX448.TXT  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SX448.WAV  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SX448.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SX448.WRD  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SX88.PHN  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SX88.TXT  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SX88.WAV  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SX88.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FCLT0/SX88.WRD  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SI1302.PHN  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SI1302.TXT  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SI1302.WAV  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SI1302.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SI1302.WRD  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SI1932.PHN  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SI1932.TXT  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SI1932.WAV  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SI1932.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SI1932.WRD  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SI672.PHN  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SI672.TXT  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SI672.WAV  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SI672.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SI672.WRD  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SX132.PHN  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SX132.TXT  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SX132.WAV  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SX132.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SX132.WRD  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SX222.PHN  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SX222.TXT  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SX222.WAV  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SX222.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SX222.WRD  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SX312.PHN  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SX312.TXT  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SX312.WAV  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SX312.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SX312.WRD  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SX402.PHN  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SX402.TXT  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SX402.WAV  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SX402.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SX402.WRD  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SX42.PHN  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SX42.TXT  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SX42.WAV  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SX42.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FJRB0/SX42.WRD  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SI1257.PHN  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SI1257.TXT  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SI1257.WAV  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SI1257.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SI1257.WRD  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SI1887.PHN  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SI1887.TXT  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SI1887.WAV  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SI1887.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SI1887.WRD  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SI627.PHN  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SI627.TXT  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SI627.WAV  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SI627.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SI627.WRD  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SX177.PHN  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SX177.TXT  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SX177.WAV  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SX177.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SX177.WRD  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SX267.PHN  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SX267.TXT  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SX267.WAV  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SX267.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SX267.WRD  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SX357.PHN  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SX357.TXT  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SX357.WAV  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SX357.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SX357.WRD  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SX447.PHN  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SX447.TXT  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SX447.WAV  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SX447.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SX447.WRD  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SX87.PHN  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SX87.TXT  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SX87.WAV  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SX87.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FKLH0/SX87.WRD  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SI1160.PHN  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SI1160.TXT  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SI1160.WAV  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SI1160.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SI1160.WRD  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SI1790.PHN  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SI1790.TXT  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SI1790.WAV  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SI1790.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SI1790.WRD  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SI2264.PHN  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SI2264.TXT  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SI2264.WAV  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SI2264.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SI2264.WRD  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SX260.PHN  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SX260.TXT  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SX260.WAV  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SX260.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SX260.WRD  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SX3.PHN  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SX3.TXT  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SX3.WAV  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SX3.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SX3.WRD  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SX350.PHN  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SX350.TXT  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SX350.WAV  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SX350.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SX350.WRD  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SX440.PHN  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SX440.TXT  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SX440.WAV  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SX440.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SX440.WRD  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SX80.PHN  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SX80.TXT  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SX80.WAV  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SX80.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FMBG0/SX80.WRD  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SI1522.PHN  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SI1522.TXT  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SI1522.WAV  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SI1522.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SI1522.WRD  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SI2152.PHN  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SI2152.TXT  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SI2152.WAV  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SI2152.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SI2152.WRD  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SI892.PHN  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SI892.TXT  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SI892.WAV  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SI892.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SI892.WRD  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SX172.PHN  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SX172.TXT  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SX172.WAV  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SX172.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SX172.WRD  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SX196.PHN  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SX196.TXT  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SX196.WAV  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SX196.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SX196.WRD  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SX262.PHN  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SX262.TXT  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SX262.WAV  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SX262.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SX262.WRD  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SX442.PHN  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SX442.TXT  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SX442.WAV  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SX442.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SX442.WRD  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SX82.PHN  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SX82.TXT  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SX82.WAV  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SX82.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FNKL0/SX82.WRD  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SI1590.PHN  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SI1590.TXT  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SI1590.WAV  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SI1590.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SI1590.WRD  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SI2220.PHN  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SI2220.TXT  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SI2220.WAV  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SI2220.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SI2220.WRD  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SI960.PHN  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SI960.TXT  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SI960.WAV  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SI960.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SI960.WRD  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SX150.PHN  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SX150.TXT  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SX150.WAV  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SX150.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SX150.WRD  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SX240.PHN  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SX240.TXT  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SX240.WAV  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SX240.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SX240.WRD  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SX3.PHN  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SX3.TXT  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SX3.WAV  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SX3.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SX3.WRD  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SX330.PHN  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SX330.TXT  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SX330.WAV  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SX330.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SX330.WRD  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SX60.PHN  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SX60.TXT  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SX60.WAV  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SX60.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/FPLS0/SX60.WRD  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SI2217.PHN  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SI2217.TXT  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SI2217.WAV  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SI2217.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SI2217.WRD  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SI486.PHN  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SI486.TXT  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SI486.WAV  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SI486.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SI486.WRD  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SI957.PHN  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SI957.TXT  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SI957.WAV  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SI957.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SI957.WRD  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SX147.PHN  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SX147.TXT  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SX147.WAV  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SX147.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SX147.WRD  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SX237.PHN  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SX237.TXT  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SX237.WAV  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SX237.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SX237.WRD  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SX327.PHN  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SX327.TXT  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SX327.WAV  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SX327.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SX327.WRD  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SX417.PHN  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SX417.TXT  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SX417.WAV  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SX417.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SX417.WRD  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SX57.PHN  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SX57.TXT  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SX57.WAV  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SX57.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MBCG0/SX57.WRD  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SI1353.PHN  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SI1353.TXT  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SI1353.WAV  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SI1353.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SI1353.WRD  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SI1983.PHN  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SI1983.TXT  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SI1983.WAV  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SI1983.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SI1983.WRD  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SI723.PHN  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SI723.TXT  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SI723.WAV  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SI723.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SI723.WRD  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SX183.PHN  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SX183.TXT  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SX183.WAV  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SX183.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SX183.WRD  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SX273.PHN  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SX273.TXT  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SX273.WAV  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SX273.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SX273.WRD  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SX3.PHN  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SX3.TXT  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SX3.WAV  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SX3.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SX3.WRD  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SX363.PHN  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SX363.TXT  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SX363.WAV  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SX363.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SX363.WRD  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SX93.PHN  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SX93.TXT  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SX93.WAV  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SX93.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MBSB0/SX93.WRD  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SI1351.PHN  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SI1351.TXT  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SI1351.WAV  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SI1351.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SI1351.WRD  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SI1981.PHN  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SI1981.TXT  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SI1981.WAV  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SI1981.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SI1981.WRD  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SI721.PHN  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SI721.TXT  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SI721.WAV  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SI721.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SI721.WRD  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SX181.PHN  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SX181.TXT  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SX181.WAV  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SX181.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SX181.WRD  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SX271.PHN  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SX271.TXT  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SX271.WAV  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SX271.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SX271.WRD  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SX361.PHN  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SX361.TXT  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SX361.WAV  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SX361.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SX361.WRD  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SX451.PHN  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SX451.TXT  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SX451.WAV  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SX451.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SX451.WRD  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SX91.PHN  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SX91.TXT  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SX91.WAV  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SX91.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MCXM0/SX91.WRD  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SI1240.PHN  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SI1240.TXT  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SI1240.WAV  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SI1240.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SI1240.WRD  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SI1870.PHN  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SI1870.TXT  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SI1870.WAV  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SI1870.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SI1870.WRD  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SI610.PHN  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SI610.TXT  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SI610.WAV  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SI610.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SI610.WRD  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SX160.PHN  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SX160.TXT  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SX160.WAV  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SX160.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SX160.WRD  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SX250.PHN  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SX250.TXT  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SX250.WAV  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SX250.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SX250.WRD  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SX340.PHN  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SX340.TXT  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SX340.WAV  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SX340.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SX340.WRD  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SX430.PHN  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SX430.TXT  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SX430.WAV  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SX430.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SX430.WRD  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SX70.PHN  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SX70.TXT  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SX70.WAV  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SX70.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MEJS0/SX70.WRD  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SI1567.PHN  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SI1567.TXT  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SI1567.WAV  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SI1567.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SI1567.WRD  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SI2197.PHN  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SI2197.TXT  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SI2197.WAV  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SI2197.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SI2197.WRD  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SI937.PHN  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SI937.TXT  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SI937.WAV  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SI937.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SI937.WRD  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SX127.PHN  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SX127.TXT  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SX127.WAV  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SX127.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SX127.WRD  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SX217.PHN  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SX217.TXT  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SX217.WAV  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SX217.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SX217.WRD  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SX307.PHN  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SX307.TXT  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SX307.WAV  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SX307.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SX307.WRD  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SX37.PHN  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SX37.TXT  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SX37.WAV  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SX37.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SX37.WRD  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SX397.PHN  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SX397.TXT  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SX397.WAV  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SX397.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MKDD0/SX397.WRD  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SI1491.PHN  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SI1491.TXT  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SI1491.WAV  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SI1491.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SI1491.WRD  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SI2121.PHN  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SI2121.TXT  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SI2121.WAV  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SI2121.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SI2121.WRD  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SI861.PHN  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SI861.TXT  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SI861.WAV  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SI861.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SI861.WRD  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SX141.PHN  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SX141.TXT  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SX141.WAV  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SX141.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SX141.WRD  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SX231.PHN  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SX231.TXT  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SX231.WAV  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SX231.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SX231.WRD  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SX31.PHN  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SX31.TXT  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SX31.WAV  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SX31.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SX31.WRD  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SX411.PHN  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SX411.TXT  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SX411.WAV  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SX411.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SX411.WRD  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SX51.PHN  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SX51.TXT  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SX51.WAV  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SX51.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MKRG0/SX51.WRD  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SI1388.PHN  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SI1388.TXT  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SI1388.WAV  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SI1388.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SI1388.WRD  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SI2018.PHN  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SI2018.TXT  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SI2018.WAV  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SI2018.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SI2018.WRD  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SI758.PHN  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SI758.TXT  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SI758.WAV  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SI758.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SI758.WRD  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SX128.PHN  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SX128.TXT  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SX128.WAV  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SX128.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SX128.WRD  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SX218.PHN  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SX218.TXT  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SX218.WAV  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SX218.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SX218.WRD  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SX308.PHN  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SX308.TXT  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SX308.WAV  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SX308.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SX308.WRD  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SX38.PHN  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SX38.TXT  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SX38.WAV  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SX38.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SX38.WRD  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SX398.PHN  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SX398.TXT  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SX398.WAV  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SX398.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MMEA0/SX398.WRD  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SI1527.PHN  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SI1527.TXT  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SI1527.WAV  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SI1527.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SI1527.WRD  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SI2150.PHN  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SI2150.TXT  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SI2150.WAV  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SI2150.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SI2150.WRD  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SI897.PHN  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SI897.TXT  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SI897.WAV  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SI897.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SI897.WRD  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SX177.PHN  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SX177.TXT  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SX177.WAV  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SX177.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SX177.WRD  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SX267.PHN  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SX267.TXT  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SX267.WAV  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SX267.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SX267.WRD  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SX357.PHN  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SX357.TXT  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SX357.WAV  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SX357.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SX357.WRD  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SX447.PHN  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SX447.TXT  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SX447.WAV  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SX447.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SX447.WRD  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SX87.PHN  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SX87.TXT  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SX87.WAV  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SX87.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MMLM0/SX87.WRD  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SI1061.PHN  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SI1061.TXT  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SI1061.WAV  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SI1061.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SI1061.WRD  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SI1691.PHN  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SI1691.TXT  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SI1691.WAV  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SI1691.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SI1691.WRD  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SI2321.PHN  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SI2321.TXT  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SI2321.WAV  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SI2321.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SI2321.WRD  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SX161.PHN  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SX161.TXT  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SX161.WAV  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SX161.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SX161.WRD  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SX251.PHN  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SX251.TXT  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SX251.WAV  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SX251.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SX251.WRD  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SX341.PHN  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SX341.TXT  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SX341.WAV  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SX341.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SX341.WRD  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SX431.PHN  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SX431.TXT  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SX431.WAV  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SX431.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SX431.WRD  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SX71.PHN  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SX71.TXT  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SX71.WAV  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SX71.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MMPM0/SX71.WRD  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SI1518.PHN  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SI1518.TXT  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SI1518.WAV  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SI1518.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SI1518.WRD  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SI559.PHN  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SI559.TXT  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SI559.WAV  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SI559.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SI559.WRD  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SI888.PHN  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SI888.TXT  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SI888.WAV  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SI888.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SI888.WRD  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SX168.PHN  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SX168.TXT  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SX168.WAV  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SX168.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SX168.WRD  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SX258.PHN  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SX258.TXT  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SX258.WAV  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SX258.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SX258.WRD  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SX348.PHN  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SX348.TXT  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SX348.WAV  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SX348.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SX348.WRD  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SX438.PHN  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SX438.TXT  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SX438.WAV  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SX438.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SX438.WRD  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SX78.PHN  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SX78.TXT  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SX78.WAV  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SX78.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MMWS0/SX78.WRD  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SI1044.PHN  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SI1044.TXT  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SI1044.WAV  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SI1044.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SI1044.WRD  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SI1595.PHN  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SI1595.TXT  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SI1595.WAV  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SI1595.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SI1595.WRD  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SI965.PHN  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SI965.TXT  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SI965.WAV  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SI965.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SI965.WRD  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SX155.PHN  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SX155.TXT  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SX155.WAV  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SX155.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SX155.WRD  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SX245.PHN  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SX245.TXT  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SX245.WAV  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SX245.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SX245.WRD  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SX335.PHN  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SX335.TXT  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SX335.WAV  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SX335.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SX335.WRD  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SX425.PHN  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SX425.TXT  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SX425.WAV  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SX425.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SX425.WRD  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SX65.PHN  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SX65.TXT  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SX65.WAV  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SX65.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MRDM0/SX65.WRD  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SI1468.PHN  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SI1468.TXT  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SI1468.WAV  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SI1468.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SI1468.WRD  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SI2140.PHN  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SI2140.TXT  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SI2140.WAV  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SI2140.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SI2140.WRD  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SI843.PHN  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SI843.TXT  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SI843.WAV  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SI843.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SI843.WRD  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SX123.PHN  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SX123.TXT  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SX123.WAV  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SX123.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SX123.WRD  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SX213.PHN  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SX213.TXT  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SX213.WAV  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SX213.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SX213.WRD  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SX303.PHN  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SX303.TXT  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SX303.WAV  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SX303.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SX303.WRD  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SX33.PHN  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SX33.TXT  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SX33.WAV  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SX33.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SX33.WRD  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SX393.PHN  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SX393.TXT  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SX393.WAV  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SX393.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MRLK0/SX393.WRD  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SI1334.PHN  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SI1334.TXT  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SI1334.WAV  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SI1334.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SI1334.WRD  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SI704.PHN  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SI704.TXT  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SI704.WAV  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SI704.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SI704.WRD  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SI952.PHN  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SI952.TXT  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SI952.WAV  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SI952.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SI952.WRD  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SX164.PHN  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SX164.TXT  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SX164.WAV  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SX164.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SX164.WRD  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SX254.PHN  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SX254.TXT  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SX254.WAV  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SX254.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SX254.WRD  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SX344.PHN  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SX344.TXT  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SX344.WAV  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SX344.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SX344.WRD  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SX434.PHN  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SX434.TXT  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SX434.WAV  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SX434.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SX434.WRD  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SX74.PHN  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SX74.TXT  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SX74.WAV  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SX74.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MRRE0/SX74.WRD  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SA1.PHN  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SA1.TXT  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SA1.WAV  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SA1.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SA1.WRD  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SA2.PHN  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SA2.TXT  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SA2.WAV  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SA2.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SA2.WRD  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SI1972.PHN  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SI1972.TXT  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SI1972.WAV  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SI1972.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SI1972.WRD  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SI2265.PHN  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SI2265.TXT  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SI2265.WAV  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SI2265.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SI2265.WRD  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SI712.PHN  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SI712.TXT  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SI712.WAV  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SI712.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SI712.WRD  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SX172.PHN  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SX172.TXT  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SX172.WAV  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SX172.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SX172.WRD  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SX262.PHN  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SX262.TXT  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SX262.WAV  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SX262.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SX262.WRD  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SX352.PHN  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SX352.TXT  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SX352.WAV  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SX352.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SX352.WRD  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SX442.PHN  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SX442.TXT  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SX442.WAV  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SX442.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SX442.WRD  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SX82.PHN  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SX82.TXT  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SX82.WAV  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SX82.WAV.wav  \n",
            "  inflating: data/TRAIN/DR8/MTCS0/SX82.WRD  \n",
            "  inflating: test_data.csv           \n",
            "  inflating: train_data.csv          \n",
            "unzip:  cannot find or open data.zip, data.zip.zip or data.zip.ZIP.\n",
            "Collecting webrtcvad\n",
            "  Downloading webrtcvad-2.0.10.tar.gz (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.2/66.2 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: webrtcvad\n",
            "  Building wheel for webrtcvad (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for webrtcvad: filename=webrtcvad-2.0.10-cp310-cp310-linux_x86_64.whl size=73471 sha256=b4b4d0201f3e5396b390b81a9bad48bd3de0733db13a72d98cd24b00ea9c49fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/2b/84/ac7bacfe8c68a87c1ee3dd3c66818a54c71599abf308e8eb35\n",
            "Successfully built webrtcvad\n",
            "Installing collected packages: webrtcvad\n",
            "Successfully installed webrtcvad-2.0.10\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d mfekadu/darpa-timit-acousticphonetic-continuous-speech\n",
        "!unzip darpa-timit-acousticphonetic-continuous-speech.zip\n",
        "!unzip data.zip\n",
        "!pip install webrtcvad"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code segment defines a Python dictionary named dict1 with various key-value pairs. Each key represents a configuration parameter, and its corresponding value specifies a setting or value for that parameter."
      ],
      "metadata": {
        "id": "1UkJwcWws1j-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFm5GTA2cYIu"
      },
      "outputs": [],
      "source": [
        "dict1={\"training\": True,\n",
        "\"device\": \"cuda\",\n",
        "\"unprocessed_data\":'/content/data/TRAIN/*/*/*.wav  ',\n",
        "\"train_path\" : '/content/train_tisv',\n",
        "\"train_path_unprocessed\": '/content/data/TRAIN/*/*/*.wav',\n",
        "\"test_path\": 'test_tisv',\n",
        "\"test_path_unprocessed\": '/content/data/TRAIN/*/*/*.wav',\n",
        "\"data_preprocessed\":True ,\n",
        "\"sr\": 16000,\n",
        "\"nfft\": 512,\n",
        "\"window\": 0.025 ,\n",
        "\"hop\": 0.01 ,\n",
        "\"nmels\": 40,\n",
        "\"tisv_frame\": 180 ,\n",
        "\"hidden\": 768 ,\n",
        "\"num_layer\": 3 ,\n",
        "\"proj\": 256 ,\n",
        "\"model_path\": 'model' ,\n",
        "\"N\" : 4 ,\n",
        "\"M\" : 5 ,\n",
        "\"num_workers\": 0 ,\n",
        "\"lr\": 0.01 ,\n",
        "\"epochs\": 450 ,\n",
        "\"log_interval\": 30 ,\n",
        "\"log_file\": '/content/speech_id_checkpoint/Stats',\n",
        "\"checkpoint_interval\": 120 ,\n",
        "\"checkpoint_dir\": '/content/speech_id_checkpoint',\n",
        "\"restore\": False ,\n",
        "\"N\" : 4 ,\n",
        "\"M\" : 6 ,\n",
        "\"num_workers\": 8,\n",
        "\"epochs_t\": 10 }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a set of functions for speaker embedding analysis and loss calculation in the context of speaker diarization.\n",
        "* Processes an audio file, extracts spectrograms and features.\n",
        "* Uses Librosa for audio processing.\n",
        "* Returns Mel-frequency cepstral coefficients (MFCCs), Mel spectrogram, and magnitude spectrogram."
      ],
      "metadata": {
        "id": "eYcrMeq-s7LH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQutuf3DcYQ_"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.autograd as grad\n",
        "import torch.nn.functional as F\n",
        "def get_centroids(embeddings):\n",
        "    centroids = []\n",
        "    for speaker in embeddings:\n",
        "        centroid = 0\n",
        "        for utterance in speaker:\n",
        "            centroid = centroid + utterance\n",
        "        centroid = centroid/len(speaker)\n",
        "        centroids.append(centroid)\n",
        "    centroids = torch.stack(centroids)\n",
        "    return centroids\n",
        "\n",
        "def get_centroid(embeddings, speaker_num, utterance_num):\n",
        "    centroid = 0\n",
        "    for utterance_id, utterance in enumerate(embeddings[speaker_num]):\n",
        "        if utterance_id == utterance_num:\n",
        "            continue\n",
        "        centroid = centroid + utterance\n",
        "    centroid = centroid/(len(embeddings[speaker_num])-1)\n",
        "    return centroid\n",
        "\n",
        "def get_cossim(embeddings, centroids):\n",
        "    # Calculates cosine similarity matrix. Requires (N, M, feature) input\n",
        "    cossim = torch.zeros(embeddings.size(0),embeddings.size(1),centroids.size(0))\n",
        "    for speaker_num, speaker in enumerate(embeddings):\n",
        "        for utterance_num, utterance in enumerate(speaker):\n",
        "            for centroid_num, centroid in enumerate(centroids):\n",
        "                if speaker_num == centroid_num:\n",
        "                    centroid = get_centroid(embeddings, speaker_num, utterance_num)\n",
        "                output = F.cosine_similarity(utterance,centroid,dim=0)+1e-6\n",
        "                cossim[speaker_num][utterance_num][centroid_num] = output\n",
        "    return cossim\n",
        "\n",
        "def calc_loss(sim_matrix):\n",
        "    # Calculates loss from (N, M, K) similarity matrix\n",
        "    per_embedding_loss = torch.zeros(sim_matrix.size(0), sim_matrix.size(1))\n",
        "    for j in range(len(sim_matrix)):\n",
        "        for i in range(sim_matrix.size(1)):\n",
        "            per_embedding_loss[j][i] = -(sim_matrix[j][i][j] - ((torch.exp(sim_matrix[j][i]).sum()+1e-6).log_()))\n",
        "    loss = per_embedding_loss.sum()\n",
        "    return loss, per_embedding_loss\n",
        "\n",
        "def normalize_0_1(values, max_value, min_value):\n",
        "    normalized = np.clip((values - min_value) / (max_value - min_value), 0, 1)\n",
        "    return normalized\n",
        "\n",
        "def mfccs_and_spec(wav_file, wav_process = False, calc_mfccs=False, calc_mag_db=False):\n",
        "    sound_file, _ = librosa.core.load(wav_file, sr=hp.data.sr)\n",
        "    window_length = int(hp.data.window*hp.data.sr)\n",
        "    hop_length = int(hp.data.hop*hp.data.sr)\n",
        "    duration = hp.data.tisv_frame * hp.data.hop + hp.data.window\n",
        "\n",
        "    # Cut silence and fix length\n",
        "    if wav_process == True:\n",
        "        sound_file, index = librosa.effects.trim(sound_file, frame_length=window_length, hop_length=hop_length)\n",
        "        length = int(hp.data.sr * duration)\n",
        "        sound_file = librosa.util.fix_length(sound_file, length)\n",
        "\n",
        "    spec = librosa.stft(sound_file, n_fft=hp.data.nfft, hop_length=hop_length, win_length=window_length)\n",
        "    mag_spec = np.abs(spec)\n",
        "\n",
        "    mel_basis = librosa.filters.mel(hp.data.sr, hp.data.nfft, n_mels=hp.data.nmels)\n",
        "    mel_spec = np.dot(mel_basis, mag_spec)\n",
        "\n",
        "    mag_db = librosa.amplitude_to_db(mag_spec)\n",
        "    #db mel spectrogram\n",
        "    mel_db = librosa.amplitude_to_db(mel_spec).T\n",
        "\n",
        "    mfccs = None\n",
        "    if calc_mfccs:\n",
        "        mfccs = np.dot(librosa.filters.dct(40, mel_db.shape[0]), mel_db).T\n",
        "\n",
        "    return mfccs, mel_db, mag_db\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    w = grad.Variable(torch.tensor(1.0))\n",
        "    b = grad.Variable(torch.tensor(0.0))\n",
        "    embeddings = torch.tensor([[0,1,0],[0,0,1], [0,1,0], [0,1,0], [1,0,0], [1,0,0]]).to(torch.float).reshape(3,2,3)\n",
        "    centroids = get_centroids(embeddings)\n",
        "    cossim = get_cossim(embeddings, centroids)\n",
        "    sim_matrix = w*cossim + b\n",
        "    loss, per_embedding_loss = calc_loss(sim_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code implements Voice Activity Detection (VAD) using the WebRTC VAD library to identify speech segments in an audio file\n",
        "* This script essentially performs voice activity detection on the specified audio file and identifies time intervals and segments where speech is present. * The aggressiveness level (parameter to VAD) determines the sensitivity of the VAD algorithm to detect speech."
      ],
      "metadata": {
        "id": "M_SXH6hvtAs-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJi3dXl1cYX9"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import contextlib\n",
        "import numpy as np\n",
        "import sys\n",
        "import librosa\n",
        "import wave\n",
        "\n",
        "import webrtcvad\n",
        "\n",
        "class Frame(object):\n",
        "    def __init__(self, bytes, timestamp, duration):\n",
        "        self.bytes = bytes\n",
        "        self.timestamp = timestamp\n",
        "        self.duration = duration\n",
        "\n",
        "def read_wave(path, sr):\n",
        "    \"\"\"Reads a .wav file.\n",
        "    Takes the path, and returns (PCM audio data, sample rate).\n",
        "    Assumes sample width == 2\n",
        "    \"\"\"\n",
        "    with contextlib.closing(wave.open(path, 'rb')) as wf:\n",
        "        num_channels = wf.getnchannels()\n",
        "        sample_width = wf.getsampwidth()\n",
        "        assert sample_width == 2\n",
        "        sample_rate = wf.getframerate()\n",
        "        pcm_data = wf.readframes(wf.getnframes())\n",
        "        pcm_data = np.frombuffer(pcm_data, dtype=np.int16)  # Convert bytes to numpy array\n",
        "    assert sr in (8000, 16000, 32000, 48000)\n",
        "    return pcm_data, sample_rate\n",
        "\n",
        "def frame_generator(frame_duration_ms, audio, sample_rate):\n",
        "    \"\"\"Generates audio frames from PCM audio data.\n",
        "    Takes the desired frame duration in milliseconds, the PCM data, and\n",
        "    the sample rate.\n",
        "    Yields Frames of the requested duration.\n",
        "    \"\"\"\n",
        "    n = int(sample_rate * (frame_duration_ms / 1000.0) * 2)\n",
        "    offset = 0\n",
        "    timestamp = 0.0\n",
        "    duration = (float(n) / sample_rate) / 2.0\n",
        "    while offset + n < len(audio):\n",
        "        yield Frame(audio[offset:offset + n], timestamp, duration)\n",
        "        timestamp += duration\n",
        "        offset += n\n",
        "\n",
        "def vad_collector(sample_rate, frame_duration_ms,\n",
        "                  padding_duration_ms, vad, frames):\n",
        "    \"\"\"Filters out non-voiced audio frames.\n",
        "    Given a webrtcvad.Vad and a source of audio frames, yields only\n",
        "    the voiced audio.\n",
        "    Uses a padded, sliding window algorithm over the audio frames.\n",
        "    When more than 90% of the frames in the window are voiced (as\n",
        "    reported by the VAD), the collector triggers and begins yielding\n",
        "    audio frames. Then the collector waits until 90% of the frames in\n",
        "    the window are unvoiced to detrigger.\n",
        "    The window is padded at the front and back to provide a small\n",
        "    amount of silence or the beginnings/endings of speech around the\n",
        "    voiced frames.\n",
        "    Arguments:\n",
        "    sample_rate - The audio sample rate, in Hz.\n",
        "    frame_duration_ms - The frame duration in milliseconds.\n",
        "    padding_duration_ms - The amount to pad the window, in milliseconds.\n",
        "    vad - An instance of webrtcvad.Vad.\n",
        "    frames - a source of audio frames (sequence or generator).\n",
        "    Returns: A generator that yields PCM audio data.\n",
        "    \"\"\"\n",
        "    num_padding_frames = int(padding_duration_ms / frame_duration_ms)\n",
        "    # We use a deque for our sliding window/ring buffer.\n",
        "    ring_buffer = collections.deque(maxlen=num_padding_frames)\n",
        "    # We have two states: TRIGGERED and NOTTRIGGERED. We start in the\n",
        "    # NOTTRIGGERED state.\n",
        "    triggered = False\n",
        "\n",
        "    voiced_frames = []\n",
        "    for frame in frames:\n",
        "        is_speech = vad.is_speech(frame.bytes, sample_rate)\n",
        "        if not triggered:\n",
        "            ring_buffer.append((frame, is_speech))\n",
        "            num_voiced = len([f for f, speech in ring_buffer if speech])\n",
        "            # If we're NOTTRIGGERED and more than 90% of the frames in\n",
        "            # the ring buffer are voiced frames, then enter the\n",
        "            # TRIGGERED state.\n",
        "            if num_voiced > 0.9 * ring_buffer.maxlen:\n",
        "                triggered = True\n",
        "                start = ring_buffer[0][0].timestamp\n",
        "                # We want to yield all the audio we see from now until\n",
        "                # we are NOTTRIGGERED, but we have to start with the\n",
        "                # audio that's already in the ring buffer.\n",
        "                for f, s in ring_buffer:\n",
        "                    voiced_frames.append(f)\n",
        "                ring_buffer.clear()\n",
        "        else:\n",
        "            # We're in the TRIGGERED state, so collect the audio data\n",
        "            # and add it to the ring buffer.\n",
        "            voiced_frames.append(frame)\n",
        "            ring_buffer.append((frame, is_speech))\n",
        "            num_unvoiced = len([f for f, speech in ring_buffer if not speech])\n",
        "            # If more than 90% of the frames in the ring buffer are\n",
        "            # unvoiced, then enter NOTTRIGGERED and yield whatever\n",
        "            # audio we've collected.\n",
        "            if num_unvoiced > 0.9 * ring_buffer.maxlen:\n",
        "                triggered = False\n",
        "                yield (start, frame.timestamp + frame.duration)\n",
        "                ring_buffer.clear()\n",
        "                voiced_frames = []\n",
        "    # If we have any leftover voiced audio when we run out of input,\n",
        "    # yield it.\n",
        "    if voiced_frames:\n",
        "        yield (start, frame.timestamp + frame.duration)\n",
        "\n",
        "def VAD_chunk(aggressiveness, path):\n",
        "    audio, _ = read_wave(path, sr)\n",
        "    vad = webrtcvad.Vad(int(aggressiveness))\n",
        "    frames = frame_generator(20, audio, sr)\n",
        "    frames = list(frames)\n",
        "    times = vad_collector(sr, 20, 200, vad, frames)\n",
        "\n",
        "    speech_times = []\n",
        "    speech_segs = []\n",
        "\n",
        "    for i, time in enumerate(times):\n",
        "        start = np.round(time[0], decimals=2)\n",
        "        end = np.round(time[1], decimals=2)\n",
        "        j = start\n",
        "\n",
        "        while j + 0.4 < end:\n",
        "            end_j = np.round(j + 0.4, decimals=2)\n",
        "            speech_times.append((j, end_j))\n",
        "            speech_segs.append(audio[int(j * sr):int(end_j * sr)])\n",
        "            j = end_j\n",
        "        else:\n",
        "            speech_times.append((j, end))\n",
        "            speech_segs.append(audio[int(j * sr):int(end * sr)])\n",
        "\n",
        "    return speech_times, speech_segs\n",
        "\n",
        "# Example usage\n",
        "sr = dict1['sr']\n",
        "speech_times, speech_segs = VAD_chunk(3, \"/content/data/TRAIN/DR8/MRDM0/SI1044.WAV.wav\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are two custom PyTorch datasets (SpeakerDatasetTIMIT and SpeakerDatasetTIMITPreprocessed) designed for speaker diarization tasks, presumably using the TIMIT dataset.\n",
        "\n",
        "SpeakerDatasetTIMIT: This dataset is for unprocessed TIMIT data.\n",
        "\n",
        "Initialization (init):\n",
        "\n",
        "Based on the training flag in dict1, it selects either the training or testing path. Shuffles the list of speaker directories. len: Returns the number of speakers.\n",
        "\n",
        "getitem(self, idx):\n",
        "\n",
        "Retrieves the directory path of a speaker. Randomly selects M utterances from the speaker's directory. For each selected utterance, computes the mel-frequency spectrogram using the mfccs_and_spec function. Returns a PyTorch tensor containing mel-frequency spectrograms of selected utterances for a speaker. SpeakerDatasetTIMITPreprocessed: This dataset is for preprocessed TIMIT data.\n",
        "\n",
        "Initialization (init):\n",
        "\n",
        "Based on the training flag in dict1, it selects either the training or testing path. Sets the number of utterances per speaker (M). Gets the list of files in the specified path. Allows for optional shuffling and starting index for utterances. len: Returns the number of files (speakers).\n",
        "\n",
        "getitem(self, idx):\n",
        "\n",
        "Loads the utterance spectrograms of the selected speaker. If shuffling is enabled, selects M random utterances per speaker; otherwise, selects a contiguous subset. Trims each utterance to a fixed length (160 frames). Transposes the dimensions for PyTorch compatibility. Returns a PyTorch tensor containing the selected utterances for a speaker.\n",
        "\n"
      ],
      "metadata": {
        "id": "aBqzppBptF1d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yf7Xgs4KcYbj"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "from random import shuffle\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SpeakerDatasetTIMIT(Dataset):\n",
        "    def __init__(self,dict1):\n",
        "\n",
        "        if dict1['training']:\n",
        "            self.path = dict1['train_path_unprocessed']\n",
        "            self.utterance_number = dict1['M']\n",
        "        else:\n",
        "            self.path = dict1['test_path_unprocessed']\n",
        "            self.utterance_number = dict1['M']\n",
        "        self.speakers = glob.glob(os.path.dirname(self.path))\n",
        "        shuffle(self.speakers)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.speakers)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        speaker = self.speakers[idx]\n",
        "        wav_files = glob.glob(speaker+'/*.WAV')\n",
        "        shuffle(wav_files)\n",
        "        wav_files = wav_files[0:self.utterance_number]\n",
        "\n",
        "        mel_dbs = []\n",
        "        for f in wav_files:\n",
        "            _, mel_db, _ = mfccs_and_spec(f, wav_process = True)\n",
        "            mel_dbs.append(mel_db)\n",
        "        return torch.Tensor(mel_dbs)\n",
        "\n",
        "class SpeakerDatasetTIMITPreprocessed(Dataset):\n",
        "\n",
        "    def __init__(self,dict1, shuffle=True, utter_start=0):\n",
        "\n",
        "        # data path\n",
        "        if dict1['training']:\n",
        "            self.path = dict1['train_path']\n",
        "            self.utter_num = dict1['M']\n",
        "        else:\n",
        "            self.path = dict1['test_path']\n",
        "            self.utter_num = dict1['M']\n",
        "        self.file_list = os.listdir(self.path)\n",
        "        self.shuffle=shuffle\n",
        "        self.utter_start = utter_start\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        np_file_list = os.listdir(self.path)\n",
        "\n",
        "        if self.shuffle:\n",
        "            selected_file = random.sample(np_file_list, 1)[0]  # select random speaker\n",
        "        else:\n",
        "            selected_file = np_file_list[idx]\n",
        "\n",
        "        utters = np.load(os.path.join(self.path, selected_file))        # load utterance spectrogram of selected speaker\n",
        "        if self.shuffle:\n",
        "            utter_index = np.random.randint(0, utters.shape[0], self.utter_num)   # select M utterances per speaker\n",
        "            utterance = utters[utter_index]\n",
        "        else:\n",
        "            utterance = utters[self.utter_start: self.utter_start+self.utter_num] # utterances of a speaker [batch(M), n_mels, frames]\n",
        "\n",
        "        utterance = utterance[:,:,:160]               # TODO implement variable length batch size\n",
        "\n",
        "        utterance = torch.tensor(np.transpose(utterance, axes=(0,2,1)))     # transpose [batch, frames, n_mels]\n",
        "        return utterance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKdgRtf1cYfo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "class SpeechEmbedder(nn.Module):\n",
        "\n",
        "    def __init__(self,dict1):\n",
        "        super(SpeechEmbedder, self).__init__()\n",
        "        self.LSTM_stack = nn.LSTM(dict1['nmels'], dict1['hidden'], dict1['num_layer'], batch_first=True)\n",
        "        for name, param in self.LSTM_stack.named_parameters():\n",
        "          if 'bias' in name:\n",
        "             nn.init.constant_(param, 0.0)\n",
        "          elif 'weight' in name:\n",
        "             nn.init.xavier_normal_(param)\n",
        "        self.projection = nn.Linear(dict1['hidden'], dict1['proj'])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _ = self.LSTM_stack(x.float()) #(batch, frames, n_mels)\n",
        "        #only use last frame\n",
        "        x = x[:,x.size(1)-1]\n",
        "        x = self.projection(x.float())\n",
        "        x = x / torch.norm(x, dim=1).unsqueeze(1)\n",
        "        return x\n",
        "\n",
        "class GE2ELoss(nn.Module):\n",
        "\n",
        "    def __init__(self, device):\n",
        "        super(GE2ELoss, self).__init__()\n",
        "        self.w = nn.Parameter(torch.tensor(10.0).to(device), requires_grad=True)\n",
        "        self.b = nn.Parameter(torch.tensor(-5.0).to(device), requires_grad=True)\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        torch.clamp(self.w, 1e-6)\n",
        "        centroids = get_centroids(embeddings)\n",
        "        cossim = get_cossim(embeddings, centroids)\n",
        "        sim_matrix = self.w*cossim.to(self.device) + self.b\n",
        "        loss, _ = calc_loss(sim_matrix)\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script appears to preprocess the TIMIT dataset for text-independent speaker verification\n",
        "\n",
        "The script saves the log mel spectrogram of the first and last 180 frames of each partial utterance. This is intended for text-independent speaker verification.\n",
        "The script assumes a specific directory structure for TIMIT speakers, where each speaker has a folder containing their utterances.\n",
        "The train/test split is based on the number of speakers.\n",
        "The configuration parameters (nfft, window, hop, nmels, tisv_frame, etc.) are presumably defined in dict1. Adjustments may be needed based on specific requirements."
      ],
      "metadata": {
        "id": "-7NmP9SOtORO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpZwo55hcYiX",
        "outputId": "ff1a7a60-1108-40d0-c800-9344829c559f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start text-independent utterance feature extraction\n",
            "total speaker number : 462\n",
            "train : 414, test : 48\n",
            "0th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "1th speaker processing...\n",
            "2 (22, 40, 180)\n",
            "2th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "3th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "4th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "5th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "6th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "7th speaker processing...\n",
            "2 (18, 40, 180)\n",
            "8th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "9th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "10th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "11th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "12th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "13th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "14th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "15th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "16th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "17th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "18th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "19th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "20th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "21th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "22th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "23th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "24th speaker processing...\n",
            "2 (18, 40, 180)\n",
            "25th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "26th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "27th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "28th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "29th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "30th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "31th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "32th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "33th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "34th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "35th speaker processing...\n",
            "2 (18, 40, 180)\n",
            "36th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "37th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "38th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "39th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "40th speaker processing...\n",
            "2 (18, 40, 180)\n",
            "41th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "42th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "43th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "44th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "45th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "46th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "47th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "48th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "49th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "50th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "51th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "52th speaker processing...\n",
            "2 (22, 40, 180)\n",
            "53th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "54th speaker processing...\n",
            "2 (22, 40, 180)\n",
            "55th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "56th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "57th speaker processing...\n",
            "2 (18, 40, 180)\n",
            "58th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "59th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "60th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "61th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "62th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "63th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "64th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "65th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "66th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "67th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "68th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "69th speaker processing...\n",
            "2 (18, 40, 180)\n",
            "70th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "71th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "72th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "73th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "74th speaker processing...\n",
            "2 (18, 40, 180)\n",
            "75th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "76th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "77th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "78th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "79th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "80th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "81th speaker processing...\n",
            "2 (22, 40, 180)\n",
            "82th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "83th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "84th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "85th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "86th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "87th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "88th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "89th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "90th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "91th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "92th speaker processing...\n",
            "2 (18, 40, 180)\n",
            "93th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "94th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "95th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "96th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "97th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "98th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "99th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "100th speaker processing...\n",
            "2 (18, 40, 180)\n",
            "101th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "102th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "103th speaker processing...\n",
            "2 (18, 40, 180)\n",
            "104th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "105th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "106th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "107th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "108th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "109th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "110th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "111th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "112th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "113th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "114th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "115th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "116th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "117th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "118th speaker processing...\n",
            "2 (18, 40, 180)\n",
            "119th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "120th speaker processing...\n",
            "2 (22, 40, 180)\n",
            "121th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "122th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "123th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "124th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "125th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "126th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "127th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "128th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "129th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "130th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "131th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "132th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "133th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "134th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "135th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "136th speaker processing...\n",
            "2 (18, 40, 180)\n",
            "137th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "138th speaker processing...\n",
            "2 (18, 40, 180)\n",
            "139th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "140th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "141th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "142th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "143th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "144th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "145th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "146th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "147th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "148th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "149th speaker processing...\n",
            "2 (22, 40, 180)\n",
            "150th speaker processing...\n",
            "2 (24, 40, 180)\n",
            "151th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "152th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "153th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "154th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "155th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "156th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "157th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "158th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "159th speaker processing...\n",
            "2 (22, 40, 180)\n",
            "160th speaker processing...\n",
            "2 (22, 40, 180)\n",
            "161th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "162th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "163th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "164th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "165th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "166th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "167th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "168th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "169th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "170th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "171th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "172th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "173th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "174th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "175th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "176th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "177th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "178th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "179th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "180th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "181th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "182th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "183th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "184th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "185th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "186th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "187th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "188th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "189th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "190th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "191th speaker processing...\n",
            "2 (18, 40, 180)\n",
            "192th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "193th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "194th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "195th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "196th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "197th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "198th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "199th speaker processing...\n",
            "2 (18, 40, 180)\n",
            "200th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "201th speaker processing...\n",
            "2 (18, 40, 180)\n",
            "202th speaker processing...\n",
            "2 (18, 40, 180)\n",
            "203th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "204th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "205th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "206th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "207th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "208th speaker processing...\n",
            "2 (18, 40, 180)\n",
            "209th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "210th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "211th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "212th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "213th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "214th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "215th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "216th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "217th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "218th speaker processing...\n",
            "2 (22, 40, 180)\n",
            "219th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "220th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "221th speaker processing...\n",
            "2 (18, 40, 180)\n",
            "222th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "223th speaker processing...\n",
            "2 (18, 40, 180)\n",
            "224th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "225th speaker processing...\n",
            "2 (22, 40, 180)\n",
            "226th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "227th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "228th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "229th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "230th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "231th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "232th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "233th speaker processing...\n",
            "2 (18, 40, 180)\n",
            "234th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "235th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "236th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "237th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "238th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "239th speaker processing...\n",
            "2 (22, 40, 180)\n",
            "240th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "241th speaker processing...\n",
            "2 (22, 40, 180)\n",
            "242th speaker processing...\n",
            "2 (22, 40, 180)\n",
            "243th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "244th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "245th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "246th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "247th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "248th speaker processing...\n",
            "2 (22, 40, 180)\n",
            "249th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "250th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "251th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "252th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "253th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "254th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "255th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "256th speaker processing...\n",
            "2 (18, 40, 180)\n",
            "257th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "258th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "259th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "260th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "261th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "262th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "263th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "264th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "265th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "266th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "267th speaker processing...\n",
            "2 (18, 40, 180)\n",
            "268th speaker processing...\n",
            "2 (18, 40, 180)\n",
            "269th speaker processing...\n",
            "2 (18, 40, 180)\n",
            "270th speaker processing...\n",
            "2 (18, 40, 180)\n",
            "271th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "272th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "273th speaker processing...\n",
            "2 (18, 40, 180)\n",
            "274th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "275th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "276th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "277th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "278th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "279th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "280th speaker processing...\n",
            "2 (18, 40, 180)\n",
            "281th speaker processing...\n",
            "2 (18, 40, 180)\n",
            "282th speaker processing...\n",
            "2 (22, 40, 180)\n",
            "283th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "284th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "285th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "286th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "287th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "288th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "289th speaker processing...\n",
            "2 (22, 40, 180)\n",
            "290th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "291th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "292th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "293th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "294th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "295th speaker processing...\n",
            "2 (18, 40, 180)\n",
            "296th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "297th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "298th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "299th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "300th speaker processing...\n",
            "2 (18, 40, 180)\n",
            "301th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "302th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "303th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "304th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "305th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "306th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "307th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "308th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "309th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "310th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "311th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "312th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "313th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "314th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "315th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "316th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "317th speaker processing...\n",
            "2 (18, 40, 180)\n",
            "318th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "319th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "320th speaker processing...\n",
            "2 (18, 40, 180)\n",
            "321th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "322th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "323th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "324th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "325th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "326th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "327th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "328th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "329th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "330th speaker processing...\n",
            "2 (18, 40, 180)\n",
            "331th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "332th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "333th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "334th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "335th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "336th speaker processing...\n",
            "2 (18, 40, 180)\n",
            "337th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "338th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "339th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "340th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "341th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "342th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "343th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "344th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "345th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "346th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "347th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "348th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "349th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "350th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "351th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "352th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "353th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "354th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "355th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "356th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "357th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "358th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "359th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "360th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "361th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "362th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "363th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "364th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "365th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "366th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "367th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "368th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "369th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "370th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "371th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "372th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "373th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "374th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "375th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "376th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "377th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "378th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "379th speaker processing...\n",
            "2 (18, 40, 180)\n",
            "380th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "381th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "382th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "383th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "384th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "385th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "386th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "387th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "388th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "389th speaker processing...\n",
            "2 (18, 40, 180)\n",
            "390th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "391th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "392th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "393th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "394th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "395th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "396th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "397th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "398th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "399th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "400th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "401th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "402th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "403th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "404th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "405th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "406th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "407th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "408th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "409th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "410th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "411th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "412th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "413th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "414th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "415th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "416th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "417th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "418th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "419th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "420th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "421th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "422th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "423th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "424th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "425th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "426th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "427th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "428th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "429th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "430th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "431th speaker processing...\n",
            "2 (18, 40, 180)\n",
            "432th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "433th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "434th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "435th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "436th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "437th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "438th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "439th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "440th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "441th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "442th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "443th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "444th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "445th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "446th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "447th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "448th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "449th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "450th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "451th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "452th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "453th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "454th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "455th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "456th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "457th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "458th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "459th speaker processing...\n",
            "2 (20, 40, 180)\n",
            "460th speaker processing...\n",
            "2 (22, 40, 180)\n",
            "461th speaker processing...\n",
            "2 (20, 40, 180)\n"
          ]
        }
      ],
      "source": [
        "audio_path = glob.glob(os.path.dirname(dict1['unprocessed_data']))\n",
        "\n",
        "def save_spectrogram_tisv(dict1):\n",
        "    \"\"\" Full preprocess of text-independent utterance. The log-mel-spectrogram is saved as a numpy file.\n",
        "        Each partial utterance is split by voice detection using DB\n",
        "        and the first and the last 180 frames from each partial utterance are saved.\n",
        "        Need: utterance data set (TIMIT)\n",
        "    \"\"\"\n",
        "    print(\"start text-independent utterance feature extraction\")\n",
        "    os.makedirs(dict1['train_path'], exist_ok=True)   # make folder to save train file\n",
        "    utter_min_len = (dict1['tisv_frame'] * dict1['hop'] + dict1['window']) * dict1['sr']   # lower bound of utterance length\n",
        "    total_speaker_num = len(audio_path)\n",
        "    train_speaker_num = (total_speaker_num // 10) * 9  # split total data 90% train and 10% test\n",
        "    print(\"total speaker number : %d\" % total_speaker_num)\n",
        "    print(\"train : %d, test : %d\" % (train_speaker_num, total_speaker_num - train_speaker_num))\n",
        "    for i, folder in enumerate(audio_path):\n",
        "        print(\"%dth speaker processing...\" % i)\n",
        "        utterances_spec = []\n",
        "        for utter_name in os.listdir(folder):\n",
        "            if utter_name[-4:] == '.wav':\n",
        "                utter_path = os.path.join(folder, utter_name)  # path of each utterance\n",
        "                utter, _ = librosa.core.load(utter_path)  # load utterance audio\n",
        "                intervals = librosa.effects.split(utter, top_db=60)  # voice activity detection\n",
        "                for interval in intervals:\n",
        "                    if (interval[1] - interval[0]) > utter_min_len:  # If partial utterance is sufficiently long,\n",
        "                        utter_part = utter[interval[0]:interval[1]]  # save first and last 180 frames of spectrogram.\n",
        "                        S = librosa.core.stft(y=utter_part, n_fft=dict1['nfft'],\n",
        "                                              win_length=int(dict1['window'] * dict1['sr']),\n",
        "                                              hop_length=int(dict1['hop'] * dict1['sr']))\n",
        "                        S = np.abs(S) ** 2\n",
        "                        mel_basis = librosa.filters.mel(sr=dict1['sr'], n_fft=dict1['nfft'], n_mels=dict1['nmels'])\n",
        "                        S = np.log10(np.dot(mel_basis, S) + 1e-6)  # log mel spectrogram of utterances\n",
        "                        utterances_spec.append(S[:, :dict1['tisv_frame']])  # first 180 frames of partial utterance\n",
        "                        utterances_spec.append(S[:, -dict1['tisv_frame']:])  # last 180 frames of partial utterance\n",
        "\n",
        "        utterances_spec = np.array(utterances_spec)\n",
        "        print(\"2\", utterances_spec.shape)\n",
        "        if i < train_speaker_num:  # save spectrogram as numpy file\n",
        "            np.save(os.path.join(dict1['train_path'], \"speaker%d.npy\" % i), utterances_spec)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    save_spectrogram_tisv(dict1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script trains a speech embedding model using the GE2E loss for text-independent speaker verification.\n",
        "* The script uses the SpeechEmbedder and GE2ELoss classes defined earlier.\n",
        "* The training process involves iterating through batches, computing embeddings, and optimizing the model using the GE2E loss.\n",
        "* The final trained model is saved at the specified checkpoint directory."
      ],
      "metadata": {
        "id": "FKpNYVl2tS7G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpFsAK4XcY1p",
        "outputId": "cbbe4c43-26c9-4ba2-9585-740ccd743acc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<torch.utils.data.dataloader.DataLoader object at 0x7c8c1f5f6980>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Dec 21 07:41:05 2023\tEpoch:1[30/103],Iteration:30\tLoss:33.4674\tTLoss:32.9911\t\n",
            "\n",
            "Thu Dec 21 07:41:11 2023\tEpoch:1[60/103],Iteration:60\tLoss:26.2805\tTLoss:32.7670\t\n",
            "\n",
            "Thu Dec 21 07:41:17 2023\tEpoch:1[90/103],Iteration:90\tLoss:33.5739\tTLoss:32.8087\t\n",
            "\n",
            "Thu Dec 21 07:41:25 2023\tEpoch:2[30/103],Iteration:133\tLoss:26.3699\tTLoss:31.1960\t\n",
            "\n",
            "Thu Dec 21 07:41:30 2023\tEpoch:2[60/103],Iteration:163\tLoss:31.6539\tTLoss:31.4962\t\n",
            "\n",
            "Thu Dec 21 07:41:36 2023\tEpoch:2[90/103],Iteration:193\tLoss:32.6031\tTLoss:31.5179\t\n",
            "\n",
            "Thu Dec 21 07:41:45 2023\tEpoch:3[30/103],Iteration:236\tLoss:32.4745\tTLoss:31.2964\t\n",
            "\n",
            "Thu Dec 21 07:41:50 2023\tEpoch:3[60/103],Iteration:266\tLoss:32.2766\tTLoss:31.1024\t\n",
            "\n",
            "Thu Dec 21 07:41:56 2023\tEpoch:3[90/103],Iteration:296\tLoss:31.9336\tTLoss:30.8861\t\n",
            "\n",
            "Thu Dec 21 07:42:05 2023\tEpoch:4[30/103],Iteration:339\tLoss:29.4208\tTLoss:30.5334\t\n",
            "\n",
            "Thu Dec 21 07:42:11 2023\tEpoch:4[60/103],Iteration:369\tLoss:32.8827\tTLoss:30.4046\t\n",
            "\n",
            "Thu Dec 21 07:42:16 2023\tEpoch:4[90/103],Iteration:399\tLoss:33.5657\tTLoss:30.3520\t\n",
            "\n",
            "Thu Dec 21 07:42:25 2023\tEpoch:5[30/103],Iteration:442\tLoss:30.1353\tTLoss:31.0261\t\n",
            "\n",
            "Thu Dec 21 07:42:31 2023\tEpoch:5[60/103],Iteration:472\tLoss:29.5207\tTLoss:30.6089\t\n",
            "\n",
            "Thu Dec 21 07:42:36 2023\tEpoch:5[90/103],Iteration:502\tLoss:27.1319\tTLoss:30.3017\t\n",
            "\n",
            "Thu Dec 21 07:42:46 2023\tEpoch:6[30/103],Iteration:545\tLoss:33.8033\tTLoss:30.3779\t\n",
            "\n",
            "Thu Dec 21 07:42:52 2023\tEpoch:6[60/103],Iteration:575\tLoss:31.1278\tTLoss:29.6934\t\n",
            "\n",
            "Thu Dec 21 07:42:57 2023\tEpoch:6[90/103],Iteration:605\tLoss:29.6327\tTLoss:29.4881\t\n",
            "\n",
            "Thu Dec 21 07:43:06 2023\tEpoch:7[30/103],Iteration:648\tLoss:29.2921\tTLoss:28.8704\t\n",
            "\n",
            "Thu Dec 21 07:43:12 2023\tEpoch:7[60/103],Iteration:678\tLoss:29.0384\tTLoss:28.2894\t\n",
            "\n",
            "Thu Dec 21 07:43:18 2023\tEpoch:7[90/103],Iteration:708\tLoss:25.6949\tTLoss:28.1262\t\n",
            "\n",
            "Thu Dec 21 07:43:27 2023\tEpoch:8[30/103],Iteration:751\tLoss:27.2813\tTLoss:26.6204\t\n",
            "\n",
            "Thu Dec 21 07:43:33 2023\tEpoch:8[60/103],Iteration:781\tLoss:16.9369\tTLoss:26.6868\t\n",
            "\n",
            "Thu Dec 21 07:43:38 2023\tEpoch:8[90/103],Iteration:811\tLoss:25.0005\tTLoss:26.3609\t\n",
            "\n",
            "Thu Dec 21 07:43:48 2023\tEpoch:9[30/103],Iteration:854\tLoss:25.8992\tTLoss:25.1732\t\n",
            "\n",
            "Thu Dec 21 07:43:54 2023\tEpoch:9[60/103],Iteration:884\tLoss:21.6119\tTLoss:25.2348\t\n",
            "\n",
            "Thu Dec 21 07:44:00 2023\tEpoch:9[90/103],Iteration:914\tLoss:29.4193\tTLoss:25.0303\t\n",
            "\n",
            "Thu Dec 21 07:44:09 2023\tEpoch:10[30/103],Iteration:957\tLoss:17.9111\tTLoss:25.5673\t\n",
            "\n",
            "Thu Dec 21 07:44:15 2023\tEpoch:10[60/103],Iteration:987\tLoss:16.8129\tTLoss:24.8476\t\n",
            "\n",
            "Thu Dec 21 07:44:20 2023\tEpoch:10[90/103],Iteration:1017\tLoss:21.0104\tTLoss:24.3080\t\n",
            "\n",
            "Thu Dec 21 07:44:29 2023\tEpoch:11[30/103],Iteration:1060\tLoss:19.1325\tTLoss:21.6036\t\n",
            "\n",
            "Thu Dec 21 07:44:35 2023\tEpoch:11[60/103],Iteration:1090\tLoss:27.4118\tTLoss:21.3038\t\n",
            "\n",
            "Thu Dec 21 07:44:41 2023\tEpoch:11[90/103],Iteration:1120\tLoss:21.2640\tTLoss:21.9563\t\n",
            "\n",
            "Thu Dec 21 07:44:51 2023\tEpoch:12[30/103],Iteration:1163\tLoss:20.0476\tTLoss:22.6057\t\n",
            "\n",
            "Thu Dec 21 07:44:57 2023\tEpoch:12[60/103],Iteration:1193\tLoss:29.8464\tTLoss:22.8362\t\n",
            "\n",
            "Thu Dec 21 07:45:03 2023\tEpoch:12[90/103],Iteration:1223\tLoss:23.5715\tTLoss:22.5598\t\n",
            "\n",
            "Thu Dec 21 07:45:11 2023\tEpoch:13[30/103],Iteration:1266\tLoss:20.6956\tTLoss:21.3704\t\n",
            "\n",
            "Thu Dec 21 07:45:18 2023\tEpoch:13[60/103],Iteration:1296\tLoss:14.6507\tTLoss:20.7994\t\n",
            "\n",
            "Thu Dec 21 07:45:23 2023\tEpoch:13[90/103],Iteration:1326\tLoss:19.6998\tTLoss:20.6232\t\n",
            "\n",
            "Thu Dec 21 07:45:33 2023\tEpoch:14[30/103],Iteration:1369\tLoss:27.0563\tTLoss:20.8283\t\n",
            "\n",
            "Thu Dec 21 07:45:39 2023\tEpoch:14[60/103],Iteration:1399\tLoss:17.8051\tTLoss:20.4867\t\n",
            "\n",
            "Thu Dec 21 07:45:44 2023\tEpoch:14[90/103],Iteration:1429\tLoss:13.0456\tTLoss:19.7803\t\n",
            "\n",
            "Thu Dec 21 07:45:54 2023\tEpoch:15[30/103],Iteration:1472\tLoss:14.1084\tTLoss:21.8261\t\n",
            "\n",
            "Thu Dec 21 07:45:59 2023\tEpoch:15[60/103],Iteration:1502\tLoss:10.5200\tTLoss:20.4974\t\n",
            "\n",
            "Thu Dec 21 07:46:06 2023\tEpoch:15[90/103],Iteration:1532\tLoss:22.2843\tTLoss:20.1274\t\n",
            "\n",
            "Thu Dec 21 07:46:15 2023\tEpoch:16[30/103],Iteration:1575\tLoss:20.2379\tTLoss:20.4100\t\n",
            "\n",
            "Thu Dec 21 07:46:21 2023\tEpoch:16[60/103],Iteration:1605\tLoss:20.9604\tTLoss:19.9986\t\n",
            "\n",
            "Thu Dec 21 07:46:26 2023\tEpoch:16[90/103],Iteration:1635\tLoss:13.7384\tTLoss:20.4371\t\n",
            "\n",
            "Thu Dec 21 07:46:36 2023\tEpoch:17[30/103],Iteration:1678\tLoss:24.4448\tTLoss:20.6854\t\n",
            "\n",
            "Thu Dec 21 07:46:42 2023\tEpoch:17[60/103],Iteration:1708\tLoss:28.1023\tTLoss:21.3192\t\n",
            "\n",
            "Thu Dec 21 07:46:47 2023\tEpoch:17[90/103],Iteration:1738\tLoss:15.7098\tTLoss:20.2559\t\n",
            "\n",
            "Thu Dec 21 07:46:58 2023\tEpoch:18[30/103],Iteration:1781\tLoss:17.7062\tTLoss:18.5097\t\n",
            "\n",
            "Thu Dec 21 07:47:04 2023\tEpoch:18[60/103],Iteration:1811\tLoss:25.8412\tTLoss:18.1746\t\n",
            "\n",
            "Thu Dec 21 07:47:10 2023\tEpoch:18[90/103],Iteration:1841\tLoss:26.9257\tTLoss:18.2541\t\n",
            "\n",
            "Thu Dec 21 07:47:19 2023\tEpoch:19[30/103],Iteration:1884\tLoss:12.6431\tTLoss:18.4552\t\n",
            "\n",
            "Thu Dec 21 07:47:25 2023\tEpoch:19[60/103],Iteration:1914\tLoss:3.2803\tTLoss:18.3115\t\n",
            "\n",
            "Thu Dec 21 07:47:31 2023\tEpoch:19[90/103],Iteration:1944\tLoss:18.9148\tTLoss:18.1871\t\n",
            "\n",
            "Thu Dec 21 07:47:41 2023\tEpoch:20[30/103],Iteration:1987\tLoss:8.0714\tTLoss:15.3969\t\n",
            "\n",
            "Thu Dec 21 07:47:47 2023\tEpoch:20[60/103],Iteration:2017\tLoss:29.4371\tTLoss:16.3248\t\n",
            "\n",
            "Thu Dec 21 07:47:53 2023\tEpoch:20[90/103],Iteration:2047\tLoss:19.5051\tTLoss:16.3298\t\n",
            "\n",
            "Thu Dec 21 07:48:01 2023\tEpoch:21[30/103],Iteration:2090\tLoss:13.5392\tTLoss:17.6576\t\n",
            "\n",
            "Thu Dec 21 07:48:07 2023\tEpoch:21[60/103],Iteration:2120\tLoss:6.2006\tTLoss:16.3407\t\n",
            "\n",
            "Thu Dec 21 07:48:13 2023\tEpoch:21[90/103],Iteration:2150\tLoss:15.7068\tTLoss:16.2518\t\n",
            "\n",
            "Thu Dec 21 07:48:23 2023\tEpoch:22[30/103],Iteration:2193\tLoss:12.2387\tTLoss:15.6117\t\n",
            "\n",
            "Thu Dec 21 07:48:29 2023\tEpoch:22[60/103],Iteration:2223\tLoss:13.3491\tTLoss:16.3780\t\n",
            "\n",
            "Thu Dec 21 07:48:34 2023\tEpoch:22[90/103],Iteration:2253\tLoss:12.9582\tTLoss:16.1375\t\n",
            "\n",
            "Thu Dec 21 07:48:44 2023\tEpoch:23[30/103],Iteration:2296\tLoss:19.1271\tTLoss:17.1861\t\n",
            "\n",
            "Thu Dec 21 07:48:49 2023\tEpoch:23[60/103],Iteration:2326\tLoss:17.3261\tTLoss:17.0769\t\n",
            "\n",
            "Thu Dec 21 07:48:56 2023\tEpoch:23[90/103],Iteration:2356\tLoss:18.5299\tTLoss:16.8477\t\n",
            "\n",
            "Thu Dec 21 07:49:05 2023\tEpoch:24[30/103],Iteration:2399\tLoss:8.1149\tTLoss:14.8324\t\n",
            "\n",
            "Thu Dec 21 07:49:11 2023\tEpoch:24[60/103],Iteration:2429\tLoss:12.2252\tTLoss:15.4487\t\n",
            "\n",
            "Thu Dec 21 07:49:17 2023\tEpoch:24[90/103],Iteration:2459\tLoss:12.8114\tTLoss:15.3035\t\n",
            "\n",
            "Thu Dec 21 07:49:26 2023\tEpoch:25[30/103],Iteration:2502\tLoss:21.0657\tTLoss:15.3047\t\n",
            "\n",
            "Thu Dec 21 07:49:32 2023\tEpoch:25[60/103],Iteration:2532\tLoss:12.7050\tTLoss:15.1735\t\n",
            "\n",
            "Thu Dec 21 07:49:37 2023\tEpoch:25[90/103],Iteration:2562\tLoss:15.7050\tTLoss:15.0740\t\n",
            "\n",
            "Thu Dec 21 07:49:48 2023\tEpoch:26[30/103],Iteration:2605\tLoss:15.3952\tTLoss:14.7880\t\n",
            "\n",
            "Thu Dec 21 07:49:53 2023\tEpoch:26[60/103],Iteration:2635\tLoss:19.4673\tTLoss:14.7842\t\n",
            "\n",
            "Thu Dec 21 07:49:59 2023\tEpoch:26[90/103],Iteration:2665\tLoss:13.0093\tTLoss:14.2433\t\n",
            "\n",
            "Thu Dec 21 07:50:08 2023\tEpoch:27[30/103],Iteration:2708\tLoss:18.6239\tTLoss:16.1459\t\n",
            "\n",
            "Thu Dec 21 07:50:15 2023\tEpoch:27[60/103],Iteration:2738\tLoss:22.4664\tTLoss:15.1869\t\n",
            "\n",
            "Thu Dec 21 07:50:20 2023\tEpoch:27[90/103],Iteration:2768\tLoss:19.1206\tTLoss:14.3681\t\n",
            "\n",
            "Thu Dec 21 07:50:30 2023\tEpoch:28[30/103],Iteration:2811\tLoss:22.9696\tTLoss:13.9176\t\n",
            "\n",
            "Thu Dec 21 07:50:36 2023\tEpoch:28[60/103],Iteration:2841\tLoss:25.5429\tTLoss:14.1103\t\n",
            "\n",
            "Thu Dec 21 07:50:42 2023\tEpoch:28[90/103],Iteration:2871\tLoss:16.2778\tTLoss:13.9104\t\n",
            "\n",
            "Thu Dec 21 07:50:51 2023\tEpoch:29[30/103],Iteration:2914\tLoss:15.7780\tTLoss:15.0837\t\n",
            "\n",
            "Thu Dec 21 07:50:57 2023\tEpoch:29[60/103],Iteration:2944\tLoss:16.2875\tTLoss:14.3110\t\n",
            "\n",
            "Thu Dec 21 07:51:03 2023\tEpoch:29[90/103],Iteration:2974\tLoss:16.4968\tTLoss:13.4432\t\n",
            "\n",
            "Thu Dec 21 07:51:13 2023\tEpoch:30[30/103],Iteration:3017\tLoss:13.0926\tTLoss:13.2351\t\n",
            "\n",
            "Thu Dec 21 07:51:19 2023\tEpoch:30[60/103],Iteration:3047\tLoss:10.1767\tTLoss:12.7934\t\n",
            "\n",
            "Thu Dec 21 07:51:24 2023\tEpoch:30[90/103],Iteration:3077\tLoss:13.9240\tTLoss:12.9521\t\n",
            "\n",
            "Thu Dec 21 07:51:34 2023\tEpoch:31[30/103],Iteration:3120\tLoss:5.7228\tTLoss:12.9339\t\n",
            "\n",
            "Thu Dec 21 07:51:39 2023\tEpoch:31[60/103],Iteration:3150\tLoss:26.3463\tTLoss:12.1292\t\n",
            "\n",
            "Thu Dec 21 07:51:45 2023\tEpoch:31[90/103],Iteration:3180\tLoss:15.8692\tTLoss:12.9873\t\n",
            "\n",
            "Thu Dec 21 07:51:55 2023\tEpoch:32[30/103],Iteration:3223\tLoss:15.3111\tTLoss:11.5902\t\n",
            "\n",
            "Thu Dec 21 07:52:01 2023\tEpoch:32[60/103],Iteration:3253\tLoss:1.3179\tTLoss:12.0172\t\n",
            "\n",
            "Thu Dec 21 07:52:07 2023\tEpoch:32[90/103],Iteration:3283\tLoss:6.9204\tTLoss:11.9893\t\n",
            "\n",
            "Thu Dec 21 07:52:16 2023\tEpoch:33[30/103],Iteration:3326\tLoss:3.8886\tTLoss:12.1642\t\n",
            "\n",
            "Thu Dec 21 07:52:22 2023\tEpoch:33[60/103],Iteration:3356\tLoss:18.6449\tTLoss:11.9309\t\n",
            "\n",
            "Thu Dec 21 07:52:28 2023\tEpoch:33[90/103],Iteration:3386\tLoss:8.7103\tTLoss:12.0775\t\n",
            "\n",
            "Thu Dec 21 07:52:38 2023\tEpoch:34[30/103],Iteration:3429\tLoss:1.5095\tTLoss:15.0323\t\n",
            "\n",
            "Thu Dec 21 07:52:44 2023\tEpoch:34[60/103],Iteration:3459\tLoss:9.8260\tTLoss:13.3119\t\n",
            "\n",
            "Thu Dec 21 07:52:50 2023\tEpoch:34[90/103],Iteration:3489\tLoss:10.6676\tTLoss:12.4985\t\n",
            "\n",
            "Thu Dec 21 07:52:58 2023\tEpoch:35[30/103],Iteration:3532\tLoss:12.2053\tTLoss:11.8632\t\n",
            "\n",
            "Thu Dec 21 07:53:04 2023\tEpoch:35[60/103],Iteration:3562\tLoss:21.3420\tTLoss:10.6742\t\n",
            "\n",
            "Thu Dec 21 07:53:10 2023\tEpoch:35[90/103],Iteration:3592\tLoss:5.8714\tTLoss:10.7944\t\n",
            "\n",
            "Thu Dec 21 07:53:20 2023\tEpoch:36[30/103],Iteration:3635\tLoss:11.2641\tTLoss:10.5406\t\n",
            "\n",
            "Thu Dec 21 07:53:26 2023\tEpoch:36[60/103],Iteration:3665\tLoss:13.3002\tTLoss:10.8410\t\n",
            "\n",
            "Thu Dec 21 07:53:31 2023\tEpoch:36[90/103],Iteration:3695\tLoss:7.0339\tTLoss:11.1829\t\n",
            "\n",
            "Thu Dec 21 07:53:41 2023\tEpoch:37[30/103],Iteration:3738\tLoss:10.8323\tTLoss:11.6803\t\n",
            "\n",
            "Thu Dec 21 07:53:47 2023\tEpoch:37[60/103],Iteration:3768\tLoss:18.3049\tTLoss:11.2538\t\n",
            "\n",
            "Thu Dec 21 07:53:53 2023\tEpoch:37[90/103],Iteration:3798\tLoss:15.1824\tTLoss:11.1814\t\n",
            "\n",
            "Thu Dec 21 07:54:03 2023\tEpoch:38[30/103],Iteration:3841\tLoss:11.7607\tTLoss:10.4505\t\n",
            "\n",
            "Thu Dec 21 07:54:09 2023\tEpoch:38[60/103],Iteration:3871\tLoss:12.7983\tTLoss:11.1624\t\n",
            "\n",
            "Thu Dec 21 07:54:14 2023\tEpoch:38[90/103],Iteration:3901\tLoss:12.3446\tTLoss:11.1399\t\n",
            "\n",
            "Thu Dec 21 07:54:24 2023\tEpoch:39[30/103],Iteration:3944\tLoss:3.1006\tTLoss:8.4744\t\n",
            "\n",
            "Thu Dec 21 07:54:29 2023\tEpoch:39[60/103],Iteration:3974\tLoss:7.5858\tTLoss:9.5233\t\n",
            "\n",
            "Thu Dec 21 07:54:35 2023\tEpoch:39[90/103],Iteration:4004\tLoss:13.2726\tTLoss:9.6584\t\n",
            "\n",
            "Thu Dec 21 07:54:46 2023\tEpoch:40[30/103],Iteration:4047\tLoss:9.2346\tTLoss:9.5339\t\n",
            "\n",
            "Thu Dec 21 07:54:51 2023\tEpoch:40[60/103],Iteration:4077\tLoss:11.8625\tTLoss:9.7119\t\n",
            "\n",
            "Thu Dec 21 07:54:57 2023\tEpoch:40[90/103],Iteration:4107\tLoss:12.3475\tTLoss:10.3598\t\n",
            "\n",
            "Thu Dec 21 07:55:06 2023\tEpoch:41[30/103],Iteration:4150\tLoss:12.2037\tTLoss:9.4315\t\n",
            "\n",
            "Thu Dec 21 07:55:12 2023\tEpoch:41[60/103],Iteration:4180\tLoss:19.6888\tTLoss:9.6837\t\n",
            "\n",
            "Thu Dec 21 07:55:18 2023\tEpoch:41[90/103],Iteration:4210\tLoss:12.6224\tTLoss:9.6188\t\n",
            "\n",
            "Thu Dec 21 07:55:28 2023\tEpoch:42[30/103],Iteration:4253\tLoss:13.4474\tTLoss:10.9288\t\n",
            "\n",
            "Thu Dec 21 07:55:34 2023\tEpoch:42[60/103],Iteration:4283\tLoss:12.0161\tTLoss:10.3547\t\n",
            "\n",
            "Thu Dec 21 07:55:40 2023\tEpoch:42[90/103],Iteration:4313\tLoss:11.6728\tTLoss:10.1097\t\n",
            "\n",
            "Thu Dec 21 07:55:49 2023\tEpoch:43[30/103],Iteration:4356\tLoss:17.3695\tTLoss:11.2875\t\n",
            "\n",
            "Thu Dec 21 07:55:55 2023\tEpoch:43[60/103],Iteration:4386\tLoss:4.9269\tTLoss:10.5028\t\n",
            "\n",
            "Thu Dec 21 07:56:01 2023\tEpoch:43[90/103],Iteration:4416\tLoss:14.0250\tTLoss:10.4707\t\n",
            "\n",
            "Thu Dec 21 07:56:11 2023\tEpoch:44[30/103],Iteration:4459\tLoss:3.6136\tTLoss:8.3042\t\n",
            "\n",
            "Thu Dec 21 07:56:18 2023\tEpoch:44[60/103],Iteration:4489\tLoss:18.6558\tTLoss:9.0436\t\n",
            "\n",
            "Thu Dec 21 07:56:24 2023\tEpoch:44[90/103],Iteration:4519\tLoss:5.9480\tTLoss:8.7900\t\n",
            "\n",
            "Thu Dec 21 07:56:33 2023\tEpoch:45[30/103],Iteration:4562\tLoss:3.2527\tTLoss:10.2183\t\n",
            "\n",
            "Thu Dec 21 07:56:39 2023\tEpoch:45[60/103],Iteration:4592\tLoss:6.0552\tTLoss:9.8569\t\n",
            "\n",
            "Thu Dec 21 07:56:45 2023\tEpoch:45[90/103],Iteration:4622\tLoss:8.8919\tTLoss:9.0984\t\n",
            "\n",
            "Thu Dec 21 07:56:55 2023\tEpoch:46[30/103],Iteration:4665\tLoss:18.6517\tTLoss:10.7253\t\n",
            "\n",
            "Thu Dec 21 07:57:01 2023\tEpoch:46[60/103],Iteration:4695\tLoss:5.0249\tTLoss:10.6990\t\n",
            "\n",
            "Thu Dec 21 07:57:07 2023\tEpoch:46[90/103],Iteration:4725\tLoss:8.1701\tTLoss:9.9729\t\n",
            "\n",
            "Thu Dec 21 07:57:16 2023\tEpoch:47[30/103],Iteration:4768\tLoss:15.2105\tTLoss:8.7005\t\n",
            "\n",
            "Thu Dec 21 07:57:22 2023\tEpoch:47[60/103],Iteration:4798\tLoss:2.6068\tTLoss:8.7669\t\n",
            "\n",
            "Thu Dec 21 07:57:28 2023\tEpoch:47[90/103],Iteration:4828\tLoss:6.5125\tTLoss:9.2359\t\n",
            "\n",
            "Thu Dec 21 07:57:38 2023\tEpoch:48[30/103],Iteration:4871\tLoss:11.5045\tTLoss:9.4319\t\n",
            "\n",
            "Thu Dec 21 07:57:43 2023\tEpoch:48[60/103],Iteration:4901\tLoss:15.9850\tTLoss:9.9743\t\n",
            "\n",
            "Thu Dec 21 07:57:49 2023\tEpoch:48[90/103],Iteration:4931\tLoss:5.3290\tTLoss:9.2350\t\n",
            "\n",
            "Thu Dec 21 07:57:58 2023\tEpoch:49[30/103],Iteration:4974\tLoss:7.7909\tTLoss:7.8988\t\n",
            "\n",
            "Thu Dec 21 07:58:04 2023\tEpoch:49[60/103],Iteration:5004\tLoss:15.4063\tTLoss:9.0099\t\n",
            "\n",
            "Thu Dec 21 07:58:10 2023\tEpoch:49[90/103],Iteration:5034\tLoss:7.0238\tTLoss:8.7452\t\n",
            "\n",
            "Thu Dec 21 07:58:20 2023\tEpoch:50[30/103],Iteration:5077\tLoss:9.3020\tTLoss:7.6852\t\n",
            "\n",
            "Thu Dec 21 07:58:26 2023\tEpoch:50[60/103],Iteration:5107\tLoss:11.2448\tTLoss:8.3484\t\n",
            "\n",
            "Thu Dec 21 07:58:31 2023\tEpoch:50[90/103],Iteration:5137\tLoss:13.9497\tTLoss:9.3218\t\n",
            "\n",
            "Thu Dec 21 07:58:41 2023\tEpoch:51[30/103],Iteration:5180\tLoss:20.2556\tTLoss:8.7149\t\n",
            "\n",
            "Thu Dec 21 07:58:46 2023\tEpoch:51[60/103],Iteration:5210\tLoss:1.6491\tTLoss:8.8062\t\n",
            "\n",
            "Thu Dec 21 07:58:52 2023\tEpoch:51[90/103],Iteration:5240\tLoss:10.5776\tTLoss:8.8613\t\n",
            "\n",
            "Thu Dec 21 07:59:02 2023\tEpoch:52[30/103],Iteration:5283\tLoss:4.8549\tTLoss:8.6963\t\n",
            "\n",
            "Thu Dec 21 07:59:08 2023\tEpoch:52[60/103],Iteration:5313\tLoss:4.0177\tTLoss:8.0311\t\n",
            "\n",
            "Thu Dec 21 07:59:14 2023\tEpoch:52[90/103],Iteration:5343\tLoss:4.5993\tTLoss:8.5975\t\n",
            "\n",
            "Thu Dec 21 07:59:23 2023\tEpoch:53[30/103],Iteration:5386\tLoss:11.3099\tTLoss:8.0020\t\n",
            "\n",
            "Thu Dec 21 07:59:29 2023\tEpoch:53[60/103],Iteration:5416\tLoss:8.4699\tTLoss:7.7720\t\n",
            "\n",
            "Thu Dec 21 07:59:35 2023\tEpoch:53[90/103],Iteration:5446\tLoss:12.0072\tTLoss:8.3828\t\n",
            "\n",
            "Thu Dec 21 07:59:45 2023\tEpoch:54[30/103],Iteration:5489\tLoss:4.8760\tTLoss:8.0999\t\n",
            "\n",
            "Thu Dec 21 07:59:51 2023\tEpoch:54[60/103],Iteration:5519\tLoss:13.2385\tTLoss:8.0470\t\n",
            "\n",
            "Thu Dec 21 07:59:56 2023\tEpoch:54[90/103],Iteration:5549\tLoss:11.6505\tTLoss:8.7603\t\n",
            "\n",
            "Thu Dec 21 08:00:06 2023\tEpoch:55[30/103],Iteration:5592\tLoss:2.7794\tTLoss:9.5038\t\n",
            "\n",
            "Thu Dec 21 08:00:11 2023\tEpoch:55[60/103],Iteration:5622\tLoss:10.3008\tTLoss:8.9415\t\n",
            "\n",
            "Thu Dec 21 08:00:17 2023\tEpoch:55[90/103],Iteration:5652\tLoss:9.4047\tTLoss:8.9067\t\n",
            "\n",
            "Thu Dec 21 08:00:27 2023\tEpoch:56[30/103],Iteration:5695\tLoss:2.9599\tTLoss:8.5452\t\n",
            "\n",
            "Thu Dec 21 08:00:33 2023\tEpoch:56[60/103],Iteration:5725\tLoss:4.0919\tTLoss:9.1663\t\n",
            "\n",
            "Thu Dec 21 08:00:39 2023\tEpoch:56[90/103],Iteration:5755\tLoss:8.9881\tTLoss:9.3403\t\n",
            "\n",
            "Thu Dec 21 08:00:48 2023\tEpoch:57[30/103],Iteration:5798\tLoss:7.5502\tTLoss:9.3984\t\n",
            "\n",
            "Thu Dec 21 08:00:54 2023\tEpoch:57[60/103],Iteration:5828\tLoss:0.8435\tTLoss:8.8159\t\n",
            "\n",
            "Thu Dec 21 08:01:00 2023\tEpoch:57[90/103],Iteration:5858\tLoss:1.2981\tTLoss:8.8948\t\n",
            "\n",
            "Thu Dec 21 08:01:09 2023\tEpoch:58[30/103],Iteration:5901\tLoss:7.5137\tTLoss:8.1822\t\n",
            "\n",
            "Thu Dec 21 08:01:15 2023\tEpoch:58[60/103],Iteration:5931\tLoss:12.5286\tTLoss:8.4332\t\n",
            "\n",
            "Thu Dec 21 08:01:21 2023\tEpoch:58[90/103],Iteration:5961\tLoss:13.1549\tTLoss:8.5317\t\n",
            "\n",
            "Thu Dec 21 08:01:30 2023\tEpoch:59[30/103],Iteration:6004\tLoss:10.2000\tTLoss:9.0001\t\n",
            "\n",
            "Thu Dec 21 08:01:36 2023\tEpoch:59[60/103],Iteration:6034\tLoss:6.4729\tTLoss:9.1946\t\n",
            "\n",
            "Thu Dec 21 08:01:42 2023\tEpoch:59[90/103],Iteration:6064\tLoss:5.5736\tTLoss:9.4323\t\n",
            "\n",
            "Thu Dec 21 08:01:52 2023\tEpoch:60[30/103],Iteration:6107\tLoss:8.0167\tTLoss:8.1425\t\n",
            "\n",
            "Thu Dec 21 08:01:58 2023\tEpoch:60[60/103],Iteration:6137\tLoss:5.7682\tTLoss:7.6666\t\n",
            "\n",
            "Thu Dec 21 08:02:04 2023\tEpoch:60[90/103],Iteration:6167\tLoss:11.1982\tTLoss:8.1946\t\n",
            "\n",
            "Thu Dec 21 08:02:13 2023\tEpoch:61[30/103],Iteration:6210\tLoss:9.1145\tTLoss:8.8004\t\n",
            "\n",
            "Thu Dec 21 08:02:19 2023\tEpoch:61[60/103],Iteration:6240\tLoss:10.0813\tTLoss:9.7674\t\n",
            "\n",
            "Thu Dec 21 08:02:25 2023\tEpoch:61[90/103],Iteration:6270\tLoss:1.4472\tTLoss:9.1847\t\n",
            "\n",
            "Thu Dec 21 08:02:34 2023\tEpoch:62[30/103],Iteration:6313\tLoss:7.4373\tTLoss:8.9903\t\n",
            "\n",
            "Thu Dec 21 08:02:40 2023\tEpoch:62[60/103],Iteration:6343\tLoss:15.2957\tTLoss:8.8642\t\n",
            "\n",
            "Thu Dec 21 08:02:46 2023\tEpoch:62[90/103],Iteration:6373\tLoss:12.2249\tTLoss:9.0167\t\n",
            "\n",
            "Thu Dec 21 08:02:56 2023\tEpoch:63[30/103],Iteration:6416\tLoss:13.1031\tTLoss:10.1740\t\n",
            "\n",
            "Thu Dec 21 08:03:01 2023\tEpoch:63[60/103],Iteration:6446\tLoss:11.7794\tTLoss:9.6032\t\n",
            "\n",
            "Thu Dec 21 08:03:08 2023\tEpoch:63[90/103],Iteration:6476\tLoss:12.5224\tTLoss:9.3364\t\n",
            "\n",
            "Thu Dec 21 08:03:17 2023\tEpoch:64[30/103],Iteration:6519\tLoss:2.7732\tTLoss:9.6214\t\n",
            "\n",
            "Thu Dec 21 08:03:23 2023\tEpoch:64[60/103],Iteration:6549\tLoss:6.9475\tTLoss:8.7968\t\n",
            "\n",
            "Thu Dec 21 08:03:29 2023\tEpoch:64[90/103],Iteration:6579\tLoss:6.0096\tTLoss:8.6533\t\n",
            "\n",
            "Thu Dec 21 08:03:38 2023\tEpoch:65[30/103],Iteration:6622\tLoss:3.7769\tTLoss:7.2229\t\n",
            "\n",
            "Thu Dec 21 08:03:44 2023\tEpoch:65[60/103],Iteration:6652\tLoss:9.3313\tTLoss:7.9817\t\n",
            "\n",
            "Thu Dec 21 08:03:50 2023\tEpoch:65[90/103],Iteration:6682\tLoss:4.2758\tTLoss:8.0492\t\n",
            "\n",
            "Thu Dec 21 08:04:00 2023\tEpoch:66[30/103],Iteration:6725\tLoss:1.1844\tTLoss:8.1972\t\n",
            "\n",
            "Thu Dec 21 08:04:06 2023\tEpoch:66[60/103],Iteration:6755\tLoss:15.7797\tTLoss:7.9341\t\n",
            "\n",
            "Thu Dec 21 08:04:12 2023\tEpoch:66[90/103],Iteration:6785\tLoss:17.2967\tTLoss:7.6118\t\n",
            "\n",
            "Thu Dec 21 08:04:21 2023\tEpoch:67[30/103],Iteration:6828\tLoss:12.1400\tTLoss:6.7378\t\n",
            "\n",
            "Thu Dec 21 08:04:27 2023\tEpoch:67[60/103],Iteration:6858\tLoss:6.9918\tTLoss:7.6411\t\n",
            "\n",
            "Thu Dec 21 08:04:33 2023\tEpoch:67[90/103],Iteration:6888\tLoss:10.1329\tTLoss:7.4205\t\n",
            "\n",
            "Thu Dec 21 08:04:43 2023\tEpoch:68[30/103],Iteration:6931\tLoss:7.0295\tTLoss:9.2473\t\n",
            "\n",
            "Thu Dec 21 08:04:48 2023\tEpoch:68[60/103],Iteration:6961\tLoss:7.4673\tTLoss:8.9413\t\n",
            "\n",
            "Thu Dec 21 08:04:54 2023\tEpoch:68[90/103],Iteration:6991\tLoss:9.0420\tTLoss:8.3749\t\n",
            "\n",
            "Thu Dec 21 08:05:03 2023\tEpoch:69[30/103],Iteration:7034\tLoss:6.8031\tTLoss:8.3646\t\n",
            "\n",
            "Thu Dec 21 08:05:09 2023\tEpoch:69[60/103],Iteration:7064\tLoss:6.9574\tTLoss:7.8082\t\n",
            "\n",
            "Thu Dec 21 08:05:15 2023\tEpoch:69[90/103],Iteration:7094\tLoss:14.0426\tTLoss:7.4801\t\n",
            "\n",
            "Thu Dec 21 08:05:25 2023\tEpoch:70[30/103],Iteration:7137\tLoss:18.7621\tTLoss:6.9780\t\n",
            "\n",
            "Thu Dec 21 08:05:31 2023\tEpoch:70[60/103],Iteration:7167\tLoss:8.1544\tTLoss:7.1598\t\n",
            "\n",
            "Thu Dec 21 08:05:37 2023\tEpoch:70[90/103],Iteration:7197\tLoss:10.8736\tTLoss:7.8775\t\n",
            "\n",
            "Thu Dec 21 08:05:47 2023\tEpoch:71[30/103],Iteration:7240\tLoss:3.3163\tTLoss:6.9647\t\n",
            "\n",
            "Thu Dec 21 08:05:53 2023\tEpoch:71[60/103],Iteration:7270\tLoss:6.9072\tTLoss:7.2021\t\n",
            "\n",
            "Thu Dec 21 08:05:59 2023\tEpoch:71[90/103],Iteration:7300\tLoss:2.0840\tTLoss:8.1885\t\n",
            "\n",
            "Thu Dec 21 08:06:09 2023\tEpoch:72[30/103],Iteration:7343\tLoss:21.0292\tTLoss:7.1653\t\n",
            "\n",
            "Thu Dec 21 08:06:15 2023\tEpoch:72[60/103],Iteration:7373\tLoss:4.9176\tTLoss:6.4614\t\n",
            "\n",
            "Thu Dec 21 08:06:20 2023\tEpoch:72[90/103],Iteration:7403\tLoss:3.9158\tTLoss:7.0348\t\n",
            "\n",
            "Thu Dec 21 08:06:30 2023\tEpoch:73[30/103],Iteration:7446\tLoss:6.9785\tTLoss:7.4375\t\n",
            "\n",
            "Thu Dec 21 08:06:36 2023\tEpoch:73[60/103],Iteration:7476\tLoss:0.1226\tTLoss:7.6631\t\n",
            "\n",
            "Thu Dec 21 08:06:41 2023\tEpoch:73[90/103],Iteration:7506\tLoss:5.4134\tTLoss:8.1288\t\n",
            "\n",
            "Thu Dec 21 08:06:52 2023\tEpoch:74[30/103],Iteration:7549\tLoss:4.0347\tTLoss:6.4044\t\n",
            "\n",
            "Thu Dec 21 08:06:57 2023\tEpoch:74[60/103],Iteration:7579\tLoss:5.4324\tTLoss:7.2144\t\n",
            "\n",
            "Thu Dec 21 08:07:04 2023\tEpoch:74[90/103],Iteration:7609\tLoss:10.2328\tTLoss:7.3876\t\n",
            "\n",
            "Thu Dec 21 08:07:13 2023\tEpoch:75[30/103],Iteration:7652\tLoss:3.8658\tTLoss:8.2330\t\n",
            "\n",
            "Thu Dec 21 08:07:19 2023\tEpoch:75[60/103],Iteration:7682\tLoss:12.8331\tTLoss:8.8319\t\n",
            "\n",
            "Thu Dec 21 08:07:25 2023\tEpoch:75[90/103],Iteration:7712\tLoss:2.2520\tTLoss:8.7338\t\n",
            "\n",
            "Thu Dec 21 08:07:35 2023\tEpoch:76[30/103],Iteration:7755\tLoss:3.4491\tTLoss:8.2077\t\n",
            "\n",
            "Thu Dec 21 08:07:40 2023\tEpoch:76[60/103],Iteration:7785\tLoss:8.1349\tTLoss:8.2847\t\n",
            "\n",
            "Thu Dec 21 08:07:46 2023\tEpoch:76[90/103],Iteration:7815\tLoss:7.6586\tTLoss:7.7989\t\n",
            "\n",
            "Thu Dec 21 08:07:55 2023\tEpoch:77[30/103],Iteration:7858\tLoss:13.2966\tTLoss:8.2263\t\n",
            "\n",
            "Thu Dec 21 08:08:01 2023\tEpoch:77[60/103],Iteration:7888\tLoss:4.8544\tTLoss:8.7252\t\n",
            "\n",
            "Thu Dec 21 08:08:07 2023\tEpoch:77[90/103],Iteration:7918\tLoss:12.2620\tTLoss:8.6951\t\n",
            "\n",
            "Thu Dec 21 08:08:17 2023\tEpoch:78[30/103],Iteration:7961\tLoss:8.0067\tTLoss:9.5056\t\n",
            "\n",
            "Thu Dec 21 08:08:23 2023\tEpoch:78[60/103],Iteration:7991\tLoss:7.6005\tTLoss:8.5476\t\n",
            "\n",
            "Thu Dec 21 08:08:28 2023\tEpoch:78[90/103],Iteration:8021\tLoss:13.2536\tTLoss:7.9653\t\n",
            "\n",
            "Thu Dec 21 08:08:38 2023\tEpoch:79[30/103],Iteration:8064\tLoss:20.0719\tTLoss:7.8078\t\n",
            "\n",
            "Thu Dec 21 08:08:44 2023\tEpoch:79[60/103],Iteration:8094\tLoss:6.6273\tTLoss:7.9616\t\n",
            "\n",
            "Thu Dec 21 08:08:49 2023\tEpoch:79[90/103],Iteration:8124\tLoss:7.6478\tTLoss:7.9179\t\n",
            "\n",
            "Thu Dec 21 08:09:00 2023\tEpoch:80[30/103],Iteration:8167\tLoss:12.0597\tTLoss:8.4192\t\n",
            "\n",
            "Thu Dec 21 08:09:05 2023\tEpoch:80[60/103],Iteration:8197\tLoss:9.5822\tTLoss:8.0337\t\n",
            "\n",
            "Thu Dec 21 08:09:11 2023\tEpoch:80[90/103],Iteration:8227\tLoss:1.9111\tTLoss:7.6407\t\n",
            "\n",
            "Thu Dec 21 08:09:20 2023\tEpoch:81[30/103],Iteration:8270\tLoss:10.6844\tTLoss:8.2815\t\n",
            "\n",
            "Thu Dec 21 08:09:26 2023\tEpoch:81[60/103],Iteration:8300\tLoss:18.3676\tTLoss:7.5813\t\n",
            "\n",
            "Thu Dec 21 08:09:32 2023\tEpoch:81[90/103],Iteration:8330\tLoss:0.0026\tTLoss:7.0632\t\n",
            "\n",
            "Thu Dec 21 08:09:42 2023\tEpoch:82[30/103],Iteration:8373\tLoss:11.6766\tTLoss:7.7669\t\n",
            "\n",
            "Thu Dec 21 08:09:48 2023\tEpoch:82[60/103],Iteration:8403\tLoss:14.9943\tTLoss:8.2066\t\n",
            "\n",
            "Thu Dec 21 08:09:54 2023\tEpoch:82[90/103],Iteration:8433\tLoss:7.0800\tTLoss:7.8537\t\n",
            "\n",
            "Thu Dec 21 08:10:03 2023\tEpoch:83[30/103],Iteration:8476\tLoss:11.0315\tTLoss:6.0181\t\n",
            "\n",
            "Thu Dec 21 08:10:09 2023\tEpoch:83[60/103],Iteration:8506\tLoss:3.5844\tTLoss:6.2731\t\n",
            "\n",
            "Thu Dec 21 08:10:15 2023\tEpoch:83[90/103],Iteration:8536\tLoss:13.6911\tTLoss:6.8889\t\n",
            "\n",
            "Thu Dec 21 08:10:24 2023\tEpoch:84[30/103],Iteration:8579\tLoss:8.4935\tTLoss:8.6574\t\n",
            "\n",
            "Thu Dec 21 08:10:31 2023\tEpoch:84[60/103],Iteration:8609\tLoss:2.3057\tTLoss:7.8347\t\n",
            "\n",
            "Thu Dec 21 08:10:36 2023\tEpoch:84[90/103],Iteration:8639\tLoss:14.0479\tTLoss:7.6067\t\n",
            "\n",
            "Thu Dec 21 08:10:46 2023\tEpoch:85[30/103],Iteration:8682\tLoss:10.8419\tTLoss:7.6923\t\n",
            "\n",
            "Thu Dec 21 08:10:51 2023\tEpoch:85[60/103],Iteration:8712\tLoss:10.2580\tTLoss:7.4026\t\n",
            "\n",
            "Thu Dec 21 08:10:57 2023\tEpoch:85[90/103],Iteration:8742\tLoss:6.2383\tTLoss:6.8457\t\n",
            "\n",
            "Thu Dec 21 08:11:07 2023\tEpoch:86[30/103],Iteration:8785\tLoss:4.8763\tTLoss:7.8190\t\n",
            "\n",
            "Thu Dec 21 08:11:13 2023\tEpoch:86[60/103],Iteration:8815\tLoss:12.8422\tTLoss:7.3219\t\n",
            "\n",
            "Thu Dec 21 08:11:19 2023\tEpoch:86[90/103],Iteration:8845\tLoss:12.8267\tTLoss:6.8845\t\n",
            "\n",
            "Thu Dec 21 08:11:28 2023\tEpoch:87[30/103],Iteration:8888\tLoss:6.7447\tTLoss:6.7819\t\n",
            "\n",
            "Thu Dec 21 08:11:34 2023\tEpoch:87[60/103],Iteration:8918\tLoss:10.9722\tTLoss:6.8945\t\n",
            "\n",
            "Thu Dec 21 08:11:40 2023\tEpoch:87[90/103],Iteration:8948\tLoss:2.6777\tTLoss:7.1761\t\n",
            "\n",
            "Thu Dec 21 08:11:50 2023\tEpoch:88[30/103],Iteration:8991\tLoss:10.6319\tTLoss:5.5194\t\n",
            "\n",
            "Thu Dec 21 08:11:56 2023\tEpoch:88[60/103],Iteration:9021\tLoss:3.9145\tTLoss:6.1156\t\n",
            "\n",
            "Thu Dec 21 08:12:01 2023\tEpoch:88[90/103],Iteration:9051\tLoss:13.3968\tTLoss:6.0923\t\n",
            "\n",
            "Thu Dec 21 08:12:11 2023\tEpoch:89[30/103],Iteration:9094\tLoss:6.1417\tTLoss:6.6786\t\n",
            "\n",
            "Thu Dec 21 08:12:16 2023\tEpoch:89[60/103],Iteration:9124\tLoss:1.0387\tTLoss:7.2012\t\n",
            "\n",
            "Thu Dec 21 08:12:22 2023\tEpoch:89[90/103],Iteration:9154\tLoss:8.6321\tTLoss:7.6555\t\n",
            "\n",
            "Thu Dec 21 08:12:32 2023\tEpoch:90[30/103],Iteration:9197\tLoss:20.0708\tTLoss:7.0450\t\n",
            "\n",
            "Thu Dec 21 08:12:38 2023\tEpoch:90[60/103],Iteration:9227\tLoss:15.2101\tTLoss:7.3352\t\n",
            "\n",
            "Thu Dec 21 08:12:43 2023\tEpoch:90[90/103],Iteration:9257\tLoss:7.2055\tTLoss:7.1588\t\n",
            "\n",
            "Thu Dec 21 08:12:53 2023\tEpoch:91[30/103],Iteration:9300\tLoss:9.5074\tTLoss:6.8986\t\n",
            "\n",
            "Thu Dec 21 08:12:59 2023\tEpoch:91[60/103],Iteration:9330\tLoss:0.1347\tTLoss:6.2622\t\n",
            "\n",
            "Thu Dec 21 08:13:05 2023\tEpoch:91[90/103],Iteration:9360\tLoss:0.9426\tTLoss:6.2394\t\n",
            "\n",
            "Thu Dec 21 08:13:14 2023\tEpoch:92[30/103],Iteration:9403\tLoss:6.2999\tTLoss:6.9163\t\n",
            "\n",
            "Thu Dec 21 08:13:20 2023\tEpoch:92[60/103],Iteration:9433\tLoss:5.6307\tTLoss:7.5260\t\n",
            "\n",
            "Thu Dec 21 08:13:26 2023\tEpoch:92[90/103],Iteration:9463\tLoss:3.5630\tTLoss:6.5472\t\n",
            "\n",
            "Thu Dec 21 08:13:35 2023\tEpoch:93[30/103],Iteration:9506\tLoss:8.4305\tTLoss:7.0105\t\n",
            "\n",
            "Thu Dec 21 08:13:41 2023\tEpoch:93[60/103],Iteration:9536\tLoss:10.8849\tTLoss:6.5943\t\n",
            "\n",
            "Thu Dec 21 08:13:47 2023\tEpoch:93[90/103],Iteration:9566\tLoss:0.5874\tTLoss:6.3550\t\n",
            "\n",
            "Thu Dec 21 08:13:58 2023\tEpoch:94[30/103],Iteration:9609\tLoss:4.1328\tTLoss:7.2109\t\n",
            "\n",
            "Thu Dec 21 08:14:03 2023\tEpoch:94[60/103],Iteration:9639\tLoss:12.5756\tTLoss:7.0026\t\n",
            "\n",
            "Thu Dec 21 08:14:09 2023\tEpoch:94[90/103],Iteration:9669\tLoss:0.1363\tTLoss:7.4941\t\n",
            "\n",
            "Thu Dec 21 08:14:18 2023\tEpoch:95[30/103],Iteration:9712\tLoss:5.6430\tTLoss:5.5959\t\n",
            "\n",
            "Thu Dec 21 08:14:24 2023\tEpoch:95[60/103],Iteration:9742\tLoss:3.3221\tTLoss:5.9712\t\n",
            "\n",
            "Thu Dec 21 08:14:30 2023\tEpoch:95[90/103],Iteration:9772\tLoss:6.8754\tTLoss:6.3237\t\n",
            "\n",
            "Thu Dec 21 08:14:40 2023\tEpoch:96[30/103],Iteration:9815\tLoss:9.7144\tTLoss:6.8859\t\n",
            "\n",
            "Thu Dec 21 08:14:45 2023\tEpoch:96[60/103],Iteration:9845\tLoss:8.1722\tTLoss:7.2180\t\n",
            "\n",
            "Thu Dec 21 08:14:51 2023\tEpoch:96[90/103],Iteration:9875\tLoss:7.0216\tTLoss:7.0697\t\n",
            "\n",
            "Thu Dec 21 08:15:01 2023\tEpoch:97[30/103],Iteration:9918\tLoss:5.5352\tTLoss:7.5524\t\n",
            "\n",
            "Thu Dec 21 08:15:07 2023\tEpoch:97[60/103],Iteration:9948\tLoss:9.2524\tTLoss:8.0017\t\n",
            "\n",
            "Thu Dec 21 08:15:14 2023\tEpoch:97[90/103],Iteration:9978\tLoss:5.3107\tTLoss:7.3865\t\n",
            "\n",
            "Thu Dec 21 08:15:23 2023\tEpoch:98[30/103],Iteration:10021\tLoss:33.5418\tTLoss:6.3707\t\n",
            "\n",
            "Thu Dec 21 08:15:29 2023\tEpoch:98[60/103],Iteration:10051\tLoss:9.0524\tTLoss:6.5396\t\n",
            "\n",
            "Thu Dec 21 08:15:35 2023\tEpoch:98[90/103],Iteration:10081\tLoss:3.4429\tTLoss:6.5166\t\n",
            "\n",
            "Thu Dec 21 08:15:44 2023\tEpoch:99[30/103],Iteration:10124\tLoss:0.6229\tTLoss:4.3926\t\n",
            "\n",
            "Thu Dec 21 08:15:50 2023\tEpoch:99[60/103],Iteration:10154\tLoss:3.8773\tTLoss:5.8712\t\n",
            "\n",
            "Thu Dec 21 08:15:56 2023\tEpoch:99[90/103],Iteration:10184\tLoss:7.4330\tTLoss:6.1418\t\n",
            "\n",
            "Thu Dec 21 08:16:06 2023\tEpoch:100[30/103],Iteration:10227\tLoss:7.6788\tTLoss:6.8136\t\n",
            "\n",
            "Thu Dec 21 08:16:11 2023\tEpoch:100[60/103],Iteration:10257\tLoss:5.4070\tTLoss:5.9522\t\n",
            "\n",
            "Thu Dec 21 08:16:17 2023\tEpoch:100[90/103],Iteration:10287\tLoss:7.4960\tTLoss:6.6038\t\n",
            "\n",
            "\n",
            "Done, trained model saved at /content/speech_id_checkpoint/final_epoch_100_batch_id_103.model\n"
          ]
        }
      ],
      "source": [
        "import time as time\n",
        "from torch.utils.data import DataLoader\n",
        "def train(model_path,dict1):\n",
        "    device = torch.device(dict1['device'])\n",
        "    device=dict1['device']\n",
        "    if dict1['data_preprocessed']:\n",
        "        train_dataset = SpeakerDatasetTIMITPreprocessed(dict1)\n",
        "    else:\n",
        "        train_dataset = SpeakerDatasetTIMIT(dict1)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=dict1['N'], shuffle=True, num_workers=dict1['num_workers'], drop_last=True)\n",
        "    print(train_loader)\n",
        "    embedder_net = SpeechEmbedder(dict1).to(device)\n",
        "    if dict1['restore']:\n",
        "        embedder_net.load_state_dict(torch.load(model_path))\n",
        "    ge2e_loss = GE2ELoss(device)\n",
        "    #Both net and loss have trainable parameters\n",
        "    optimizer = torch.optim.SGD([\n",
        "                    {'params': embedder_net.parameters()},\n",
        "                    {'params': ge2e_loss.parameters()}\n",
        "                ], lr=dict1['lr'])\n",
        "\n",
        "    os.makedirs(dict1['checkpoint_dir'], exist_ok=True)\n",
        "\n",
        "    embedder_net.train()\n",
        "    iteration = 0\n",
        "    for e in range(100):\n",
        "        total_loss = 0\n",
        "        for batch_id, mel_db_batch in enumerate(train_loader):\n",
        "            mel_db_batch = mel_db_batch.to(device)\n",
        "\n",
        "            mel_db_batch = torch.reshape(mel_db_batch, (dict1['N']*dict1['M'], mel_db_batch.size(2), mel_db_batch.size(3)))\n",
        "            perm = random.sample(range(0, dict1['N']*dict1['M']),dict1['N']*dict1['M'])\n",
        "            unperm = list(perm)\n",
        "            for i,j in enumerate(perm):\n",
        "                unperm[j] = i\n",
        "            mel_db_batch = mel_db_batch[perm]\n",
        "            #gradient accumulates\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            embeddings = embedder_net(mel_db_batch)\n",
        "            embeddings = embeddings[unperm]\n",
        "            embeddings = torch.reshape(embeddings, (dict1['N'],dict1['M'], embeddings.size(1)))\n",
        "\n",
        "            #get loss, call backward, step optimizer\n",
        "            loss = ge2e_loss(embeddings) #wants (Speaker, Utterances, embedding)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(embedder_net.parameters(), 3.0)\n",
        "            torch.nn.utils.clip_grad_norm_(ge2e_loss.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss = total_loss + loss\n",
        "            iteration += 1\n",
        "            if (batch_id + 1) % dict1['log_interval'] == 0:\n",
        "                mesg = \"{0}\\tEpoch:{1}[{2}/{3}],Iteration:{4}\\tLoss:{5:.4f}\\tTLoss:{6:.4f}\\t\\n\".format(time.ctime(), e+1,\n",
        "                        batch_id+1, len(train_dataset)//dict1['N'], iteration,loss, total_loss / (batch_id + 1))\n",
        "                print(mesg)\n",
        "                if dict1['log_file'] is not None:\n",
        "                    with open(dict1['log_file'],'a') as f:\n",
        "                        f.write(mesg)\n",
        "\n",
        "\n",
        "        if dict1['checkpoint_dir'] is not None and (e + 1) % dict1['checkpoint_interval'] == 0:\n",
        "            embedder_net.eval().cpu()\n",
        "            ckpt_model_filename = \"ckpt_epoch_\" + str(e+1) + \"_batch_id_\" + str(batch_id+1) + \".pth\"\n",
        "            ckpt_model_path = os.path.join(dict1['checkpoint_dir'], ckpt_model_filename)\n",
        "            torch.save(embedder_net.state_dict(), ckpt_model_path)\n",
        "            embedder_net.to(device).train()\n",
        "\n",
        "    #save model\n",
        "    embedder_net.eval().cpu()\n",
        "    save_model_filename = \"final_epoch_\" + str(e + 1) + \"_batch_id_\" + str(batch_id + 1) + \".model\"\n",
        "    save_model_path = os.path.join(dict1['checkpoint_dir'], save_model_filename)\n",
        "    torch.save(embedder_net.state_dict(), save_model_path)\n",
        "\n",
        "    print(\"\\nDone, trained model saved at\", save_model_path)\n",
        "train(dict1['model_path'],dict1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MGRbfQQcZN3"
      },
      "outputs": [],
      "source": [
        "_DEFAULT_OBSERVATION_DIM = 256\n",
        "\n",
        "import argparse\n",
        "def str2bool(value):\n",
        "  \"\"\"A function to convert string to bool value.\"\"\"\n",
        "  if value.lower() in {'yes', 'true', 't', 'y', '1'}:\n",
        "    return True\n",
        "  if value.lower() in {'no', 'false', 'f', 'n', '0'}:\n",
        "    return False\n",
        "# model configurations\n",
        "class arguments:\n",
        "    def __init__(self):\n",
        "        self.observation_dim=_DEFAULT_OBSERVATION_DIM\n",
        "        self.rnn_hidden_size=512\n",
        "        self.rnn_depth=1\n",
        "        self.rnn_dropout=0.2\n",
        "        self.transition_bias=None\n",
        "        self.crp_alpha=1.0\n",
        "        self.sigma2=None\n",
        "        self.verbosity=2\n",
        "        self.enable_cuda=True\n",
        "        self.optimizer='adam'\n",
        "        self.learning_rate=1e-5\n",
        "        self.train_iteration=20000\n",
        "        self.batch_size=20\n",
        "        self.num_permutations=10\n",
        "        self.sigma_alpha=1\n",
        "        self.sigma_beta=1\n",
        "        self.regularization_weight=1e-5\n",
        "        self.grad_max_norm=5\n",
        "        self.enforce_cluster_id_uniqueness=True\n",
        "        self.beam_size=10\n",
        "        self.look_ahead=1\n",
        "        self.test_iteration=2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are three loss functions are essential components of a neural network training process, providing regularization and incorporating specific weighting schemes. They are often used in conjunction with optimization algorithms to train models effectively.\n",
        "1. weighted_mse_loss\n",
        "2. sigma2_prior_loss\n",
        "3. regularization_loss"
      ],
      "metadata": {
        "id": "oE4LmuuOtad9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4h8ecj4cZQ0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "def weighted_mse_loss(input_tensor, target_tensor, weight=1):\n",
        "  \"\"\"Compute weighted MSE loss.\n",
        "  Note that we are doing weighted loss that only sum up over non-zero entries.\n",
        "  Args:\n",
        "    input_tensor: input tensor\n",
        "    target_tensor: target tensor\n",
        "    weight: weight tensor, in this case 1/sigma^2\n",
        "  Returns:\n",
        "    the weighted MSE loss\n",
        "  \"\"\"\n",
        "  observation_dim = input_tensor.size()[-1]\n",
        "  streched_tensor = ((input_tensor - target_tensor) ** 2).view(\n",
        "      -1, observation_dim)\n",
        "  entry_num = float(streched_tensor.size()[0])\n",
        "  non_zero_entry_num = torch.sum(streched_tensor[:, 0] != 0).float()\n",
        "  weighted_tensor = torch.mm(\n",
        "      ((input_tensor - target_tensor)**2).view(-1, observation_dim),\n",
        "      (torch.diag(weight.float().view(-1))))\n",
        "  return torch.mean(\n",
        "      weighted_tensor) * weight.nelement() * entry_num / non_zero_entry_num\n",
        "\n",
        "\n",
        "def sigma2_prior_loss(num_non_zero, sigma_alpha, sigma_beta, sigma2):\n",
        "    \"\"\"Compute sigma2 prior loss.\n",
        "  Args:\n",
        "    num_non_zero: since rnn_truth is a collection of different length sequences\n",
        "        padded with zeros to fit them into a tensor, we count the sum of\n",
        "        'real lengths' of all sequences\n",
        "    sigma_alpha: inverse gamma shape\n",
        "    sigma_beta: inverse gamma scale\n",
        "    sigma2: sigma squared\n",
        "  Returns:\n",
        "    the sigma2 prior loss\n",
        "    \"\"\"\n",
        "    return ((2 * sigma_alpha + num_non_zero + 2) /\n",
        "          (2 * num_non_zero) * torch.log(sigma2)).sum() + (\n",
        "              sigma_beta / (sigma2 * num_non_zero)).sum()\n",
        "\n",
        "\n",
        "def regularization_loss(params, weight):\n",
        "  \"\"\"Compute regularization loss.\n",
        "  Args:\n",
        "    params: iterable of all parameters\n",
        "    weight: weight for the regularization term\n",
        "  Returns:\n",
        "    the regularization loss\n",
        "  \"\"\"\n",
        "  l2_reg = 0\n",
        "  for param in params:\n",
        "    l2_reg += torch.norm(param)\n",
        "  return weight * l2_reg"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two utility functions related to sequence matchin. These functions can be useful for evaluating the similarity or matching accuracy between two sequences, where the sequences are represented as lists of unique values."
      ],
      "metadata": {
        "id": "Fxf2HjNitdrW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1P-SnW8cZWW"
      },
      "outputs": [],
      "source": [
        "from scipy import optimize\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def get_list_inverse_index(unique_ids):\n",
        "  \"\"\"Get value to position index from a list of unique ids.\n",
        "  Args:\n",
        "    unique_ids: A list of unique integers of strings.\n",
        "  Returns:\n",
        "    result: a dict from value to position\n",
        "  Raises:\n",
        "    TypeError: If unique_ids is not a list.\n",
        "  \"\"\"\n",
        "  if not isinstance(unique_ids, list):\n",
        "    raise TypeError('unique_ids must be a list')\n",
        "  result = dict()\n",
        "  for i, unique_id in enumerate(unique_ids):\n",
        "    result[unique_id] = i\n",
        "  return result\n",
        "\n",
        "def compute_sequence_match_accuracy(sequence1, sequence2):\n",
        "  \"\"\"Compute the accuracy between two sequences by finding optimal matching.\n",
        "  Args:\n",
        "    sequence1: A list of integers or strings.\n",
        "    sequence2: A list of integers or strings.\n",
        "  Returns:\n",
        "    accuracy: sequence matching accuracy as a number in [0.0, 1.0]\n",
        "  Raises:\n",
        "    TypeError: If sequence1 or sequence2 is not list.\n",
        "    ValueError: If sequence1 and sequence2 are not same size.\n",
        "  \"\"\"\n",
        "  sequence1=list(sequence1)\n",
        "  if not isinstance(sequence1, list) or not isinstance(sequence2, list):\n",
        "    raise TypeError('sequence1 and sequence2 must be lists')\n",
        "  if not sequence1 or len(sequence1) != len(sequence2):\n",
        "    raise ValueError(\n",
        "        'sequence1 and sequence2 must have the same non-zero length')\n",
        "  # get unique ids from sequences\n",
        "  unique_ids1 = sorted(set(sequence1))\n",
        "  unique_ids2 = sorted(set(sequence2))\n",
        "  inverse_index1 = get_list_inverse_index(unique_ids1)\n",
        "  inverse_index2 = get_list_inverse_index(unique_ids2)\n",
        "  print(sequence1)\n",
        "  print(sequence2)\n",
        "  print(inverse_index1)\n",
        "  print(inverse_index2)\n",
        "  # get the count matrix\n",
        "  count_matrix = np.zeros((len(unique_ids1), len(unique_ids2)))\n",
        "  for item1, item2 in zip(sequence1, sequence2):\n",
        "    index1 = inverse_index1[item1]\n",
        "    index2 = inverse_index2[item2]\n",
        "    count_matrix[index1, index2] += 1.0\n",
        "  row_index, col_index = optimize.linear_sum_assignment(-count_matrix)\n",
        "  optimal_match_count = count_matrix[row_index, col_index].sum()\n",
        "  accuracy = optimal_match_count / len(sequence1)\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recurrent neural network (RNN) model\n",
        "The code comprises utility functions for sequence matching tasks, specifically within the context of recurrent neural network (RNN) models. The Logger class facilitates logging with different verbosity levels. Functions such as **generate_random_string** and **enforce_cluster_id_uniqueness** contribute to data preprocessing, while **concatenate_training_data** combines training sequences and cluster IDs. **sample_permuted_segments** and **resize_sequence** are involved in data augmentation and preparation for training. The pack_sequence function prepares sequences for input to an RNN, supporting batch learning. Lastly, **output_result** generates a summary of experiment results, including configuration parameters and performance metrics, saved in a file. Together, these functions suggest a framework for training and evaluating RNN models on sequence matching or clustering tasks."
      ],
      "metadata": {
        "id": "A7q5FUW7tjP1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTyCEgnPcZZU"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import string\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import autograd\n",
        "\n",
        "\n",
        "class Logger:\n",
        "  \"\"\"A class for printing logging information to screen.\"\"\"\n",
        "\n",
        "  def __init__(self, verbosity):\n",
        "    self._verbosity = verbosity\n",
        "\n",
        "  def print(self, level, message):\n",
        "    \"\"\"Print a message if level is not higher than verbosity.\n",
        "    Args:\n",
        "      level: the level of this message, smaller value means more important\n",
        "      message: the message to be printed\n",
        "    \"\"\"\n",
        "    if level <= self._verbosity:\n",
        "      print(message)\n",
        "\n",
        "\n",
        "def generate_random_string(length=6):\n",
        "  \"\"\"Generate a random string of upper case letters and digits.\n",
        "  Args:\n",
        "    length: length of the generated string\n",
        "  Returns:\n",
        "    the generated string\n",
        "  \"\"\"\n",
        "  return ''.join([\n",
        "      random.choice(string.ascii_uppercase + string.digits)\n",
        "      for _ in range(length)])\n",
        "\n",
        "\n",
        "def enforce_cluster_id_uniqueness(cluster_ids):\n",
        "  \"\"\"Enforce uniqueness of cluster id across sequences.\n",
        "  Args:\n",
        "    cluster_ids: a list of 1-dim list/numpy.ndarray of strings\n",
        "  Returns:\n",
        "    a new list with same length of cluster_ids\n",
        "  Raises:\n",
        "    TypeError: if cluster_ids or its element has wrong type\n",
        "  \"\"\"\n",
        "  if not isinstance(cluster_ids, list):\n",
        "    raise TypeError('cluster_ids must be a list')\n",
        "  new_cluster_ids = []\n",
        "  for cluster_id in cluster_ids:\n",
        "    sequence_id = generate_random_string()\n",
        "    if isinstance(cluster_id, np.ndarray):\n",
        "      cluster_id = cluster_id.tolist()\n",
        "    if not isinstance(cluster_id, list):\n",
        "      raise TypeError('Elements of cluster_ids must be list or numpy.ndarray')\n",
        "    new_cluster_id = ['_'.join([sequence_id, s]) for s in cluster_id]\n",
        "    new_cluster_ids.append(new_cluster_id)\n",
        "  return new_cluster_ids\n",
        "\n",
        "\n",
        "def concatenate_training_data(train_sequences, train_cluster_ids,\n",
        "                              enforce_uniqueness=True, shuffle=True):\n",
        "  \"\"\"Concatenate training data.\n",
        "  Args:\n",
        "    train_sequences: a list of 2-dim numpy arrays to be concatenated\n",
        "    train_cluster_ids: a list of 1-dim list/numpy.ndarray of strings\n",
        "    enforce_uniqueness: a boolean indicated whether we should enfore uniqueness\n",
        "      to train_cluster_ids\n",
        "    shuffle: whether to randomly shuffle input order\n",
        "  Returns:\n",
        "    concatenated_train_sequence: a 2-dim numpy array\n",
        "    concatenated_train_cluster_id: a list of strings\n",
        "  Raises:\n",
        "    TypeError: if input has wrong type\n",
        "    ValueError: if sizes/dimensions of input or their elements are incorrect\n",
        "  \"\"\"\n",
        "  # check input\n",
        "  if not isinstance(train_sequences, list) or not isinstance(\n",
        "      train_cluster_ids, list):\n",
        "    raise TypeError('train_sequences and train_cluster_ids must be lists')\n",
        "  if len(train_sequences) != len(train_cluster_ids):\n",
        "    raise ValueError(\n",
        "        'train_sequences and train_cluster_ids must have same size')\n",
        "  train_cluster_ids = [\n",
        "      x.tolist() if isinstance(x, np.ndarray) else x\n",
        "      for x in train_cluster_ids]\n",
        "  global_observation_dim = None\n",
        "  for i, (train_sequence, train_cluster_id) in enumerate(\n",
        "      zip(train_sequences, train_cluster_ids)):\n",
        "    train_length, observation_dim = train_sequence.shape\n",
        "    if i == 0:\n",
        "      global_observation_dim = observation_dim\n",
        "    elif global_observation_dim != observation_dim:\n",
        "      raise ValueError(\n",
        "          'train_sequences must have consistent observation dimension')\n",
        "    if not isinstance(train_cluster_id, list):\n",
        "      raise TypeError(\n",
        "          'Elements of train_cluster_ids must be list or numpy.ndarray')\n",
        "    if len(train_cluster_id) != train_length:\n",
        "      raise ValueError(\n",
        "          'Each train_sequence and its train_cluster_id must have same length')\n",
        "\n",
        "  # enforce uniqueness\n",
        "  if enforce_uniqueness:\n",
        "    train_cluster_ids = enforce_cluster_id_uniqueness(train_cluster_ids)\n",
        "\n",
        "  # random shuffle\n",
        "  if shuffle:\n",
        "    zipped_input = list(zip(train_sequences, train_cluster_ids))\n",
        "    random.shuffle(zipped_input)\n",
        "    train_sequences, train_cluster_ids = zip(*zipped_input)\n",
        "\n",
        "  # concatenate\n",
        "  concatenated_train_sequence = np.concatenate(train_sequences, axis=0)\n",
        "  concatenated_train_cluster_id = [x for train_cluster_id in train_cluster_ids\n",
        "                                   for x in train_cluster_id]\n",
        "  return concatenated_train_sequence, concatenated_train_cluster_id\n",
        "\n",
        "\n",
        "def sample_permuted_segments(index_sequence, number_samples):\n",
        "  \"\"\"Sample sequences with permuted blocks.\n",
        "  Args:\n",
        "    index_sequence: (integer array, size: L)\n",
        "      - subsequence index\n",
        "      For example, index_sequence = [1,2,6,10,11,12].\n",
        "    number_samples: (integer)\n",
        "      - number of subsampled block-preserving permuted sequences.\n",
        "      For example, number_samples = 5\n",
        "  Returns:\n",
        "    sampled_index_sequences: (a list of numpy arrays) - a list of subsampled\n",
        "      block-preserving permuted sequences. For example,\n",
        "    ```\n",
        "    sampled_index_sequences =\n",
        "    [[10,11,12,1,2,6],\n",
        "     [6,1,2,10,11,12],\n",
        "     [1,2,10,11,12,6],\n",
        "     [6,1,2,10,11,12],\n",
        "     [1,2,6,10,11,12]]\n",
        "    ```\n",
        "      The length of \"sampled_index_sequences\" is \"number_samples\".\n",
        "  \"\"\"\n",
        "  segments = []\n",
        "  if len(index_sequence) == 1:\n",
        "    segments.append(index_sequence)\n",
        "  else:\n",
        "    prev = 0\n",
        "    for i in range(len(index_sequence) - 1):\n",
        "      if index_sequence[i + 1] != index_sequence[i] + 1:\n",
        "        segments.append(index_sequence[prev:(i + 1)])\n",
        "        prev = i + 1\n",
        "      if i + 1 == len(index_sequence) - 1:\n",
        "        segments.append(index_sequence[prev:])\n",
        "  # sample permutations\n",
        "  sampled_index_sequences = []\n",
        "  for _ in range(number_samples):\n",
        "    segments_array = []\n",
        "    permutation = np.random.permutation(len(segments))\n",
        "    for permutation_item in permutation:\n",
        "      segments_array.append(segments[permutation_item])\n",
        "    sampled_index_sequences.append(np.concatenate(segments_array))\n",
        "  return sampled_index_sequences\n",
        "\n",
        "\n",
        "def resize_sequence(sequence, cluster_id, num_permutations=None):\n",
        "  \"\"\"Resize sequences for packing and batching.\n",
        "  Args:\n",
        "    sequence: (real numpy matrix, size: seq_len*obs_size) - observed sequence\n",
        "    cluster_id: (numpy vector, size: seq_len) - cluster indicator sequence\n",
        "    num_permutations: int - Number of permutations per utterance sampled.\n",
        "  Returns:\n",
        "    sub_sequences: A list of numpy array, with obsevation vector from the same\n",
        "      cluster in the same list.\n",
        "    seq_lengths: The length of each cluster (+1).\n",
        "    bias: Flipping coin head probability.\n",
        "    bias_denominator: The denominator of the bias, used for multiple calls to\n",
        "      fit().\n",
        "  \"\"\"\n",
        "  # merge sub-sequences that belong to a single cluster to a single sequence\n",
        "  unique_id = np.unique(cluster_id)\n",
        "  sub_sequences = []\n",
        "  seq_lengths = []\n",
        "  if num_permutations and num_permutations > 1:\n",
        "    for i in unique_id:\n",
        "      idx_set = np.where(cluster_id == i)[0]\n",
        "      sampled_idx_sets = sample_permuted_segments(idx_set, num_permutations)\n",
        "      for j in range(num_permutations):\n",
        "        sub_sequences.append(sequence[sampled_idx_sets[j], :])\n",
        "        seq_lengths.append(len(idx_set) + 1)\n",
        "  else:\n",
        "    for i in unique_id:\n",
        "      idx_set = np.where(cluster_id == i)\n",
        "      sub_sequences.append(sequence[idx_set, :][0])\n",
        "      seq_lengths.append(len(idx_set[0]) + 1)\n",
        "\n",
        "  # compute bias\n",
        "  transit_num = 0\n",
        "  for entry in range(len(cluster_id) - 1):\n",
        "    transit_num += (cluster_id[entry] != cluster_id[entry + 1])\n",
        "  bias_denominator = len(cluster_id)\n",
        "  bias = (transit_num + 1) / bias_denominator\n",
        "  return sub_sequences, seq_lengths, bias, bias_denominator\n",
        "\n",
        "\n",
        "def pack_sequence(\n",
        "    sub_sequences, seq_lengths, batch_size, observation_dim, device):\n",
        "  \"\"\"Pack sequences for training.\n",
        "  Args:\n",
        "    sub_sequences: A list of numpy array, with obsevation vector from the same\n",
        "      cluster in the same list.\n",
        "    seq_lengths: The length of each cluster (+1).\n",
        "    batch_size: int or None - Run batch learning if batch_size is None. Else,\n",
        "      run online learning with specified batch size.\n",
        "    observation_dim: int - dimension for observation vectors\n",
        "    device: str - Your device. E.g., `cuda:0` or `cpu`.\n",
        "  Returns:\n",
        "    packed_rnn_input: (PackedSequence object) packed rnn input\n",
        "    rnn_truth: ground truth\n",
        "  \"\"\"\n",
        "  num_clusters = len(seq_lengths)\n",
        "  sorted_seq_lengths = np.sort(seq_lengths)[::-1]\n",
        "  permute_index = np.argsort(seq_lengths)[::-1]\n",
        "\n",
        "  if batch_size is None:\n",
        "    rnn_input = np.zeros((sorted_seq_lengths[0],\n",
        "                          num_clusters,\n",
        "                          observation_dim))\n",
        "    for i in range(num_clusters):\n",
        "      rnn_input[1:sorted_seq_lengths[i], i,\n",
        "                :] = sub_sequences[permute_index[i]]\n",
        "    rnn_input = autograd.Variable(\n",
        "        torch.from_numpy(rnn_input).float()).to(device)\n",
        "    packed_rnn_input = torch.nn.utils.rnn.pack_padded_sequence(\n",
        "        rnn_input, sorted_seq_lengths, batch_first=False)\n",
        "  else:\n",
        "    mini_batch = np.sort(np.random.choice(num_clusters, batch_size))\n",
        "    rnn_input = np.zeros((sorted_seq_lengths[mini_batch[0]],\n",
        "                          batch_size,\n",
        "                          observation_dim))\n",
        "    for i in range(batch_size):\n",
        "      rnn_input[1:sorted_seq_lengths[mini_batch[i]],\n",
        "                i, :] = sub_sequences[permute_index[mini_batch[i]]]\n",
        "    rnn_input = autograd.Variable(\n",
        "        torch.from_numpy(rnn_input).float()).to(device)\n",
        "    packed_rnn_input = torch.nn.utils.rnn.pack_padded_sequence(\n",
        "        rnn_input, sorted_seq_lengths[mini_batch], batch_first=False)\n",
        "  # ground truth is the shifted input\n",
        "  rnn_truth = rnn_input[1:, :, :]\n",
        "  return packed_rnn_input, rnn_truth\n",
        "\n",
        "\n",
        "def output_result(model_args, training_args, test_record):\n",
        "  \"\"\"Produce a string to summarize the experiment.\"\"\"\n",
        "  accuracy_array, _ = zip(*test_record)\n",
        "  total_accuracy = np.mean(accuracy_array)\n",
        "  output_string = \"\"\"\n",
        "Config:\n",
        "  sigma_alpha: {}\n",
        "  sigma_beta: {}\n",
        "  crp_alpha: {}\n",
        "  learning rate: {}\n",
        "  regularization: {}\n",
        "  batch size: {}\n",
        "Performance:\n",
        "  averaged accuracy: {:.6f}\n",
        "  accuracy numbers for all testing sequences:\n",
        "  \"\"\".strip().format(\n",
        "      training_args.sigma_alpha,\n",
        "      training_args.sigma_beta,\n",
        "      model_args.crp_alpha,\n",
        "      training_args.learning_rate,\n",
        "      training_args.regularization_weight,\n",
        "      training_args.batch_size,\n",
        "      total_accuracy)\n",
        "  for accuracy in accuracy_array:\n",
        "    output_string += '\\n    {:.6f}'.format(accuracy)\n",
        "  output_string += '\\n' + '=' * 80 + '\\n'\n",
        "  filename = 'layer_{}_{}_{:.1f}_result.txt'.format(\n",
        "      model_args.rnn_hidden_size,\n",
        "      model_args.rnn_depth, model_args.rnn_dropout)\n",
        "  with open(filename, 'a') as file_object:\n",
        "    file_object.write(output_string)\n",
        "  return output_string"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unbounded Interleaved-State Recurrent Neural Networks\n",
        "This code defines a class UISRNN (Unbounded Interleaved-State Recurrent Neural Networks) for speaker diarization, a task involving the identification and clustering of speakers in an audio recording. The implementation uses a recurrent neural network (RNN) with a Gated Recurrent Unit (GRU) architecture. Key components include a training method (**fit_concatenated and fit**) for model training, a prediction method (predict_single and predict) for speaker diarization on test sequences, and beam search for efficient decoding. The code supports online and batch learning, and the model is equipped with hyperparameters for training and inference configurations. The UISRNN model employs negative log likelihood as the loss function, with components for likelihood, **sigma2** prior, and regularization. The code is structured with various utility functions and classes, including **CoreRNN** for the core RNN model and BeamState for beam search state management."
      ],
      "metadata": {
        "id": "iyeTGwWXw-mu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLIa9B51cZjL"
      },
      "outputs": [],
      "source": [
        "_INITIAL_SIGMA2_VALUE = 0.1\n",
        "\n",
        "from torch import nn,optim\n",
        "class CoreRNN(nn.Module):\n",
        "  \"\"\"The core Recurent Neural Network used by UIS-RNN.\"\"\"\n",
        "\n",
        "  def __init__(self, input_dim, hidden_size, depth, observation_dim, dropout=0):\n",
        "    super(CoreRNN, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    if depth >= 2:\n",
        "      self.gru = nn.GRU(input_dim, hidden_size, depth, dropout=dropout)\n",
        "    else:\n",
        "      self.gru = nn.GRU(input_dim, hidden_size, depth)\n",
        "    self.linear_mean1 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.linear_mean2 = nn.Linear(hidden_size, observation_dim)\n",
        "\n",
        "  def forward(self, input_seq, hidden=None):\n",
        "    output_seq, hidden = self.gru(input_seq, hidden)\n",
        "    if isinstance(output_seq, torch.nn.utils.rnn.PackedSequence):\n",
        "      output_seq, _ = torch.nn.utils.rnn.pad_packed_sequence(\n",
        "          output_seq, batch_first=False)\n",
        "    mean = self.linear_mean2(F.relu(self.linear_mean1(output_seq)))\n",
        "    return mean, hidden\n",
        "\n",
        "\n",
        "class BeamState:\n",
        "  \"\"\"Structure that contains necessary states for beam search.\"\"\"\n",
        "\n",
        "  def __init__(self, source=None):\n",
        "    if not source:\n",
        "      self.mean_set = []\n",
        "      self.hidden_set = []\n",
        "      self.neg_likelihood = 0\n",
        "      self.trace = []\n",
        "      self.block_counts = []\n",
        "    else:\n",
        "      self.mean_set = source.mean_set.copy()\n",
        "      self.hidden_set = source.hidden_set.copy()\n",
        "      self.trace = source.trace.copy()\n",
        "      self.block_counts = source.block_counts.copy()\n",
        "      self.neg_likelihood = source.neg_likelihood\n",
        "\n",
        "  def append(self, mean, hidden, cluster):\n",
        "    \"\"\"Append new item to the BeamState.\"\"\"\n",
        "    self.mean_set.append(mean.clone())\n",
        "    self.hidden_set.append(hidden.clone())\n",
        "    self.block_counts.append(1)\n",
        "    self.trace.append(cluster)\n",
        "\n",
        "\n",
        "class UISRNN:\n",
        "  \"\"\"Unbounded Interleaved-State Recurrent Neural Networks.\"\"\"\n",
        "\n",
        "  def __init__(self,args):\n",
        "    \"\"\"Construct the UISRNN object.\n",
        "    Args:\n",
        "      args: Model configurations. See `arguments.py` for details.\n",
        "    \"\"\"\n",
        "    self.observation_dim = args.observation_dim\n",
        "    self.device = torch.device(\n",
        "        'cuda:0' if (torch.cuda.is_available() and args.enable_cuda) else 'cpu')\n",
        "    self.rnn_model = CoreRNN(self.observation_dim, args.rnn_hidden_size,\n",
        "                             args.rnn_depth, self.observation_dim,\n",
        "                             args.rnn_dropout).to(self.device)\n",
        "    self.rnn_init_hidden = nn.Parameter(\n",
        "        torch.zeros(args.rnn_depth, 1, args.rnn_hidden_size).to(self.device))\n",
        "    # booleans indicating which variables are trainable\n",
        "    self.estimate_sigma2 = (args.sigma2 is None)\n",
        "    self.estimate_transition_bias = (args.transition_bias is None)\n",
        "    # initial values of variables\n",
        "    sigma2 = _INITIAL_SIGMA2_VALUE if self.estimate_sigma2 else args.sigma2\n",
        "    self.sigma2 = nn.Parameter(\n",
        "        sigma2 * torch.ones(self.observation_dim).to(self.device))\n",
        "    self.transition_bias = args.transition_bias\n",
        "    self.transition_bias_denominator = 0.0\n",
        "    self.crp_alpha = args.crp_alpha\n",
        "    self.logger =Logger(args.verbosity)\n",
        "\n",
        "  def _get_optimizer(self, optimizer, learning_rate):\n",
        "    \"\"\"Get optimizer for UISRNN.\n",
        "    Args:\n",
        "      optimizer: string - name of the optimizer.\n",
        "      learning_rate: - learning rate for the entire model.\n",
        "        We do not customize learning rate for separate parts.\n",
        "    Returns:\n",
        "      a pytorch \"optim\" object\n",
        "    \"\"\"\n",
        "    params = [\n",
        "        {\n",
        "            'params': self.rnn_model.parameters()\n",
        "        },  # rnn parameters\n",
        "        {\n",
        "            'params': self.rnn_init_hidden\n",
        "        }  # rnn initial hidden state\n",
        "    ]\n",
        "    if self.estimate_sigma2:  # train sigma2\n",
        "      params.append({\n",
        "          'params': self.sigma2\n",
        "      })  # variance parameters\n",
        "    assert optimizer == 'adam', 'Only adam optimizer is supported.'\n",
        "    return optim.Adam(params, lr=learning_rate)\n",
        "\n",
        "  def save(self, filepath):\n",
        "    \"\"\"Save the model to a file.\n",
        "    Args:\n",
        "      filepath: the path of the file.\n",
        "    \"\"\"\n",
        "    torch.save({\n",
        "        'rnn_state_dict': self.rnn_model.state_dict(),\n",
        "        'rnn_init_hidden': self.rnn_init_hidden.detach().cpu().numpy(),\n",
        "        'transition_bias': self.transition_bias,\n",
        "        'transition_bias_denominator': self.transition_bias_denominator,\n",
        "        'crp_alpha': self.crp_alpha,\n",
        "        'sigma2': self.sigma2.detach().cpu().numpy()}, filepath)\n",
        "\n",
        "  def load(self, filepath):\n",
        "    \"\"\"Load the model from a file.\n",
        "    Args:\n",
        "      filepath: the path of the file.\n",
        "    \"\"\"\n",
        "    var_dict = torch.load(filepath)\n",
        "    self.rnn_model.load_state_dict(var_dict['rnn_state_dict'])\n",
        "    self.rnn_init_hidden = nn.Parameter(\n",
        "        torch.from_numpy(var_dict['rnn_init_hidden']).to(self.device))\n",
        "    self.transition_bias = float(var_dict['transition_bias'])\n",
        "    self.transition_bias_denominator = float(\n",
        "        var_dict['transition_bias_denominator'])\n",
        "    self.crp_alpha = float(var_dict['crp_alpha'])\n",
        "    self.sigma2 = nn.Parameter(\n",
        "        torch.from_numpy(var_dict['sigma2']).to(self.device))\n",
        "\n",
        "    self.logger.print(\n",
        "        3, 'Loaded model with transition_bias={}, crp_alpha={}, sigma2={}, '\n",
        "        'rnn_init_hidden={}'.format(\n",
        "            self.transition_bias, self.crp_alpha, var_dict['sigma2'],\n",
        "            var_dict['rnn_init_hidden']))\n",
        "\n",
        "  def fit_concatenated(self, train_sequence, train_cluster_id, args):\n",
        "    \"\"\"Fit UISRNN model to concatenated sequence and cluster_id.\n",
        "    Args:\n",
        "      train_sequence: the training observation sequence, which is a\n",
        "        2-dim numpy array of real numbers, of size `N * D`.\n",
        "        - `N`: summation of lengths of all utterances.\n",
        "        - `D`: observation dimension.\n",
        "        For example,\n",
        "      ```\n",
        "      train_sequence =\n",
        "      [[1.2 3.0 -4.1 6.0]    --> an entry of speaker #0 from utterance 'iaaa'\n",
        "       [0.8 -1.1 0.4 0.5]    --> an entry of speaker #1 from utterance 'iaaa'\n",
        "       [-0.2 1.0 3.8 5.7]    --> an entry of speaker #0 from utterance 'iaaa'\n",
        "       [3.8 -0.1 1.5 2.3]    --> an entry of speaker #0 from utterance 'ibbb'\n",
        "       [1.2 1.4 3.6 -2.7]]   --> an entry of speaker #0 from utterance 'ibbb'\n",
        "      ```\n",
        "        Here `N=5`, `D=4`.\n",
        "        We concatenate all training utterances into this single sequence.\n",
        "      train_cluster_id: the speaker id sequence, which is 1-dim list or\n",
        "        numpy array of strings, of size `N`.\n",
        "        For example,\n",
        "      ```\n",
        "      train_cluster_id =\n",
        "        ['iaaa_0', 'iaaa_1', 'iaaa_0', 'ibbb_0', 'ibbb_0']\n",
        "      ```\n",
        "        'iaaa_0' means the entry belongs to speaker #0 in utterance 'iaaa'.\n",
        "        Note that the order of entries within an utterance are preserved,\n",
        "        and all utterances are simply concatenated together.\n",
        "      args: Training configurations. See `arguments.py` for details.\n",
        "    Raises:\n",
        "      TypeError: If train_sequence or train_cluster_id is of wrong type.\n",
        "      ValueError: If train_sequence or train_cluster_id has wrong dimension.\n",
        "    \"\"\"\n",
        "    # check type\n",
        "    if (not isinstance(train_sequence, np.ndarray) or\n",
        "        train_sequence.dtype != float):\n",
        "      raise TypeError('train_sequence should be a numpy array of float type.')\n",
        "    if isinstance(train_cluster_id, list):\n",
        "      train_cluster_id = np.array(train_cluster_id)\n",
        "    if (not isinstance(train_cluster_id, np.ndarray) or\n",
        "        not train_cluster_id.dtype.name.startswith(('str', 'unicode'))):\n",
        "      raise TypeError('train_cluster_id type be a numpy array of strings.')\n",
        "    # check dimension\n",
        "    if train_sequence.ndim != 2:\n",
        "      raise ValueError('train_sequence must be 2-dim array.')\n",
        "    if train_cluster_id.ndim != 1:\n",
        "      raise ValueError('train_cluster_id must be 1-dim array.')\n",
        "    # check length and size\n",
        "    train_total_length, observation_dim = train_sequence.shape\n",
        "    if observation_dim != self.observation_dim:\n",
        "      raise ValueError('train_sequence does not match the dimension specified '\n",
        "                       'by args.observation_dim.')\n",
        "    if train_total_length != len(train_cluster_id):\n",
        "      raise ValueError('train_sequence length is not equal to '\n",
        "                       'train_cluster_id length.')\n",
        "\n",
        "    self.rnn_model.train()\n",
        "    optimizer = self._get_optimizer(optimizer=args.optimizer,\n",
        "                                    learning_rate=args.learning_rate)\n",
        "\n",
        "    (sub_sequences,\n",
        "     seq_lengths,\n",
        "     transition_bias,\n",
        "     transition_bias_denominator) = resize_sequence(\n",
        "         sequence=train_sequence,\n",
        "         cluster_id=train_cluster_id,\n",
        "         num_permutations=args.num_permutations)\n",
        "    if self.estimate_transition_bias:\n",
        "      if self.transition_bias is None:\n",
        "        self.transition_bias = transition_bias\n",
        "        self.transition_bias_denominator = transition_bias_denominator\n",
        "\n",
        "      else:\n",
        "\n",
        "        self.transition_bias = (\n",
        "            self.transition_bias * self.transition_bias_denominator +\n",
        "            transition_bias * transition_bias_denominator) / (\n",
        "                self.transition_bias_denominator + transition_bias_denominator)\n",
        "        self.transition_bias_denominator += transition_bias_denominator\n",
        "\n",
        "    # For batch learning, pack the entire dataset.\n",
        "    if args.batch_size is None:\n",
        "      packed_train_sequence, rnn_truth = utils.pack_sequence(\n",
        "          sub_sequences,\n",
        "          seq_lengths,\n",
        "          args.batch_size,\n",
        "          self.observation_dim,\n",
        "          self.device)\n",
        "    train_loss = []\n",
        "    for num_iter in range(args.train_iteration):\n",
        "      optimizer.zero_grad()\n",
        "      # For online learning, pack a subset in each iteration.\n",
        "      if args.batch_size is not None:\n",
        "        packed_train_sequence, rnn_truth = pack_sequence(\n",
        "            sub_sequences,\n",
        "            seq_lengths,\n",
        "            args.batch_size,\n",
        "            self.observation_dim,\n",
        "            self.device)\n",
        "      hidden = self.rnn_init_hidden.repeat(1, args.batch_size, 1)\n",
        "      mean, _ = self.rnn_model(packed_train_sequence, hidden)\n",
        "      # use mean to predict\n",
        "      mean = torch.cumsum(mean, dim=0)\n",
        "      mean_size = mean.size()\n",
        "      mean = torch.mm(\n",
        "          torch.diag(\n",
        "              1.0 / torch.arange(1, mean_size[0] + 1).float().to(self.device)),\n",
        "          mean.view(mean_size[0], -1))\n",
        "      mean = mean.view(mean_size)\n",
        "\n",
        "      # Likelihood part.\n",
        "      loss1 = weighted_mse_loss(\n",
        "          input_tensor=(rnn_truth != 0).float() * mean[:-1, :, :],\n",
        "          target_tensor=rnn_truth,\n",
        "          weight=1 / (2 * self.sigma2))\n",
        "\n",
        "      # Sigma2 prior part.\n",
        "      weight = (((rnn_truth != 0).float() * mean[:-1, :, :] - rnn_truth)\n",
        "                ** 2).view(-1, observation_dim)\n",
        "      num_non_zero = torch.sum((weight != 0).float(), dim=0).squeeze()\n",
        "      loss2 = sigma2_prior_loss(\n",
        "          num_non_zero, args.sigma_alpha, args.sigma_beta, self.sigma2)\n",
        "\n",
        "      # Regularization part.\n",
        "      loss3 = regularization_loss(\n",
        "          self.rnn_model.parameters(), args.regularization_weight)\n",
        "\n",
        "      loss = loss1 + loss2 + loss3\n",
        "      loss.backward()\n",
        "      nn.utils.clip_grad_norm_(self.rnn_model.parameters(), args.grad_max_norm)\n",
        "      optimizer.step()\n",
        "      # avoid numerical issues\n",
        "      self.sigma2.data.clamp_(min=1e-6)\n",
        "\n",
        "      if (np.remainder(num_iter, 10) == 0 or\n",
        "          num_iter == args.train_iteration - 1):\n",
        "        self.logger.print(\n",
        "            2,\n",
        "            'Iter: {:d}  \\t'\n",
        "            'Training Loss: {:.4f}    \\n'\n",
        "            '    Negative Log Likelihood: {:.4f}\\t'\n",
        "            'Sigma2 Prior: {:.4f}\\t'\n",
        "            'Regularization: {:.4f}'.format(\n",
        "                num_iter,\n",
        "                float(loss.data),\n",
        "                float(loss1.data),\n",
        "                float(loss2.data),\n",
        "                float(loss3.data)))\n",
        "      train_loss.append(float(loss1.data))  # only save the likelihood part\n",
        "    self.logger.print(\n",
        "        1, 'Done training with {} iterations'.format(args.train_iteration))\n",
        "\n",
        "  def fit(self, train_sequences, train_cluster_ids, args):\n",
        "    \"\"\"Fit UISRNN model.\n",
        "    Args:\n",
        "      train_sequences: Either a list of training sequences, or a single\n",
        "        concatenated training sequence:\n",
        "        1. train_sequences is list, and each element is a 2-dim numpy array\n",
        "           of real numbers, of size: `length * D`.\n",
        "           The length varies among different sequences, but the D is the same.\n",
        "           In speaker diarization, each sequence is the sequence of speaker\n",
        "           embeddings of one utterance.\n",
        "        2. train_sequences is a single concatenated sequence, which is a\n",
        "           2-dim numpy array of real numbers. See `fit_concatenated()`\n",
        "           for more details.\n",
        "      train_cluster_ids: Ground truth labels for train_sequences:\n",
        "        1. if train_sequences is a list, this must also be a list of the same\n",
        "           size, each element being a 1-dim list or numpy array of strings.\n",
        "        2. if train_sequences is a single concatenated sequence, this\n",
        "           must also be the concatenated 1-dim list or numpy array of strings\n",
        "      args: Training configurations. See `arguments.py` for details.\n",
        "    Raises:\n",
        "      TypeError: If train_sequences or train_cluster_ids is of wrong type.\n",
        "    \"\"\"\n",
        "    if isinstance(train_sequences, np.ndarray):\n",
        "      # train_sequences is already the concatenated sequence\n",
        "      concatenated_train_sequence = train_sequences\n",
        "      concatenated_train_cluster_id = train_cluster_ids\n",
        "    elif isinstance(train_sequences, list):\n",
        "      # train_sequences is a list of un-concatenated sequences,\n",
        "      # then we concatenate them first\n",
        "      (concatenated_train_sequence,\n",
        "       concatenated_train_cluster_id) = utils.concatenate_training_data(\n",
        "           train_sequences,\n",
        "           train_cluster_ids,\n",
        "           args.enforce_cluster_id_uniqueness,\n",
        "           True)\n",
        "    else:\n",
        "      raise TypeError('train_sequences must be a list or numpy.ndarray')\n",
        "\n",
        "    self.fit_concatenated(\n",
        "        concatenated_train_sequence, concatenated_train_cluster_id, args)\n",
        "\n",
        "  def _update_beam_state(self, beam_state, look_ahead_seq, cluster_seq):\n",
        "    \"\"\"Update a beam state given a look ahead sequence and known cluster\n",
        "    assignments.\n",
        "    Args:\n",
        "      beam_state: A BeamState object.\n",
        "      look_ahead_seq: Look ahead sequence, size: look_ahead*D.\n",
        "        look_ahead: number of step to look ahead in the beam search.\n",
        "        D: observation dimension\n",
        "      cluster_seq: Cluster assignment sequence for look_ahead_seq.\n",
        "    Returns:\n",
        "      new_beam_state: An updated BeamState object.\n",
        "    \"\"\"\n",
        "\n",
        "    loss = 0\n",
        "    new_beam_state = BeamState(beam_state)\n",
        "    for sub_idx, cluster in enumerate(cluster_seq):\n",
        "      if cluster > len(new_beam_state.mean_set):  # invalid trace\n",
        "        new_beam_state.neg_likelihood = float('inf')\n",
        "        break\n",
        "      elif cluster < len(new_beam_state.mean_set):  # existing cluster\n",
        "        last_cluster = new_beam_state.trace[-1]\n",
        "        loss = weighted_mse_loss(\n",
        "            input_tensor=torch.squeeze(new_beam_state.mean_set[cluster]),\n",
        "            target_tensor=look_ahead_seq[sub_idx, :],\n",
        "            weight=1 / (2 * self.sigma2)).cpu().detach().numpy()\n",
        "        if cluster == last_cluster:\n",
        "          loss -= np.log(1 - self.transition_bias)\n",
        "        else:\n",
        "          loss -= np.log(self.transition_bias) + np.log(\n",
        "              new_beam_state.block_counts[cluster]) - np.log(\n",
        "                  sum(new_beam_state.block_counts) + self.crp_alpha)\n",
        "        # update new mean and new hidden\n",
        "        mean, hidden = self.rnn_model(\n",
        "            look_ahead_seq[sub_idx, :].unsqueeze(0).unsqueeze(0),\n",
        "            new_beam_state.hidden_set[cluster])\n",
        "        new_beam_state.mean_set[cluster] = (new_beam_state.mean_set[cluster]*(\n",
        "            (np.array(new_beam_state.trace) == cluster).sum() -\n",
        "            1).astype(float) + mean.clone()) / (\n",
        "                np.array(new_beam_state.trace) == cluster).sum().astype(\n",
        "                    float)  # use mean to predict\n",
        "        new_beam_state.hidden_set[cluster] = hidden.clone()\n",
        "        if cluster != last_cluster:\n",
        "          new_beam_state.block_counts[cluster] += 1\n",
        "        new_beam_state.trace.append(cluster)\n",
        "      else:  # new cluster\n",
        "        init_input = autograd.Variable(\n",
        "            torch.zeros(self.observation_dim)\n",
        "        ).unsqueeze(0).unsqueeze(0).to(self.device)\n",
        "        mean, hidden = self.rnn_model(init_input,\n",
        "                                      self.rnn_init_hidden)\n",
        "        loss = weighted_mse_loss(\n",
        "            input_tensor=torch.squeeze(mean),\n",
        "            target_tensor=look_ahead_seq[sub_idx, :],\n",
        "            weight=1 / (2 * self.sigma2)).cpu().detach().numpy()\n",
        "        loss -= np.log(self.transition_bias) + np.log(\n",
        "            self.crp_alpha) - np.log(\n",
        "                sum(new_beam_state.block_counts) + self.crp_alpha)\n",
        "        # update new min and new hidden\n",
        "        mean, hidden = self.rnn_model(\n",
        "            look_ahead_seq[sub_idx, :].unsqueeze(0).unsqueeze(0),\n",
        "            hidden)\n",
        "        new_beam_state.append(mean, hidden, cluster)\n",
        "      new_beam_state.neg_likelihood += loss\n",
        "    return new_beam_state\n",
        "\n",
        "  def _calculate_score(self, beam_state, look_ahead_seq):\n",
        "    \"\"\"Calculate negative log likelihoods for all possible state allocations\n",
        "       of a look ahead sequence, according to the current beam state.\n",
        "    Args:\n",
        "      beam_state: A BeamState object.\n",
        "      look_ahead_seq: Look ahead sequence, size: look_ahead*D.\n",
        "        look_ahead: number of step to look ahead in the beam search.\n",
        "        D: observation dimension\n",
        "    Returns:\n",
        "      beam_score_set: a set of scores for each possible state allocation.\n",
        "    \"\"\"\n",
        "\n",
        "    look_ahead, _ = look_ahead_seq.shape\n",
        "    beam_num_clusters = len(beam_state.mean_set)\n",
        "    beam_score_set = float('inf') * np.ones(\n",
        "        beam_num_clusters + 1 + np.arange(look_ahead))\n",
        "    for cluster_seq, _ in np.ndenumerate(beam_score_set):\n",
        "      updated_beam_state = self._update_beam_state(beam_state,\n",
        "                                                   look_ahead_seq, cluster_seq)\n",
        "      beam_score_set[cluster_seq] = updated_beam_state.neg_likelihood\n",
        "    return beam_score_set\n",
        "\n",
        "  def predict_single(self, test_sequence, args):\n",
        "    \"\"\"Predict labels for a single test sequence using UISRNN model.\n",
        "    Args:\n",
        "      test_sequence: the test observation sequence, which is 2-dim numpy array\n",
        "        of real numbers, of size `N * D`.\n",
        "        - `N`: length of one test utterance.\n",
        "        - `D` : observation dimension.\n",
        "        For example:\n",
        "      ```\n",
        "      test_sequence =\n",
        "      [[2.2 -1.0 3.0 5.6]    --> 1st entry of utterance 'iccc'\n",
        "       [0.5 1.8 -3.2 0.4]    --> 2nd entry of utterance 'iccc'\n",
        "       [-2.2 5.0 1.8 3.7]    --> 3rd entry of utterance 'iccc'\n",
        "       [-3.8 0.1 1.4 3.3]    --> 4th entry of utterance 'iccc'\n",
        "       [0.1 2.7 3.5 -1.7]]   --> 5th entry of utterance 'iccc'\n",
        "      ```\n",
        "        Here `N=5`, `D=4`.\n",
        "      args: Inference configurations. See `arguments.py` for details.\n",
        "    Returns:\n",
        "      predicted_cluster_id: predicted speaker id sequence, which is\n",
        "        an array of integers, of size `N`.\n",
        "        For example, `predicted_cluster_id = [0, 1, 0, 0, 1]`\n",
        "    Raises:\n",
        "      TypeError: If test_sequence is of wrong type.\n",
        "      ValueError: If test_sequence has wrong dimension.\n",
        "    \"\"\"\n",
        "    # check type\n",
        "    if (not isinstance(test_sequence, np.ndarray) or\n",
        "        test_sequence.dtype != float):\n",
        "      raise TypeError('test_sequence should be a numpy array of float type.')\n",
        "    # check dimension\n",
        "    if test_sequence.ndim != 2:\n",
        "      raise ValueError('test_sequence must be 2-dim array.')\n",
        "    # check size\n",
        "    test_sequence_length, observation_dim = test_sequence.shape\n",
        "    if observation_dim != self.observation_dim:\n",
        "      raise ValueError('test_sequence does not match the dimension specified '\n",
        "                       'by args.observation_dim.')\n",
        "\n",
        "    self.rnn_model.eval()\n",
        "    test_sequence = np.tile(test_sequence, (args.test_iteration, 1))\n",
        "    test_sequence = autograd.Variable(\n",
        "        torch.from_numpy(test_sequence).float()).to(self.device)\n",
        "    # bookkeeping for beam search\n",
        "    beam_set = [BeamState()]\n",
        "    for num_iter in np.arange(0, args.test_iteration * test_sequence_length,\n",
        "                              args.look_ahead):\n",
        "      max_clusters = max([len(beam_state.mean_set) for beam_state in beam_set])\n",
        "      look_ahead_seq = test_sequence[num_iter:  num_iter + args.look_ahead, :]\n",
        "      look_ahead_seq_length = look_ahead_seq.shape[0]\n",
        "      score_set = float('inf') * np.ones(\n",
        "          np.append(\n",
        "              args.beam_size, max_clusters + 1 + np.arange(\n",
        "                  look_ahead_seq_length)))\n",
        "      for beam_rank, beam_state in enumerate(beam_set):\n",
        "        beam_score_set = self._calculate_score(beam_state, look_ahead_seq)\n",
        "        score_set[beam_rank, :] = np.pad(\n",
        "            beam_score_set,\n",
        "            np.tile([[0, max_clusters - len(beam_state.mean_set)]],\n",
        "                    (look_ahead_seq_length, 1)), 'constant',\n",
        "            constant_values=float('inf'))\n",
        "      # find top scores\n",
        "      score_ranked = np.sort(score_set, axis=None)\n",
        "      score_ranked[score_ranked == float('inf')] = 0\n",
        "      score_ranked = np.trim_zeros(score_ranked)\n",
        "      idx_ranked = np.argsort(score_set, axis=None)\n",
        "      updated_beam_set = []\n",
        "      for new_beam_rank in range(\n",
        "          np.min((len(score_ranked), args.beam_size))):\n",
        "        total_idx = np.unravel_index(idx_ranked[new_beam_rank],\n",
        "                                     score_set.shape)\n",
        "        prev_beam_rank = total_idx[0]\n",
        "        cluster_seq = total_idx[1:]\n",
        "        updated_beam_state = self._update_beam_state(\n",
        "            beam_set[prev_beam_rank], look_ahead_seq, cluster_seq)\n",
        "        updated_beam_set.append(updated_beam_state)\n",
        "      beam_set = updated_beam_set\n",
        "    predicted_cluster_id = beam_set[0].trace[-test_sequence_length:]\n",
        "    return predicted_cluster_id\n",
        "\n",
        "  def predict(self, test_sequences, args):\n",
        "    \"\"\"Predict labels for a single or many test sequences using UISRNN model.\n",
        "    Args:\n",
        "      test_sequences: Either a list of test sequences, or a single test\n",
        "        sequence. Each test sequence is a 2-dim numpy array\n",
        "        of real numbers. See `predict_single()` for details.\n",
        "      args: Inference configurations. See `arguments.py` for details.\n",
        "    Returns:\n",
        "      predicted_cluster_ids: Predicted labels for test_sequences.\n",
        "        1. if test_sequences is a list, predicted_cluster_ids will be a list\n",
        "           of the same size, where each element being a 1-dim list of strings.\n",
        "        2. if test_sequences is a single sequence, predicted_cluster_ids will\n",
        "           be a 1-dim list of strings\n",
        "    Raises:\n",
        "      TypeError: If test_sequences is of wrong type.\n",
        "    \"\"\"\n",
        "    # check type\n",
        "    if isinstance(test_sequences, np.ndarray):\n",
        "      return self.predict_single(test_sequences, args)\n",
        "    if isinstance(test_sequences, list):\n",
        "      return [self.predict_single(test_sequence, args)\n",
        "              for test_sequence in test_sequences]\n",
        "    raise TypeError('test_sequences should be either a list or numpy array.')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It would appear that the code that was provided preprocesses a dataset in preparation for speaker embedding training. An explanation of the code's functions is as follows:\n",
        "\n",
        "1. **Concatenate Continuous Voiced Segments**: The function `concat_segs` takes the start and end times of voiced segments along with the corresponding audio segments and concatenates continuous voiced segments.\n",
        "\n",
        "2. **Compute Short-Time Fourier Transform (STFT) Frames**: The function `get_STFTs` computes STFT frames from the concatenated voiced segments. It uses the librosa library to compute the STFT with a specified window size and hop length.\n",
        "\n",
        "3. **Align Embeddings with STFT Frames**: The function `align_embeddings` aligns the embeddings with the STFT frames by dividing them into partitions. It averages the embeddings within each partition to get aligned embeddings.\n",
        "\n",
        "4. **Data Processing and Embedding Extraction**: The code then processes the dataset, extracting embeddings using a pre-trained SpeechEmbedder model (`embedder_net`). It iterates through the audio files, applies voice activity detection (VAD), computes STFT frames, and aligns the embeddings. The embeddings are then used for training.\n",
        "\n",
        "5. **Save Train and Test Data**: The code saves the train and test data. It splits the total data into 90% for training and 10% for testing. The embeddings and corresponding cluster labels (speaker IDs) are saved in numpy files (`train_sequence.npy`, `train_cluster_id.npy`, `test_sequence.npy`, `test_cluster_id.npy`)."
      ],
      "metadata": {
        "id": "J8wIyKchxLZG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGaYhOsGcZpD",
        "outputId": "18aaa43c-83a7-4a1b-cbda-2e7eec258094"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 100/462 files\n",
            "Processed 200/462 files\n",
            "Processed 300/462 files\n",
            "Processed 400/462 files\n",
            "Processed 500/462 files\n",
            "Processed 600/462 files\n",
            "Processed 700/462 files\n",
            "Processed 800/462 files\n",
            "Processed 900/462 files\n",
            "Processed 1000/462 files\n",
            "Processed 1100/462 files\n",
            "Processed 1200/462 files\n",
            "Processed 1300/462 files\n",
            "Processed 1400/462 files\n",
            "Processed 1500/462 files\n",
            "Processed 1600/462 files\n",
            "Processed 1700/462 files\n",
            "Processed 1800/462 files\n",
            "Processed 1900/462 files\n",
            "Processed 2000/462 files\n",
            "Processed 2100/462 files\n",
            "Processed 2200/462 files\n",
            "Processed 2300/462 files\n",
            "Processed 2400/462 files\n",
            "Processed 2500/462 files\n",
            "Processed 2600/462 files\n",
            "Processed 2700/462 files\n",
            "Processed 2800/462 files\n",
            "Processed 2900/462 files\n",
            "Processed 3000/462 files\n",
            "Processed 3100/462 files\n",
            "Processed 3200/462 files\n",
            "Processed 3300/462 files\n",
            "Processed 3400/462 files\n",
            "Processed 3500/462 files\n",
            "Processed 3600/462 files\n",
            "Processed 3700/462 files\n",
            "Processed 3800/462 files\n",
            "Processed 3900/462 files\n",
            "Processed 4000/462 files\n",
            "Processed 4100/462 files\n",
            "Processed 4200/462 files\n",
            "Processed 4300/462 files\n",
            "Processed 4400/462 files\n",
            "Processed 4500/462 files\n",
            "Processed 4600/462 files\n"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "def concat_segs(times, segs):\n",
        "    #Concatenate continuous voiced segments\n",
        "    concat_seg = []\n",
        "    seg_concat = segs[0]\n",
        "    for i in range(0, len(times)-1):\n",
        "        if times[i][1] == times[i+1][0]:\n",
        "            seg_concat = np.concatenate((seg_concat, segs[i+1]))\n",
        "        else:\n",
        "            concat_seg.append(seg_concat)\n",
        "            seg_concat = segs[i+1]\n",
        "    else:\n",
        "        concat_seg.append(seg_concat)\n",
        "    return concat_seg\n",
        "\n",
        "import librosa\n",
        "\n",
        "def get_STFTs(segs, dict1):\n",
        "    # Get 240ms STFT windows with 50% overlap\n",
        "    sr = 16000\n",
        "    STFT_frames = []\n",
        "\n",
        "    for seg in segs:\n",
        "        # Ensure seg is a NumPy array with floating-point data type\n",
        "        seg = np.asarray(seg)\n",
        "        if seg.dtype != np.float32:\n",
        "            seg = seg.astype(np.float32)\n",
        "\n",
        "        S = librosa.core.stft(y=seg, n_fft=dict1['nfft'],\n",
        "                              win_length=int(dict1['window'] * sr),\n",
        "                              hop_length=int(dict1['hop'] * sr))\n",
        "        S = np.abs(S) ** 2\n",
        "        mel_basis = librosa.filters.mel(sr=sr, n_fft=dict1['nfft'], n_mels=dict1['nmels'])\n",
        "        S = np.log10(np.dot(mel_basis, S) + 1e-6)  # log mel spectrogram of utterances\n",
        "\n",
        "        for j in range(0, S.shape[1], int(0.12 / dict1['hop'])):\n",
        "            if j + 24 < S.shape[1]:\n",
        "                STFT_frames.append(S[:, j:j + 24])\n",
        "            else:\n",
        "                break\n",
        "\n",
        "    return STFT_frames\n",
        "\n",
        "def align_embeddings(embeddings):\n",
        "    partitions = []\n",
        "    start = 0\n",
        "    end = 0\n",
        "    j = 1\n",
        "    for i, embedding in enumerate(embeddings):\n",
        "        if (i*.12)+.24 < j*.401:\n",
        "            end = end + 1\n",
        "        else:\n",
        "            partitions.append((start,end))\n",
        "            start = end\n",
        "            end = end + 1\n",
        "            j += 1\n",
        "    else:\n",
        "        partitions.append((start,end))\n",
        "    avg_embeddings = np.zeros((len(partitions),256))\n",
        "    for i, partition in enumerate(partitions):\n",
        "        avg_embeddings[i] = np.average(embeddings[partition[0]:partition[1]],axis=0)\n",
        "    return avg_embeddings\n",
        "#dataset path\n",
        "audio_path = glob.glob(os.path.dirname(dict1['unprocessed_data']))\n",
        "\n",
        "total_speaker_num = len(audio_path)\n",
        "train_speaker_num= (total_speaker_num//10)*9            # split total data 90% train and 10% test\n",
        "\n",
        "embedder_net = SpeechEmbedder(dict1)\n",
        "embedder_net.load_state_dict(torch.load(\"/content/speech_id_checkpoint/final_epoch_100_batch_id_103.model\"))\n",
        "embedder_net.eval()\n",
        "\n",
        "train_sequence = []\n",
        "train_cluster_id = []\n",
        "label = 0\n",
        "count = 0\n",
        "train_saved = False\n",
        "for i, folder in enumerate(audio_path):\n",
        "    for file in os.listdir(folder):\n",
        "        if file[-4:] == '.wav':\n",
        "            times, segs = VAD_chunk(2, folder+'/'+file)\n",
        "            if segs == []:\n",
        "                print('No voice activity detected')\n",
        "                continue\n",
        "            concat_seg= concat_segs(times, segs)\n",
        "            STFT_frames = get_STFTs(concat_seg,dict1)\n",
        "            STFT_frames = np.stack(STFT_frames, axis=2)\n",
        "            STFT_frames = torch.tensor(np.transpose(STFT_frames, axes=(2,1,0)))\n",
        "            embeddings = embedder_net(STFT_frames)\n",
        "            aligned_embeddings = align_embeddings(embeddings.detach().numpy())\n",
        "            train_sequence.append(aligned_embeddings)\n",
        "            for embedding in aligned_embeddings:\n",
        "                train_cluster_id.append(str(label))\n",
        "            count = count + 1\n",
        "            if count % 100 == 0:\n",
        "                print('Processed {0}/{1} files'.format(count, len(audio_path)))\n",
        "    label = label + 1\n",
        "\n",
        "    if not train_saved and i > train_speaker_num:\n",
        "        train_sequence = np.concatenate(train_sequence,axis=0)\n",
        "        train_cluster_id = np.asarray(train_cluster_id)\n",
        "        np.save('/content/train_sequence',train_sequence)\n",
        "        np.save('/content/train_cluster_id',train_cluster_id)\n",
        "        train_saved = True\n",
        "        train_sequence = []\n",
        "        train_cluster_id = []\n",
        "\n",
        "train_sequence = np.concatenate(train_sequence,axis=0)\n",
        "train_cluster_id = np.asarray(train_cluster_id)\n",
        "np.save('test_sequence',train_sequence)\n",
        "np.save('test_cluster_id',train_cluster_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to perform diarization, also known as speaker segmentation, using the TIMIT dataset, the code that has been provided utilizes the UISRNN model."
      ],
      "metadata": {
        "id": "pjW5VqJLxSfm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSwjXVMUcaKr",
        "outputId": "fb0142d2-a560-4bfc-9929-999fa9d23dff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['453']\n",
            "[0]\n",
            "{'453': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['453']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['453']\n",
            "[0]\n",
            "{'453': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['453']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['453']\n",
            "[0]\n",
            "{'453': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['453']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['453']\n",
            "[0]\n",
            "{'453': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['453']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['453']\n",
            "[0]\n",
            "{'453': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['453']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['453']\n",
            "[0]\n",
            "{'453': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['453']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['453']\n",
            "[0]\n",
            "{'453': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['453']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "Finished diarization experiment\n",
            "Config:\n",
            "  sigma_alpha: 1\n",
            "  sigma_beta: 1\n",
            "  crp_alpha: 1.0\n",
            "  learning rate: 1e-05\n",
            "  regularization: 1e-05\n",
            "  batch size: 20\n",
            "Performance:\n",
            "  averaged accuracy: 1.000000\n",
            "  accuracy numbers for all testing sequences:\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "SAVE_MODEL_NAME = '/content/saved_model_timit.uisrnn'\n",
        "#model training on timit dataset\n",
        "\n",
        "def diarization_experiment(args):\n",
        "  \"\"\"Experiment pipeline.\n",
        "  Load data --> train model --> test model --> output result\n",
        "  Args:\n",
        "    model_args: model configurations\n",
        "    training_args: training configurations\n",
        "    inference_args: inference configurations\n",
        "  \"\"\"\n",
        "\n",
        "  predicted_cluster_ids = []\n",
        "  test_record = []\n",
        "\n",
        "  train_sequence = np.load('/content/train_sequence.npy')\n",
        "  test_sequence= np.load('test_sequence.npy')\n",
        "  train_cluster_id= np.load('train_cluster_id.npy')\n",
        "  test_cluster_id= np.load('test_cluster_id.npy')\n",
        "  test_cluster_id=test_cluster_id.reshape(-1,1)\n",
        "  model = UISRNN(args)\n",
        "  # training\n",
        "  model.fit(train_sequence, train_cluster_id, args)\n",
        "  model.save(SAVE_MODEL_NAME)\n",
        "  # we can also skip training by calling：\n",
        "  #model.load(SAVED_MODEL_NAME)\n",
        "\n",
        "  # testing\n",
        "  for (test_sequence, test_cluster_id) in zip(test_sequence, test_cluster_id):\n",
        "    predicted_cluster_id = model.predict(test_sequence.reshape(1,-1), args)\n",
        "    predicted_cluster_ids.append(predicted_cluster_id)\n",
        "    print(type(predicted_cluster_id))\n",
        "    accuracy = compute_sequence_match_accuracy(\n",
        "        test_cluster_id, predicted_cluster_id)\n",
        "    test_record.append((accuracy, len(test_cluster_id)))\n",
        "    print('Ground truth labels:')\n",
        "    print(test_cluster_id)\n",
        "    print('Predicted labels:')\n",
        "    print(predicted_cluster_id)\n",
        "    print('-' * 80)\n",
        "  output_string = output_result(args,args,test_record)\n",
        "\n",
        "  print('Finished diarization experiment')\n",
        "  print(output_string)\n",
        "args=arguments()\n",
        "diarization_experiment(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Toy Datasets"
      ],
      "metadata": {
        "id": "F1y4auW81Q5q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " In order to check that your diarization model is functioning appropriately on a smaller dataset before applying it to larger datasets, it is a good practice to go through this procedure. On the other hand, I saw that the code makes reference to classes and functions that are not given in the code sample. These include UISRNN, compute_sequence_match_accuracy, output_result, and arguments."
      ],
      "metadata": {
        "id": "TdvfUJSExa5P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofW91kRHcabK",
        "outputId": "4431a995-37ae-4679-d2c6-8dcd9261c8a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter: 0  \tTraining Loss: -286.8101    \n",
            "    Negative Log Likelihood: 6.0257\tSigma2 Prior: -292.8364\tRegularization: 0.0006\n",
            "Iter: 10  \tTraining Loss: -287.0862    \n",
            "    Negative Log Likelihood: 5.9056\tSigma2 Prior: -292.9924\tRegularization: 0.0006\n",
            "Iter: 20  \tTraining Loss: -286.7981    \n",
            "    Negative Log Likelihood: 5.9139\tSigma2 Prior: -292.7126\tRegularization: 0.0006\n",
            "Iter: 30  \tTraining Loss: -287.0063    \n",
            "    Negative Log Likelihood: 5.9444\tSigma2 Prior: -292.9514\tRegularization: 0.0006\n",
            "Iter: 40  \tTraining Loss: -287.5560    \n",
            "    Negative Log Likelihood: 5.8618\tSigma2 Prior: -293.4185\tRegularization: 0.0006\n",
            "Iter: 50  \tTraining Loss: -287.5437    \n",
            "    Negative Log Likelihood: 5.7549\tSigma2 Prior: -293.2992\tRegularization: 0.0006\n",
            "Iter: 60  \tTraining Loss: -288.3514    \n",
            "    Negative Log Likelihood: 5.6814\tSigma2 Prior: -294.0335\tRegularization: 0.0006\n",
            "Iter: 70  \tTraining Loss: -287.6693    \n",
            "    Negative Log Likelihood: 5.6547\tSigma2 Prior: -293.3246\tRegularization: 0.0006\n",
            "Iter: 80  \tTraining Loss: -287.9300    \n",
            "    Negative Log Likelihood: 5.5974\tSigma2 Prior: -293.5281\tRegularization: 0.0006\n",
            "Iter: 90  \tTraining Loss: -288.1609    \n",
            "    Negative Log Likelihood: 5.6080\tSigma2 Prior: -293.7696\tRegularization: 0.0006\n",
            "Iter: 100  \tTraining Loss: -287.9870    \n",
            "    Negative Log Likelihood: 5.5250\tSigma2 Prior: -293.5126\tRegularization: 0.0006\n",
            "Iter: 110  \tTraining Loss: -288.6134    \n",
            "    Negative Log Likelihood: 5.4348\tSigma2 Prior: -294.0488\tRegularization: 0.0006\n",
            "Iter: 120  \tTraining Loss: -287.8799    \n",
            "    Negative Log Likelihood: 5.4773\tSigma2 Prior: -293.3578\tRegularization: 0.0006\n",
            "Iter: 130  \tTraining Loss: -288.7422    \n",
            "    Negative Log Likelihood: 5.4045\tSigma2 Prior: -294.1474\tRegularization: 0.0006\n",
            "Iter: 140  \tTraining Loss: -288.8672    \n",
            "    Negative Log Likelihood: 5.3734\tSigma2 Prior: -294.2412\tRegularization: 0.0006\n",
            "Iter: 150  \tTraining Loss: -289.0297    \n",
            "    Negative Log Likelihood: 5.3604\tSigma2 Prior: -294.3908\tRegularization: 0.0006\n",
            "Iter: 160  \tTraining Loss: -289.3311    \n",
            "    Negative Log Likelihood: 5.3154\tSigma2 Prior: -294.6472\tRegularization: 0.0006\n",
            "Iter: 170  \tTraining Loss: -289.1942    \n",
            "    Negative Log Likelihood: 5.2847\tSigma2 Prior: -294.4795\tRegularization: 0.0006\n",
            "Iter: 180  \tTraining Loss: -290.3954    \n",
            "    Negative Log Likelihood: 5.2355\tSigma2 Prior: -295.6316\tRegularization: 0.0006\n",
            "Iter: 190  \tTraining Loss: -289.2277    \n",
            "    Negative Log Likelihood: 5.2681\tSigma2 Prior: -294.4964\tRegularization: 0.0006\n",
            "Iter: 200  \tTraining Loss: -289.5467    \n",
            "    Negative Log Likelihood: 5.2043\tSigma2 Prior: -294.7517\tRegularization: 0.0006\n",
            "Iter: 210  \tTraining Loss: -290.4354    \n",
            "    Negative Log Likelihood: 5.1914\tSigma2 Prior: -295.6274\tRegularization: 0.0006\n",
            "Iter: 220  \tTraining Loss: -290.1248    \n",
            "    Negative Log Likelihood: 5.2485\tSigma2 Prior: -295.3740\tRegularization: 0.0006\n",
            "Iter: 230  \tTraining Loss: -290.5817    \n",
            "    Negative Log Likelihood: 5.1991\tSigma2 Prior: -295.7814\tRegularization: 0.0006\n",
            "Iter: 240  \tTraining Loss: -291.0137    \n",
            "    Negative Log Likelihood: 5.1762\tSigma2 Prior: -296.1905\tRegularization: 0.0006\n",
            "Iter: 250  \tTraining Loss: -290.6672    \n",
            "    Negative Log Likelihood: 5.1582\tSigma2 Prior: -295.8260\tRegularization: 0.0006\n",
            "Iter: 260  \tTraining Loss: -290.9463    \n",
            "    Negative Log Likelihood: 5.1724\tSigma2 Prior: -296.1194\tRegularization: 0.0006\n",
            "Iter: 270  \tTraining Loss: -290.8252    \n",
            "    Negative Log Likelihood: 5.1675\tSigma2 Prior: -295.9933\tRegularization: 0.0006\n",
            "Iter: 280  \tTraining Loss: -290.8848    \n",
            "    Negative Log Likelihood: 5.1719\tSigma2 Prior: -296.0574\tRegularization: 0.0006\n",
            "Iter: 290  \tTraining Loss: -291.4166    \n",
            "    Negative Log Likelihood: 5.1564\tSigma2 Prior: -296.5736\tRegularization: 0.0006\n",
            "Iter: 300  \tTraining Loss: -291.4160    \n",
            "    Negative Log Likelihood: 5.1730\tSigma2 Prior: -296.5896\tRegularization: 0.0006\n",
            "Iter: 310  \tTraining Loss: -291.4380    \n",
            "    Negative Log Likelihood: 5.1691\tSigma2 Prior: -296.6078\tRegularization: 0.0006\n",
            "Iter: 320  \tTraining Loss: -291.4304    \n",
            "    Negative Log Likelihood: 5.2002\tSigma2 Prior: -296.6312\tRegularization: 0.0006\n",
            "Iter: 330  \tTraining Loss: -292.1161    \n",
            "    Negative Log Likelihood: 5.1715\tSigma2 Prior: -297.2882\tRegularization: 0.0006\n",
            "Iter: 340  \tTraining Loss: -292.1095    \n",
            "    Negative Log Likelihood: 5.1718\tSigma2 Prior: -297.2820\tRegularization: 0.0006\n",
            "Iter: 350  \tTraining Loss: -292.3077    \n",
            "    Negative Log Likelihood: 5.1808\tSigma2 Prior: -297.4891\tRegularization: 0.0006\n",
            "Iter: 360  \tTraining Loss: -291.9010    \n",
            "    Negative Log Likelihood: 5.1867\tSigma2 Prior: -297.0883\tRegularization: 0.0006\n",
            "Iter: 370  \tTraining Loss: -292.0669    \n",
            "    Negative Log Likelihood: 5.1832\tSigma2 Prior: -297.2507\tRegularization: 0.0006\n",
            "Iter: 380  \tTraining Loss: -292.4261    \n",
            "    Negative Log Likelihood: 5.1912\tSigma2 Prior: -297.6180\tRegularization: 0.0006\n",
            "Iter: 390  \tTraining Loss: -293.2346    \n",
            "    Negative Log Likelihood: 5.1792\tSigma2 Prior: -298.4145\tRegularization: 0.0006\n",
            "Iter: 400  \tTraining Loss: -292.7094    \n",
            "    Negative Log Likelihood: 5.1941\tSigma2 Prior: -297.9041\tRegularization: 0.0006\n",
            "Iter: 410  \tTraining Loss: -292.5971    \n",
            "    Negative Log Likelihood: 5.2072\tSigma2 Prior: -297.8050\tRegularization: 0.0006\n",
            "Iter: 420  \tTraining Loss: -292.9024    \n",
            "    Negative Log Likelihood: 5.2010\tSigma2 Prior: -298.1041\tRegularization: 0.0006\n",
            "Iter: 430  \tTraining Loss: -292.1521    \n",
            "    Negative Log Likelihood: 5.2252\tSigma2 Prior: -297.3780\tRegularization: 0.0006\n",
            "Iter: 440  \tTraining Loss: -293.0820    \n",
            "    Negative Log Likelihood: 5.1932\tSigma2 Prior: -298.2758\tRegularization: 0.0006\n",
            "Iter: 450  \tTraining Loss: -293.3443    \n",
            "    Negative Log Likelihood: 5.2248\tSigma2 Prior: -298.5697\tRegularization: 0.0006\n",
            "Iter: 460  \tTraining Loss: -293.3979    \n",
            "    Negative Log Likelihood: 5.2390\tSigma2 Prior: -298.6375\tRegularization: 0.0006\n",
            "Iter: 470  \tTraining Loss: -293.6724    \n",
            "    Negative Log Likelihood: 5.2267\tSigma2 Prior: -298.8997\tRegularization: 0.0006\n",
            "Iter: 480  \tTraining Loss: -293.5424    \n",
            "    Negative Log Likelihood: 5.2202\tSigma2 Prior: -298.7633\tRegularization: 0.0006\n",
            "Iter: 490  \tTraining Loss: -294.0395    \n",
            "    Negative Log Likelihood: 5.2347\tSigma2 Prior: -299.2748\tRegularization: 0.0006\n",
            "Iter: 500  \tTraining Loss: -293.3711    \n",
            "    Negative Log Likelihood: 5.2513\tSigma2 Prior: -298.6230\tRegularization: 0.0006\n",
            "Iter: 510  \tTraining Loss: -294.0262    \n",
            "    Negative Log Likelihood: 5.2411\tSigma2 Prior: -299.2679\tRegularization: 0.0006\n",
            "Iter: 520  \tTraining Loss: -293.9470    \n",
            "    Negative Log Likelihood: 5.2674\tSigma2 Prior: -299.2150\tRegularization: 0.0006\n",
            "Iter: 530  \tTraining Loss: -294.5542    \n",
            "    Negative Log Likelihood: 5.2542\tSigma2 Prior: -299.8090\tRegularization: 0.0006\n",
            "Iter: 540  \tTraining Loss: -294.4088    \n",
            "    Negative Log Likelihood: 5.2455\tSigma2 Prior: -299.6549\tRegularization: 0.0006\n",
            "Iter: 550  \tTraining Loss: -294.7886    \n",
            "    Negative Log Likelihood: 5.2671\tSigma2 Prior: -300.0563\tRegularization: 0.0006\n",
            "Iter: 560  \tTraining Loss: -294.7225    \n",
            "    Negative Log Likelihood: 5.2668\tSigma2 Prior: -299.9900\tRegularization: 0.0006\n",
            "Iter: 570  \tTraining Loss: -294.4130    \n",
            "    Negative Log Likelihood: 5.2908\tSigma2 Prior: -299.7044\tRegularization: 0.0006\n",
            "Iter: 580  \tTraining Loss: -295.2551    \n",
            "    Negative Log Likelihood: 5.2565\tSigma2 Prior: -300.5122\tRegularization: 0.0006\n",
            "Iter: 590  \tTraining Loss: -295.2867    \n",
            "    Negative Log Likelihood: 5.2721\tSigma2 Prior: -300.5594\tRegularization: 0.0006\n",
            "Iter: 600  \tTraining Loss: -295.5142    \n",
            "    Negative Log Likelihood: 5.2707\tSigma2 Prior: -300.7855\tRegularization: 0.0006\n",
            "Iter: 610  \tTraining Loss: -295.1567    \n",
            "    Negative Log Likelihood: 5.2920\tSigma2 Prior: -300.4493\tRegularization: 0.0006\n",
            "Iter: 620  \tTraining Loss: -295.3385    \n",
            "    Negative Log Likelihood: 5.2975\tSigma2 Prior: -300.6367\tRegularization: 0.0006\n",
            "Iter: 630  \tTraining Loss: -295.5953    \n",
            "    Negative Log Likelihood: 5.2909\tSigma2 Prior: -300.8868\tRegularization: 0.0006\n",
            "Iter: 640  \tTraining Loss: -296.1054    \n",
            "    Negative Log Likelihood: 5.2872\tSigma2 Prior: -301.3933\tRegularization: 0.0006\n",
            "Iter: 650  \tTraining Loss: -296.1652    \n",
            "    Negative Log Likelihood: 5.2991\tSigma2 Prior: -301.4650\tRegularization: 0.0006\n",
            "Iter: 660  \tTraining Loss: -296.1104    \n",
            "    Negative Log Likelihood: 5.3073\tSigma2 Prior: -301.4183\tRegularization: 0.0006\n",
            "Iter: 670  \tTraining Loss: -296.1120    \n",
            "    Negative Log Likelihood: 5.2991\tSigma2 Prior: -301.4117\tRegularization: 0.0006\n",
            "Iter: 680  \tTraining Loss: -296.4797    \n",
            "    Negative Log Likelihood: 5.2843\tSigma2 Prior: -301.7647\tRegularization: 0.0006\n",
            "Iter: 690  \tTraining Loss: -296.5158    \n",
            "    Negative Log Likelihood: 5.3132\tSigma2 Prior: -301.8296\tRegularization: 0.0006\n",
            "Iter: 700  \tTraining Loss: -296.2038    \n",
            "    Negative Log Likelihood: 5.3441\tSigma2 Prior: -301.5485\tRegularization: 0.0006\n",
            "Iter: 710  \tTraining Loss: -296.4590    \n",
            "    Negative Log Likelihood: 5.3512\tSigma2 Prior: -301.8108\tRegularization: 0.0006\n",
            "Iter: 720  \tTraining Loss: -296.8915    \n",
            "    Negative Log Likelihood: 5.3341\tSigma2 Prior: -302.2262\tRegularization: 0.0006\n",
            "Iter: 730  \tTraining Loss: -297.5997    \n",
            "    Negative Log Likelihood: 5.3143\tSigma2 Prior: -302.9147\tRegularization: 0.0006\n",
            "Iter: 740  \tTraining Loss: -297.1854    \n",
            "    Negative Log Likelihood: 5.3379\tSigma2 Prior: -302.5240\tRegularization: 0.0006\n",
            "Iter: 750  \tTraining Loss: -296.0945    \n",
            "    Negative Log Likelihood: 5.3569\tSigma2 Prior: -301.4520\tRegularization: 0.0006\n",
            "Iter: 760  \tTraining Loss: -297.1955    \n",
            "    Negative Log Likelihood: 5.3610\tSigma2 Prior: -302.5571\tRegularization: 0.0006\n",
            "Iter: 770  \tTraining Loss: -297.8340    \n",
            "    Negative Log Likelihood: 5.3387\tSigma2 Prior: -303.1734\tRegularization: 0.0006\n",
            "Iter: 780  \tTraining Loss: -297.9685    \n",
            "    Negative Log Likelihood: 5.3552\tSigma2 Prior: -303.3243\tRegularization: 0.0006\n",
            "Iter: 790  \tTraining Loss: -298.2498    \n",
            "    Negative Log Likelihood: 5.3575\tSigma2 Prior: -303.6080\tRegularization: 0.0006\n",
            "Iter: 800  \tTraining Loss: -297.6457    \n",
            "    Negative Log Likelihood: 5.3749\tSigma2 Prior: -303.0213\tRegularization: 0.0006\n",
            "Iter: 810  \tTraining Loss: -298.0705    \n",
            "    Negative Log Likelihood: 5.3665\tSigma2 Prior: -303.4377\tRegularization: 0.0006\n",
            "Iter: 820  \tTraining Loss: -298.2409    \n",
            "    Negative Log Likelihood: 5.3673\tSigma2 Prior: -303.6088\tRegularization: 0.0006\n",
            "Iter: 830  \tTraining Loss: -297.9145    \n",
            "    Negative Log Likelihood: 5.3811\tSigma2 Prior: -303.2962\tRegularization: 0.0006\n",
            "Iter: 840  \tTraining Loss: -298.0083    \n",
            "    Negative Log Likelihood: 5.3885\tSigma2 Prior: -303.3975\tRegularization: 0.0006\n",
            "Iter: 850  \tTraining Loss: -298.1437    \n",
            "    Negative Log Likelihood: 5.3986\tSigma2 Prior: -303.5430\tRegularization: 0.0006\n",
            "Iter: 860  \tTraining Loss: -298.4950    \n",
            "    Negative Log Likelihood: 5.4071\tSigma2 Prior: -303.9028\tRegularization: 0.0006\n",
            "Iter: 870  \tTraining Loss: -299.3325    \n",
            "    Negative Log Likelihood: 5.4020\tSigma2 Prior: -304.7351\tRegularization: 0.0006\n",
            "Iter: 880  \tTraining Loss: -298.9893    \n",
            "    Negative Log Likelihood: 5.3836\tSigma2 Prior: -304.3735\tRegularization: 0.0006\n",
            "Iter: 890  \tTraining Loss: -298.8451    \n",
            "    Negative Log Likelihood: 5.4302\tSigma2 Prior: -304.2758\tRegularization: 0.0006\n",
            "Iter: 900  \tTraining Loss: -299.7223    \n",
            "    Negative Log Likelihood: 5.3801\tSigma2 Prior: -305.1030\tRegularization: 0.0006\n",
            "Iter: 910  \tTraining Loss: -299.4019    \n",
            "    Negative Log Likelihood: 5.4161\tSigma2 Prior: -304.8186\tRegularization: 0.0006\n",
            "Iter: 920  \tTraining Loss: -299.0882    \n",
            "    Negative Log Likelihood: 5.4140\tSigma2 Prior: -304.5028\tRegularization: 0.0006\n",
            "Iter: 930  \tTraining Loss: -299.6648    \n",
            "    Negative Log Likelihood: 5.4318\tSigma2 Prior: -305.0973\tRegularization: 0.0006\n",
            "Iter: 940  \tTraining Loss: -300.1612    \n",
            "    Negative Log Likelihood: 5.4131\tSigma2 Prior: -305.5750\tRegularization: 0.0006\n",
            "Iter: 950  \tTraining Loss: -300.4806    \n",
            "    Negative Log Likelihood: 5.4152\tSigma2 Prior: -305.8965\tRegularization: 0.0006\n",
            "Iter: 960  \tTraining Loss: -299.8188    \n",
            "    Negative Log Likelihood: 5.4449\tSigma2 Prior: -305.2643\tRegularization: 0.0006\n",
            "Iter: 970  \tTraining Loss: -300.1701    \n",
            "    Negative Log Likelihood: 5.4441\tSigma2 Prior: -305.6149\tRegularization: 0.0006\n",
            "Iter: 980  \tTraining Loss: -300.5389    \n",
            "    Negative Log Likelihood: 5.4359\tSigma2 Prior: -305.9755\tRegularization: 0.0006\n",
            "Iter: 990  \tTraining Loss: -300.5242    \n",
            "    Negative Log Likelihood: 5.4706\tSigma2 Prior: -305.9954\tRegularization: 0.0006\n",
            "Iter: 1000  \tTraining Loss: -300.6353    \n",
            "    Negative Log Likelihood: 5.4383\tSigma2 Prior: -306.0742\tRegularization: 0.0006\n",
            "Iter: 1010  \tTraining Loss: -300.7287    \n",
            "    Negative Log Likelihood: 5.4745\tSigma2 Prior: -306.2038\tRegularization: 0.0006\n",
            "Iter: 1020  \tTraining Loss: -301.3511    \n",
            "    Negative Log Likelihood: 5.4627\tSigma2 Prior: -306.8144\tRegularization: 0.0006\n",
            "Iter: 1030  \tTraining Loss: -301.0740    \n",
            "    Negative Log Likelihood: 5.4607\tSigma2 Prior: -306.5353\tRegularization: 0.0006\n",
            "Iter: 1040  \tTraining Loss: -301.4832    \n",
            "    Negative Log Likelihood: 5.4817\tSigma2 Prior: -306.9656\tRegularization: 0.0006\n",
            "Iter: 1050  \tTraining Loss: -301.0095    \n",
            "    Negative Log Likelihood: 5.4689\tSigma2 Prior: -306.4791\tRegularization: 0.0006\n",
            "Iter: 1060  \tTraining Loss: -301.6874    \n",
            "    Negative Log Likelihood: 5.4822\tSigma2 Prior: -307.1703\tRegularization: 0.0006\n",
            "Iter: 1070  \tTraining Loss: -301.7130    \n",
            "    Negative Log Likelihood: 5.4976\tSigma2 Prior: -307.2112\tRegularization: 0.0006\n",
            "Iter: 1080  \tTraining Loss: -301.9699    \n",
            "    Negative Log Likelihood: 5.4704\tSigma2 Prior: -307.4410\tRegularization: 0.0006\n",
            "Iter: 1090  \tTraining Loss: -302.1666    \n",
            "    Negative Log Likelihood: 5.4984\tSigma2 Prior: -307.6656\tRegularization: 0.0006\n",
            "Iter: 1100  \tTraining Loss: -301.9428    \n",
            "    Negative Log Likelihood: 5.5046\tSigma2 Prior: -307.4480\tRegularization: 0.0006\n",
            "Iter: 1110  \tTraining Loss: -302.6050    \n",
            "    Negative Log Likelihood: 5.4875\tSigma2 Prior: -308.0931\tRegularization: 0.0006\n",
            "Iter: 1120  \tTraining Loss: -301.9356    \n",
            "    Negative Log Likelihood: 5.5289\tSigma2 Prior: -307.4651\tRegularization: 0.0006\n",
            "Iter: 1130  \tTraining Loss: -302.9649    \n",
            "    Negative Log Likelihood: 5.5105\tSigma2 Prior: -308.4760\tRegularization: 0.0006\n",
            "Iter: 1140  \tTraining Loss: -302.9473    \n",
            "    Negative Log Likelihood: 5.4851\tSigma2 Prior: -308.4330\tRegularization: 0.0006\n",
            "Iter: 1150  \tTraining Loss: -302.1679    \n",
            "    Negative Log Likelihood: 5.5326\tSigma2 Prior: -307.7012\tRegularization: 0.0006\n",
            "Iter: 1160  \tTraining Loss: -303.2028    \n",
            "    Negative Log Likelihood: 5.5056\tSigma2 Prior: -308.7090\tRegularization: 0.0006\n",
            "Iter: 1170  \tTraining Loss: -303.0536    \n",
            "    Negative Log Likelihood: 5.5170\tSigma2 Prior: -308.5712\tRegularization: 0.0006\n",
            "Iter: 1180  \tTraining Loss: -302.4911    \n",
            "    Negative Log Likelihood: 5.5738\tSigma2 Prior: -308.0655\tRegularization: 0.0006\n",
            "Iter: 1190  \tTraining Loss: -303.4402    \n",
            "    Negative Log Likelihood: 5.5452\tSigma2 Prior: -308.9861\tRegularization: 0.0006\n",
            "Iter: 1200  \tTraining Loss: -304.1654    \n",
            "    Negative Log Likelihood: 5.4947\tSigma2 Prior: -309.6607\tRegularization: 0.0006\n",
            "Iter: 1210  \tTraining Loss: -303.9419    \n",
            "    Negative Log Likelihood: 5.5119\tSigma2 Prior: -309.4544\tRegularization: 0.0006\n",
            "Iter: 1220  \tTraining Loss: -303.9772    \n",
            "    Negative Log Likelihood: 5.5605\tSigma2 Prior: -309.5383\tRegularization: 0.0006\n",
            "Iter: 1230  \tTraining Loss: -303.7874    \n",
            "    Negative Log Likelihood: 5.5674\tSigma2 Prior: -309.3554\tRegularization: 0.0006\n",
            "Iter: 1240  \tTraining Loss: -304.7751    \n",
            "    Negative Log Likelihood: 5.5519\tSigma2 Prior: -310.3276\tRegularization: 0.0006\n",
            "Iter: 1250  \tTraining Loss: -303.4294    \n",
            "    Negative Log Likelihood: 5.6094\tSigma2 Prior: -309.0394\tRegularization: 0.0006\n",
            "Iter: 1260  \tTraining Loss: -304.2539    \n",
            "    Negative Log Likelihood: 5.5797\tSigma2 Prior: -309.8342\tRegularization: 0.0006\n",
            "Iter: 1270  \tTraining Loss: -304.2805    \n",
            "    Negative Log Likelihood: 5.5722\tSigma2 Prior: -309.8534\tRegularization: 0.0006\n",
            "Iter: 1280  \tTraining Loss: -304.5447    \n",
            "    Negative Log Likelihood: 5.6062\tSigma2 Prior: -310.1515\tRegularization: 0.0006\n",
            "Iter: 1290  \tTraining Loss: -304.9953    \n",
            "    Negative Log Likelihood: 5.5933\tSigma2 Prior: -310.5893\tRegularization: 0.0006\n",
            "Iter: 1300  \tTraining Loss: -304.2450    \n",
            "    Negative Log Likelihood: 5.6106\tSigma2 Prior: -309.8562\tRegularization: 0.0006\n",
            "Iter: 1310  \tTraining Loss: -305.7551    \n",
            "    Negative Log Likelihood: 5.5631\tSigma2 Prior: -311.3188\tRegularization: 0.0006\n",
            "Iter: 1320  \tTraining Loss: -305.6080    \n",
            "    Negative Log Likelihood: 5.5871\tSigma2 Prior: -311.1957\tRegularization: 0.0006\n",
            "Iter: 1330  \tTraining Loss: -305.2424    \n",
            "    Negative Log Likelihood: 5.5966\tSigma2 Prior: -310.8397\tRegularization: 0.0006\n",
            "Iter: 1340  \tTraining Loss: -305.6428    \n",
            "    Negative Log Likelihood: 5.6166\tSigma2 Prior: -311.2599\tRegularization: 0.0006\n",
            "Iter: 1350  \tTraining Loss: -305.5533    \n",
            "    Negative Log Likelihood: 5.6244\tSigma2 Prior: -311.1784\tRegularization: 0.0006\n",
            "Iter: 1360  \tTraining Loss: -306.5955    \n",
            "    Negative Log Likelihood: 5.5599\tSigma2 Prior: -312.1561\tRegularization: 0.0006\n",
            "Iter: 1370  \tTraining Loss: -305.9652    \n",
            "    Negative Log Likelihood: 5.6009\tSigma2 Prior: -311.5667\tRegularization: 0.0006\n",
            "Iter: 1380  \tTraining Loss: -306.4680    \n",
            "    Negative Log Likelihood: 5.6193\tSigma2 Prior: -312.0879\tRegularization: 0.0006\n",
            "Iter: 1390  \tTraining Loss: -306.2835    \n",
            "    Negative Log Likelihood: 5.6285\tSigma2 Prior: -311.9127\tRegularization: 0.0006\n",
            "Iter: 1400  \tTraining Loss: -306.2915    \n",
            "    Negative Log Likelihood: 5.6331\tSigma2 Prior: -311.9252\tRegularization: 0.0006\n",
            "Iter: 1410  \tTraining Loss: -306.6409    \n",
            "    Negative Log Likelihood: 5.6170\tSigma2 Prior: -312.2586\tRegularization: 0.0006\n",
            "Iter: 1420  \tTraining Loss: -306.9891    \n",
            "    Negative Log Likelihood: 5.6242\tSigma2 Prior: -312.6139\tRegularization: 0.0006\n",
            "Iter: 1430  \tTraining Loss: -307.7173    \n",
            "    Negative Log Likelihood: 5.6040\tSigma2 Prior: -313.3219\tRegularization: 0.0006\n",
            "Iter: 1440  \tTraining Loss: -307.0017    \n",
            "    Negative Log Likelihood: 5.6605\tSigma2 Prior: -312.6629\tRegularization: 0.0006\n",
            "Iter: 1450  \tTraining Loss: -307.2724    \n",
            "    Negative Log Likelihood: 5.6893\tSigma2 Prior: -312.9624\tRegularization: 0.0006\n",
            "Iter: 1460  \tTraining Loss: -307.2112    \n",
            "    Negative Log Likelihood: 5.6735\tSigma2 Prior: -312.8854\tRegularization: 0.0006\n",
            "Iter: 1470  \tTraining Loss: -307.9081    \n",
            "    Negative Log Likelihood: 5.6295\tSigma2 Prior: -313.5383\tRegularization: 0.0006\n",
            "Iter: 1480  \tTraining Loss: -308.0519    \n",
            "    Negative Log Likelihood: 5.6506\tSigma2 Prior: -313.7032\tRegularization: 0.0006\n",
            "Iter: 1490  \tTraining Loss: -307.4726    \n",
            "    Negative Log Likelihood: 5.6836\tSigma2 Prior: -313.1569\tRegularization: 0.0006\n",
            "Iter: 1500  \tTraining Loss: -308.3205    \n",
            "    Negative Log Likelihood: 5.6419\tSigma2 Prior: -313.9630\tRegularization: 0.0006\n",
            "Iter: 1510  \tTraining Loss: -308.4607    \n",
            "    Negative Log Likelihood: 5.6708\tSigma2 Prior: -314.1321\tRegularization: 0.0006\n",
            "Iter: 1520  \tTraining Loss: -308.3392    \n",
            "    Negative Log Likelihood: 5.7110\tSigma2 Prior: -314.0508\tRegularization: 0.0006\n",
            "Iter: 1530  \tTraining Loss: -308.4671    \n",
            "    Negative Log Likelihood: 5.6991\tSigma2 Prior: -314.1669\tRegularization: 0.0006\n",
            "Iter: 1540  \tTraining Loss: -307.8993    \n",
            "    Negative Log Likelihood: 5.7092\tSigma2 Prior: -313.6092\tRegularization: 0.0006\n",
            "Iter: 1550  \tTraining Loss: -308.7345    \n",
            "    Negative Log Likelihood: 5.7117\tSigma2 Prior: -314.4469\tRegularization: 0.0006\n",
            "Iter: 1560  \tTraining Loss: -308.6688    \n",
            "    Negative Log Likelihood: 5.7333\tSigma2 Prior: -314.4027\tRegularization: 0.0006\n",
            "Iter: 1570  \tTraining Loss: -308.9377    \n",
            "    Negative Log Likelihood: 5.7140\tSigma2 Prior: -314.6523\tRegularization: 0.0006\n",
            "Iter: 1580  \tTraining Loss: -308.7096    \n",
            "    Negative Log Likelihood: 5.7451\tSigma2 Prior: -314.4554\tRegularization: 0.0006\n",
            "Iter: 1590  \tTraining Loss: -308.4839    \n",
            "    Negative Log Likelihood: 5.7582\tSigma2 Prior: -314.2427\tRegularization: 0.0006\n",
            "Iter: 1600  \tTraining Loss: -309.1545    \n",
            "    Negative Log Likelihood: 5.7403\tSigma2 Prior: -314.8955\tRegularization: 0.0006\n",
            "Iter: 1610  \tTraining Loss: -310.0243    \n",
            "    Negative Log Likelihood: 5.7227\tSigma2 Prior: -315.7477\tRegularization: 0.0006\n",
            "Iter: 1620  \tTraining Loss: -309.7821    \n",
            "    Negative Log Likelihood: 5.7358\tSigma2 Prior: -315.5185\tRegularization: 0.0006\n",
            "Iter: 1630  \tTraining Loss: -310.9544    \n",
            "    Negative Log Likelihood: 5.6954\tSigma2 Prior: -316.6505\tRegularization: 0.0006\n",
            "Iter: 1640  \tTraining Loss: -310.0649    \n",
            "    Negative Log Likelihood: 5.7386\tSigma2 Prior: -315.8042\tRegularization: 0.0006\n",
            "Iter: 1650  \tTraining Loss: -310.8170    \n",
            "    Negative Log Likelihood: 5.7153\tSigma2 Prior: -316.5330\tRegularization: 0.0006\n",
            "Iter: 1660  \tTraining Loss: -311.2283    \n",
            "    Negative Log Likelihood: 5.7037\tSigma2 Prior: -316.9326\tRegularization: 0.0006\n",
            "Iter: 1670  \tTraining Loss: -311.1143    \n",
            "    Negative Log Likelihood: 5.7424\tSigma2 Prior: -316.8574\tRegularization: 0.0006\n",
            "Iter: 1680  \tTraining Loss: -310.8781    \n",
            "    Negative Log Likelihood: 5.8026\tSigma2 Prior: -316.6813\tRegularization: 0.0006\n",
            "Iter: 1690  \tTraining Loss: -309.9434    \n",
            "    Negative Log Likelihood: 5.8292\tSigma2 Prior: -315.7732\tRegularization: 0.0006\n",
            "Iter: 1700  \tTraining Loss: -310.8640    \n",
            "    Negative Log Likelihood: 5.7950\tSigma2 Prior: -316.6597\tRegularization: 0.0006\n",
            "Iter: 1710  \tTraining Loss: -311.4967    \n",
            "    Negative Log Likelihood: 5.8168\tSigma2 Prior: -317.3142\tRegularization: 0.0006\n",
            "Iter: 1720  \tTraining Loss: -311.5811    \n",
            "    Negative Log Likelihood: 5.7798\tSigma2 Prior: -317.3616\tRegularization: 0.0006\n",
            "Iter: 1730  \tTraining Loss: -312.1511    \n",
            "    Negative Log Likelihood: 5.7763\tSigma2 Prior: -317.9280\tRegularization: 0.0006\n",
            "Iter: 1740  \tTraining Loss: -310.8849    \n",
            "    Negative Log Likelihood: 5.8572\tSigma2 Prior: -316.7428\tRegularization: 0.0006\n",
            "Iter: 1750  \tTraining Loss: -311.8729    \n",
            "    Negative Log Likelihood: 5.8090\tSigma2 Prior: -317.6825\tRegularization: 0.0006\n",
            "Iter: 1760  \tTraining Loss: -311.4021    \n",
            "    Negative Log Likelihood: 5.8473\tSigma2 Prior: -317.2500\tRegularization: 0.0006\n",
            "Iter: 1770  \tTraining Loss: -312.3032    \n",
            "    Negative Log Likelihood: 5.8312\tSigma2 Prior: -318.1351\tRegularization: 0.0006\n",
            "Iter: 1780  \tTraining Loss: -311.8015    \n",
            "    Negative Log Likelihood: 5.8512\tSigma2 Prior: -317.6533\tRegularization: 0.0006\n",
            "Iter: 1790  \tTraining Loss: -312.6175    \n",
            "    Negative Log Likelihood: 5.8105\tSigma2 Prior: -318.4287\tRegularization: 0.0006\n",
            "Iter: 1800  \tTraining Loss: -312.0314    \n",
            "    Negative Log Likelihood: 5.8595\tSigma2 Prior: -317.8915\tRegularization: 0.0006\n",
            "Iter: 1810  \tTraining Loss: -312.6306    \n",
            "    Negative Log Likelihood: 5.8382\tSigma2 Prior: -318.4695\tRegularization: 0.0006\n",
            "Iter: 1820  \tTraining Loss: -312.3948    \n",
            "    Negative Log Likelihood: 5.8452\tSigma2 Prior: -318.2407\tRegularization: 0.0006\n",
            "Iter: 1830  \tTraining Loss: -312.6731    \n",
            "    Negative Log Likelihood: 5.8216\tSigma2 Prior: -318.4953\tRegularization: 0.0006\n",
            "Iter: 1840  \tTraining Loss: -313.5935    \n",
            "    Negative Log Likelihood: 5.8387\tSigma2 Prior: -319.4328\tRegularization: 0.0006\n",
            "Iter: 1850  \tTraining Loss: -313.2354    \n",
            "    Negative Log Likelihood: 5.8662\tSigma2 Prior: -319.1023\tRegularization: 0.0006\n",
            "Iter: 1860  \tTraining Loss: -313.9260    \n",
            "    Negative Log Likelihood: 5.8525\tSigma2 Prior: -319.7792\tRegularization: 0.0006\n",
            "Iter: 1870  \tTraining Loss: -313.7004    \n",
            "    Negative Log Likelihood: 5.8727\tSigma2 Prior: -319.5738\tRegularization: 0.0006\n",
            "Iter: 1880  \tTraining Loss: -314.4218    \n",
            "    Negative Log Likelihood: 5.8696\tSigma2 Prior: -320.2920\tRegularization: 0.0006\n",
            "Iter: 1890  \tTraining Loss: -313.7343    \n",
            "    Negative Log Likelihood: 5.8849\tSigma2 Prior: -319.6198\tRegularization: 0.0006\n",
            "Iter: 1900  \tTraining Loss: -314.2790    \n",
            "    Negative Log Likelihood: 5.8617\tSigma2 Prior: -320.1413\tRegularization: 0.0006\n",
            "Iter: 1910  \tTraining Loss: -313.9527    \n",
            "    Negative Log Likelihood: 5.9180\tSigma2 Prior: -319.8713\tRegularization: 0.0006\n",
            "Iter: 1920  \tTraining Loss: -314.8014    \n",
            "    Negative Log Likelihood: 5.8964\tSigma2 Prior: -320.6984\tRegularization: 0.0006\n",
            "Iter: 1930  \tTraining Loss: -313.5492    \n",
            "    Negative Log Likelihood: 5.9499\tSigma2 Prior: -319.4998\tRegularization: 0.0006\n",
            "Iter: 1940  \tTraining Loss: -314.6996    \n",
            "    Negative Log Likelihood: 5.9207\tSigma2 Prior: -320.6209\tRegularization: 0.0006\n",
            "Iter: 1950  \tTraining Loss: -315.1555    \n",
            "    Negative Log Likelihood: 5.9064\tSigma2 Prior: -321.0625\tRegularization: 0.0006\n",
            "Iter: 1960  \tTraining Loss: -315.1921    \n",
            "    Negative Log Likelihood: 5.9046\tSigma2 Prior: -321.0974\tRegularization: 0.0006\n",
            "Iter: 1970  \tTraining Loss: -315.3631    \n",
            "    Negative Log Likelihood: 5.9226\tSigma2 Prior: -321.2863\tRegularization: 0.0006\n",
            "Iter: 1980  \tTraining Loss: -314.5918    \n",
            "    Negative Log Likelihood: 5.9791\tSigma2 Prior: -320.5716\tRegularization: 0.0006\n",
            "Iter: 1990  \tTraining Loss: -315.0094    \n",
            "    Negative Log Likelihood: 5.9954\tSigma2 Prior: -321.0054\tRegularization: 0.0006\n",
            "Iter: 2000  \tTraining Loss: -315.9956    \n",
            "    Negative Log Likelihood: 5.9457\tSigma2 Prior: -321.9419\tRegularization: 0.0006\n",
            "Iter: 2010  \tTraining Loss: -316.4588    \n",
            "    Negative Log Likelihood: 5.9168\tSigma2 Prior: -322.3763\tRegularization: 0.0006\n",
            "Iter: 2020  \tTraining Loss: -317.0182    \n",
            "    Negative Log Likelihood: 5.9413\tSigma2 Prior: -322.9602\tRegularization: 0.0006\n",
            "Iter: 2030  \tTraining Loss: -315.9115    \n",
            "    Negative Log Likelihood: 5.9709\tSigma2 Prior: -321.8831\tRegularization: 0.0006\n",
            "Iter: 2040  \tTraining Loss: -316.9463    \n",
            "    Negative Log Likelihood: 5.9346\tSigma2 Prior: -322.8815\tRegularization: 0.0006\n",
            "Iter: 2050  \tTraining Loss: -316.5309    \n",
            "    Negative Log Likelihood: 5.9881\tSigma2 Prior: -322.5196\tRegularization: 0.0006\n",
            "Iter: 2060  \tTraining Loss: -316.9990    \n",
            "    Negative Log Likelihood: 5.9851\tSigma2 Prior: -322.9848\tRegularization: 0.0006\n",
            "Iter: 2070  \tTraining Loss: -317.0255    \n",
            "    Negative Log Likelihood: 5.9878\tSigma2 Prior: -323.0139\tRegularization: 0.0006\n",
            "Iter: 2080  \tTraining Loss: -317.8105    \n",
            "    Negative Log Likelihood: 5.9280\tSigma2 Prior: -323.7391\tRegularization: 0.0006\n",
            "Iter: 2090  \tTraining Loss: -317.7086    \n",
            "    Negative Log Likelihood: 5.9890\tSigma2 Prior: -323.6983\tRegularization: 0.0006\n",
            "Iter: 2100  \tTraining Loss: -318.3076    \n",
            "    Negative Log Likelihood: 5.8873\tSigma2 Prior: -324.1955\tRegularization: 0.0006\n",
            "Iter: 2110  \tTraining Loss: -317.5461    \n",
            "    Negative Log Likelihood: 6.0123\tSigma2 Prior: -323.5591\tRegularization: 0.0006\n",
            "Iter: 2120  \tTraining Loss: -317.7920    \n",
            "    Negative Log Likelihood: 6.0016\tSigma2 Prior: -323.7942\tRegularization: 0.0006\n",
            "Iter: 2130  \tTraining Loss: -317.4686    \n",
            "    Negative Log Likelihood: 6.0597\tSigma2 Prior: -323.5290\tRegularization: 0.0006\n",
            "Iter: 2140  \tTraining Loss: -318.1299    \n",
            "    Negative Log Likelihood: 6.0395\tSigma2 Prior: -324.1701\tRegularization: 0.0006\n",
            "Iter: 2150  \tTraining Loss: -318.7263    \n",
            "    Negative Log Likelihood: 5.9721\tSigma2 Prior: -324.6991\tRegularization: 0.0006\n",
            "Iter: 2160  \tTraining Loss: -317.0147    \n",
            "    Negative Log Likelihood: 6.1045\tSigma2 Prior: -323.1198\tRegularization: 0.0006\n",
            "Iter: 2170  \tTraining Loss: -318.4634    \n",
            "    Negative Log Likelihood: 6.0242\tSigma2 Prior: -324.4882\tRegularization: 0.0006\n",
            "Iter: 2180  \tTraining Loss: -319.3430    \n",
            "    Negative Log Likelihood: 6.0306\tSigma2 Prior: -325.3743\tRegularization: 0.0006\n",
            "Iter: 2190  \tTraining Loss: -318.8255    \n",
            "    Negative Log Likelihood: 6.0383\tSigma2 Prior: -324.8644\tRegularization: 0.0006\n",
            "Iter: 2200  \tTraining Loss: -319.0757    \n",
            "    Negative Log Likelihood: 6.0196\tSigma2 Prior: -325.0959\tRegularization: 0.0006\n",
            "Iter: 2210  \tTraining Loss: -318.8756    \n",
            "    Negative Log Likelihood: 6.0737\tSigma2 Prior: -324.9500\tRegularization: 0.0006\n",
            "Iter: 2220  \tTraining Loss: -318.5896    \n",
            "    Negative Log Likelihood: 6.1459\tSigma2 Prior: -324.7362\tRegularization: 0.0007\n",
            "Iter: 2230  \tTraining Loss: -319.2896    \n",
            "    Negative Log Likelihood: 6.0928\tSigma2 Prior: -325.3830\tRegularization: 0.0007\n",
            "Iter: 2240  \tTraining Loss: -320.4558    \n",
            "    Negative Log Likelihood: 6.0589\tSigma2 Prior: -326.5153\tRegularization: 0.0007\n",
            "Iter: 2250  \tTraining Loss: -320.3103    \n",
            "    Negative Log Likelihood: 6.0563\tSigma2 Prior: -326.3673\tRegularization: 0.0007\n",
            "Iter: 2260  \tTraining Loss: -320.4098    \n",
            "    Negative Log Likelihood: 6.1186\tSigma2 Prior: -326.5291\tRegularization: 0.0007\n",
            "Iter: 2270  \tTraining Loss: -320.0175    \n",
            "    Negative Log Likelihood: 6.1466\tSigma2 Prior: -326.1647\tRegularization: 0.0007\n",
            "Iter: 2280  \tTraining Loss: -319.5274    \n",
            "    Negative Log Likelihood: 6.1554\tSigma2 Prior: -325.6835\tRegularization: 0.0007\n",
            "Iter: 2290  \tTraining Loss: -320.4546    \n",
            "    Negative Log Likelihood: 6.1288\tSigma2 Prior: -326.5840\tRegularization: 0.0007\n",
            "Iter: 2300  \tTraining Loss: -321.2468    \n",
            "    Negative Log Likelihood: 6.0968\tSigma2 Prior: -327.3442\tRegularization: 0.0007\n",
            "Iter: 2310  \tTraining Loss: -320.7643    \n",
            "    Negative Log Likelihood: 6.0935\tSigma2 Prior: -326.8584\tRegularization: 0.0007\n",
            "Iter: 2320  \tTraining Loss: -320.6177    \n",
            "    Negative Log Likelihood: 6.1299\tSigma2 Prior: -326.7482\tRegularization: 0.0007\n",
            "Iter: 2330  \tTraining Loss: -321.4274    \n",
            "    Negative Log Likelihood: 6.1546\tSigma2 Prior: -327.5826\tRegularization: 0.0007\n",
            "Iter: 2340  \tTraining Loss: -322.0264    \n",
            "    Negative Log Likelihood: 6.0888\tSigma2 Prior: -328.1158\tRegularization: 0.0007\n",
            "Iter: 2350  \tTraining Loss: -321.5999    \n",
            "    Negative Log Likelihood: 6.1430\tSigma2 Prior: -327.7435\tRegularization: 0.0007\n",
            "Iter: 2360  \tTraining Loss: -321.1974    \n",
            "    Negative Log Likelihood: 6.1674\tSigma2 Prior: -327.3655\tRegularization: 0.0007\n",
            "Iter: 2370  \tTraining Loss: -322.1894    \n",
            "    Negative Log Likelihood: 6.1636\tSigma2 Prior: -328.3536\tRegularization: 0.0007\n",
            "Iter: 2380  \tTraining Loss: -321.3375    \n",
            "    Negative Log Likelihood: 6.2062\tSigma2 Prior: -327.5443\tRegularization: 0.0007\n",
            "Iter: 2390  \tTraining Loss: -322.8505    \n",
            "    Negative Log Likelihood: 6.1684\tSigma2 Prior: -329.0195\tRegularization: 0.0007\n",
            "Iter: 2400  \tTraining Loss: -322.3248    \n",
            "    Negative Log Likelihood: 6.2280\tSigma2 Prior: -328.5535\tRegularization: 0.0007\n",
            "Iter: 2410  \tTraining Loss: -321.7116    \n",
            "    Negative Log Likelihood: 6.2672\tSigma2 Prior: -327.9794\tRegularization: 0.0007\n",
            "Iter: 2420  \tTraining Loss: -323.7327    \n",
            "    Negative Log Likelihood: 6.1022\tSigma2 Prior: -329.8356\tRegularization: 0.0007\n",
            "Iter: 2430  \tTraining Loss: -323.0248    \n",
            "    Negative Log Likelihood: 6.2139\tSigma2 Prior: -329.2393\tRegularization: 0.0007\n",
            "Iter: 2440  \tTraining Loss: -322.9586    \n",
            "    Negative Log Likelihood: 6.2469\tSigma2 Prior: -329.2062\tRegularization: 0.0007\n",
            "Iter: 2450  \tTraining Loss: -323.1331    \n",
            "    Negative Log Likelihood: 6.2648\tSigma2 Prior: -329.3985\tRegularization: 0.0007\n",
            "Iter: 2460  \tTraining Loss: -322.7249    \n",
            "    Negative Log Likelihood: 6.2968\tSigma2 Prior: -329.0223\tRegularization: 0.0007\n",
            "Iter: 2470  \tTraining Loss: -323.6766    \n",
            "    Negative Log Likelihood: 6.2255\tSigma2 Prior: -329.9027\tRegularization: 0.0007\n",
            "Iter: 2480  \tTraining Loss: -324.3099    \n",
            "    Negative Log Likelihood: 6.2397\tSigma2 Prior: -330.5502\tRegularization: 0.0007\n",
            "Iter: 2490  \tTraining Loss: -323.6292    \n",
            "    Negative Log Likelihood: 6.2432\tSigma2 Prior: -329.8730\tRegularization: 0.0007\n",
            "Iter: 2500  \tTraining Loss: -324.4610    \n",
            "    Negative Log Likelihood: 6.2785\tSigma2 Prior: -330.7402\tRegularization: 0.0007\n",
            "Iter: 2510  \tTraining Loss: -324.7932    \n",
            "    Negative Log Likelihood: 6.2476\tSigma2 Prior: -331.0414\tRegularization: 0.0007\n",
            "Iter: 2520  \tTraining Loss: -324.8878    \n",
            "    Negative Log Likelihood: 6.1823\tSigma2 Prior: -331.0708\tRegularization: 0.0007\n",
            "Iter: 2530  \tTraining Loss: -325.1086    \n",
            "    Negative Log Likelihood: 6.2668\tSigma2 Prior: -331.3760\tRegularization: 0.0007\n",
            "Iter: 2540  \tTraining Loss: -323.8971    \n",
            "    Negative Log Likelihood: 6.3238\tSigma2 Prior: -330.2216\tRegularization: 0.0007\n",
            "Iter: 2550  \tTraining Loss: -324.7054    \n",
            "    Negative Log Likelihood: 6.2466\tSigma2 Prior: -330.9526\tRegularization: 0.0007\n",
            "Iter: 2560  \tTraining Loss: -324.3185    \n",
            "    Negative Log Likelihood: 6.3596\tSigma2 Prior: -330.6787\tRegularization: 0.0007\n",
            "Iter: 2570  \tTraining Loss: -325.8553    \n",
            "    Negative Log Likelihood: 6.2740\tSigma2 Prior: -332.1300\tRegularization: 0.0007\n",
            "Iter: 2580  \tTraining Loss: -325.7272    \n",
            "    Negative Log Likelihood: 6.2891\tSigma2 Prior: -332.0170\tRegularization: 0.0007\n",
            "Iter: 2590  \tTraining Loss: -325.5237    \n",
            "    Negative Log Likelihood: 6.2618\tSigma2 Prior: -331.7861\tRegularization: 0.0007\n",
            "Iter: 2600  \tTraining Loss: -326.0048    \n",
            "    Negative Log Likelihood: 6.2418\tSigma2 Prior: -332.2473\tRegularization: 0.0007\n",
            "Iter: 2610  \tTraining Loss: -326.3723    \n",
            "    Negative Log Likelihood: 6.3438\tSigma2 Prior: -332.7167\tRegularization: 0.0007\n",
            "Iter: 2620  \tTraining Loss: -325.5663    \n",
            "    Negative Log Likelihood: 6.3763\tSigma2 Prior: -331.9432\tRegularization: 0.0007\n",
            "Iter: 2630  \tTraining Loss: -325.9274    \n",
            "    Negative Log Likelihood: 6.3908\tSigma2 Prior: -332.3188\tRegularization: 0.0007\n",
            "Iter: 2640  \tTraining Loss: -326.5497    \n",
            "    Negative Log Likelihood: 6.3276\tSigma2 Prior: -332.8780\tRegularization: 0.0007\n",
            "Iter: 2650  \tTraining Loss: -326.9880    \n",
            "    Negative Log Likelihood: 6.3124\tSigma2 Prior: -333.3011\tRegularization: 0.0007\n",
            "Iter: 2660  \tTraining Loss: -327.8262    \n",
            "    Negative Log Likelihood: 6.3024\tSigma2 Prior: -334.1293\tRegularization: 0.0007\n",
            "Iter: 2670  \tTraining Loss: -326.6594    \n",
            "    Negative Log Likelihood: 6.4063\tSigma2 Prior: -333.0663\tRegularization: 0.0007\n",
            "Iter: 2680  \tTraining Loss: -327.2284    \n",
            "    Negative Log Likelihood: 6.3897\tSigma2 Prior: -333.6188\tRegularization: 0.0007\n",
            "Iter: 2690  \tTraining Loss: -328.1067    \n",
            "    Negative Log Likelihood: 6.3330\tSigma2 Prior: -334.4404\tRegularization: 0.0007\n",
            "Iter: 2700  \tTraining Loss: -326.4853    \n",
            "    Negative Log Likelihood: 6.4347\tSigma2 Prior: -332.9207\tRegularization: 0.0007\n",
            "Iter: 2710  \tTraining Loss: -327.8527    \n",
            "    Negative Log Likelihood: 6.3783\tSigma2 Prior: -334.2317\tRegularization: 0.0007\n",
            "Iter: 2720  \tTraining Loss: -328.0500    \n",
            "    Negative Log Likelihood: 6.3425\tSigma2 Prior: -334.3931\tRegularization: 0.0007\n",
            "Iter: 2730  \tTraining Loss: -327.0533    \n",
            "    Negative Log Likelihood: 6.4882\tSigma2 Prior: -333.5421\tRegularization: 0.0007\n",
            "Iter: 2740  \tTraining Loss: -327.0316    \n",
            "    Negative Log Likelihood: 6.4754\tSigma2 Prior: -333.5076\tRegularization: 0.0007\n",
            "Iter: 2750  \tTraining Loss: -328.4786    \n",
            "    Negative Log Likelihood: 6.4499\tSigma2 Prior: -334.9292\tRegularization: 0.0007\n",
            "Iter: 2760  \tTraining Loss: -329.3783    \n",
            "    Negative Log Likelihood: 6.3194\tSigma2 Prior: -335.6984\tRegularization: 0.0007\n",
            "Iter: 2770  \tTraining Loss: -328.8706    \n",
            "    Negative Log Likelihood: 6.4525\tSigma2 Prior: -335.3238\tRegularization: 0.0007\n",
            "Iter: 2780  \tTraining Loss: -328.2146    \n",
            "    Negative Log Likelihood: 6.4943\tSigma2 Prior: -334.7096\tRegularization: 0.0007\n",
            "Iter: 2790  \tTraining Loss: -329.3661    \n",
            "    Negative Log Likelihood: 6.4838\tSigma2 Prior: -335.8506\tRegularization: 0.0007\n",
            "Iter: 2800  \tTraining Loss: -330.0132    \n",
            "    Negative Log Likelihood: 6.4654\tSigma2 Prior: -336.4793\tRegularization: 0.0007\n",
            "Iter: 2810  \tTraining Loss: -329.2861    \n",
            "    Negative Log Likelihood: 6.5218\tSigma2 Prior: -335.8086\tRegularization: 0.0007\n",
            "Iter: 2820  \tTraining Loss: -330.5605    \n",
            "    Negative Log Likelihood: 6.4717\tSigma2 Prior: -337.0329\tRegularization: 0.0007\n",
            "Iter: 2830  \tTraining Loss: -330.4987    \n",
            "    Negative Log Likelihood: 6.4332\tSigma2 Prior: -336.9326\tRegularization: 0.0007\n",
            "Iter: 2840  \tTraining Loss: -331.2633    \n",
            "    Negative Log Likelihood: 6.4433\tSigma2 Prior: -337.7073\tRegularization: 0.0007\n",
            "Iter: 2850  \tTraining Loss: -330.6735    \n",
            "    Negative Log Likelihood: 6.4861\tSigma2 Prior: -337.1602\tRegularization: 0.0007\n",
            "Iter: 2860  \tTraining Loss: -330.8330    \n",
            "    Negative Log Likelihood: 6.4693\tSigma2 Prior: -337.3030\tRegularization: 0.0007\n",
            "Iter: 2870  \tTraining Loss: -330.3410    \n",
            "    Negative Log Likelihood: 6.5315\tSigma2 Prior: -336.8731\tRegularization: 0.0007\n",
            "Iter: 2880  \tTraining Loss: -331.3813    \n",
            "    Negative Log Likelihood: 6.4595\tSigma2 Prior: -337.8415\tRegularization: 0.0007\n",
            "Iter: 2890  \tTraining Loss: -331.4738    \n",
            "    Negative Log Likelihood: 6.5384\tSigma2 Prior: -338.0129\tRegularization: 0.0007\n",
            "Iter: 2900  \tTraining Loss: -331.8926    \n",
            "    Negative Log Likelihood: 6.4681\tSigma2 Prior: -338.3614\tRegularization: 0.0007\n",
            "Iter: 2910  \tTraining Loss: -332.4282    \n",
            "    Negative Log Likelihood: 6.4935\tSigma2 Prior: -338.9223\tRegularization: 0.0007\n",
            "Iter: 2920  \tTraining Loss: -331.4165    \n",
            "    Negative Log Likelihood: 6.5537\tSigma2 Prior: -337.9709\tRegularization: 0.0007\n",
            "Iter: 2930  \tTraining Loss: -331.5611    \n",
            "    Negative Log Likelihood: 6.6122\tSigma2 Prior: -338.1740\tRegularization: 0.0007\n",
            "Iter: 2940  \tTraining Loss: -332.0264    \n",
            "    Negative Log Likelihood: 6.5048\tSigma2 Prior: -338.5319\tRegularization: 0.0007\n",
            "Iter: 2950  \tTraining Loss: -332.9219    \n",
            "    Negative Log Likelihood: 6.5132\tSigma2 Prior: -339.4358\tRegularization: 0.0007\n",
            "Iter: 2960  \tTraining Loss: -331.2045    \n",
            "    Negative Log Likelihood: 6.6328\tSigma2 Prior: -337.8380\tRegularization: 0.0007\n",
            "Iter: 2970  \tTraining Loss: -333.3971    \n",
            "    Negative Log Likelihood: 6.5341\tSigma2 Prior: -339.9318\tRegularization: 0.0007\n",
            "Iter: 2980  \tTraining Loss: -333.7898    \n",
            "    Negative Log Likelihood: 6.5017\tSigma2 Prior: -340.2921\tRegularization: 0.0007\n",
            "Iter: 2990  \tTraining Loss: -333.9814    \n",
            "    Negative Log Likelihood: 6.4766\tSigma2 Prior: -340.4587\tRegularization: 0.0007\n",
            "Iter: 3000  \tTraining Loss: -332.7603    \n",
            "    Negative Log Likelihood: 6.6851\tSigma2 Prior: -339.4461\tRegularization: 0.0007\n",
            "Iter: 3010  \tTraining Loss: -333.2523    \n",
            "    Negative Log Likelihood: 6.6458\tSigma2 Prior: -339.8987\tRegularization: 0.0007\n",
            "Iter: 3020  \tTraining Loss: -334.6293    \n",
            "    Negative Log Likelihood: 6.5102\tSigma2 Prior: -341.1401\tRegularization: 0.0007\n",
            "Iter: 3030  \tTraining Loss: -334.2454    \n",
            "    Negative Log Likelihood: 6.5937\tSigma2 Prior: -340.8397\tRegularization: 0.0007\n",
            "Iter: 3040  \tTraining Loss: -335.1102    \n",
            "    Negative Log Likelihood: 6.5193\tSigma2 Prior: -341.6302\tRegularization: 0.0007\n",
            "Iter: 3050  \tTraining Loss: -333.3161    \n",
            "    Negative Log Likelihood: 6.6638\tSigma2 Prior: -339.9806\tRegularization: 0.0007\n",
            "Iter: 3060  \tTraining Loss: -334.8570    \n",
            "    Negative Log Likelihood: 6.6179\tSigma2 Prior: -341.4756\tRegularization: 0.0007\n",
            "Iter: 3070  \tTraining Loss: -335.1569    \n",
            "    Negative Log Likelihood: 6.5400\tSigma2 Prior: -341.6976\tRegularization: 0.0007\n",
            "Iter: 3080  \tTraining Loss: -336.0072    \n",
            "    Negative Log Likelihood: 6.6252\tSigma2 Prior: -342.6331\tRegularization: 0.0007\n",
            "Iter: 3090  \tTraining Loss: -335.7514    \n",
            "    Negative Log Likelihood: 6.6483\tSigma2 Prior: -342.4004\tRegularization: 0.0007\n",
            "Iter: 3100  \tTraining Loss: -335.4642    \n",
            "    Negative Log Likelihood: 6.6733\tSigma2 Prior: -342.1382\tRegularization: 0.0007\n",
            "Iter: 3110  \tTraining Loss: -336.0048    \n",
            "    Negative Log Likelihood: 6.6313\tSigma2 Prior: -342.6368\tRegularization: 0.0007\n",
            "Iter: 3120  \tTraining Loss: -336.7312    \n",
            "    Negative Log Likelihood: 6.6005\tSigma2 Prior: -343.3323\tRegularization: 0.0007\n",
            "Iter: 3130  \tTraining Loss: -334.9270    \n",
            "    Negative Log Likelihood: 6.7614\tSigma2 Prior: -341.6891\tRegularization: 0.0007\n",
            "Iter: 3140  \tTraining Loss: -336.7342    \n",
            "    Negative Log Likelihood: 6.5949\tSigma2 Prior: -343.3298\tRegularization: 0.0007\n",
            "Iter: 3150  \tTraining Loss: -336.5829    \n",
            "    Negative Log Likelihood: 6.6432\tSigma2 Prior: -343.2268\tRegularization: 0.0007\n",
            "Iter: 3160  \tTraining Loss: -335.7888    \n",
            "    Negative Log Likelihood: 6.7924\tSigma2 Prior: -342.5819\tRegularization: 0.0007\n",
            "Iter: 3170  \tTraining Loss: -336.8981    \n",
            "    Negative Log Likelihood: 6.6704\tSigma2 Prior: -343.5692\tRegularization: 0.0007\n",
            "Iter: 3180  \tTraining Loss: -336.7731    \n",
            "    Negative Log Likelihood: 6.7672\tSigma2 Prior: -343.5410\tRegularization: 0.0007\n",
            "Iter: 3190  \tTraining Loss: -338.0676    \n",
            "    Negative Log Likelihood: 6.7155\tSigma2 Prior: -344.7837\tRegularization: 0.0007\n",
            "Iter: 3200  \tTraining Loss: -338.2645    \n",
            "    Negative Log Likelihood: 6.6330\tSigma2 Prior: -344.8981\tRegularization: 0.0007\n",
            "Iter: 3210  \tTraining Loss: -338.1451    \n",
            "    Negative Log Likelihood: 6.5978\tSigma2 Prior: -344.7435\tRegularization: 0.0007\n",
            "Iter: 3220  \tTraining Loss: -338.0471    \n",
            "    Negative Log Likelihood: 6.7459\tSigma2 Prior: -344.7937\tRegularization: 0.0007\n",
            "Iter: 3230  \tTraining Loss: -338.0608    \n",
            "    Negative Log Likelihood: 6.7553\tSigma2 Prior: -344.8168\tRegularization: 0.0007\n",
            "Iter: 3240  \tTraining Loss: -335.8523    \n",
            "    Negative Log Likelihood: 6.9631\tSigma2 Prior: -342.8161\tRegularization: 0.0007\n",
            "Iter: 3250  \tTraining Loss: -338.7189    \n",
            "    Negative Log Likelihood: 6.7674\tSigma2 Prior: -345.4870\tRegularization: 0.0007\n",
            "Iter: 3260  \tTraining Loss: -338.4385    \n",
            "    Negative Log Likelihood: 6.7917\tSigma2 Prior: -345.2309\tRegularization: 0.0007\n",
            "Iter: 3270  \tTraining Loss: -339.2638    \n",
            "    Negative Log Likelihood: 6.7015\tSigma2 Prior: -345.9660\tRegularization: 0.0007\n",
            "Iter: 3280  \tTraining Loss: -338.3814    \n",
            "    Negative Log Likelihood: 6.9099\tSigma2 Prior: -345.2920\tRegularization: 0.0007\n",
            "Iter: 3290  \tTraining Loss: -339.1580    \n",
            "    Negative Log Likelihood: 6.8249\tSigma2 Prior: -345.9836\tRegularization: 0.0007\n",
            "Iter: 3300  \tTraining Loss: -339.1576    \n",
            "    Negative Log Likelihood: 6.8548\tSigma2 Prior: -346.0131\tRegularization: 0.0007\n",
            "Iter: 3310  \tTraining Loss: -338.4148    \n",
            "    Negative Log Likelihood: 6.8748\tSigma2 Prior: -345.2903\tRegularization: 0.0007\n",
            "Iter: 3320  \tTraining Loss: -340.0684    \n",
            "    Negative Log Likelihood: 6.7791\tSigma2 Prior: -346.8481\tRegularization: 0.0007\n",
            "Iter: 3330  \tTraining Loss: -340.8857    \n",
            "    Negative Log Likelihood: 6.7943\tSigma2 Prior: -347.6806\tRegularization: 0.0007\n",
            "Iter: 3340  \tTraining Loss: -340.1433    \n",
            "    Negative Log Likelihood: 6.8424\tSigma2 Prior: -346.9864\tRegularization: 0.0007\n",
            "Iter: 3350  \tTraining Loss: -339.2757    \n",
            "    Negative Log Likelihood: 6.9506\tSigma2 Prior: -346.2269\tRegularization: 0.0007\n",
            "Iter: 3360  \tTraining Loss: -340.7703    \n",
            "    Negative Log Likelihood: 6.8371\tSigma2 Prior: -347.6081\tRegularization: 0.0007\n",
            "Iter: 3370  \tTraining Loss: -340.5666    \n",
            "    Negative Log Likelihood: 6.9308\tSigma2 Prior: -347.4980\tRegularization: 0.0007\n",
            "Iter: 3380  \tTraining Loss: -340.3665    \n",
            "    Negative Log Likelihood: 6.9904\tSigma2 Prior: -347.3577\tRegularization: 0.0007\n",
            "Iter: 3390  \tTraining Loss: -341.2605    \n",
            "    Negative Log Likelihood: 6.9213\tSigma2 Prior: -348.1825\tRegularization: 0.0007\n",
            "Iter: 3400  \tTraining Loss: -341.7261    \n",
            "    Negative Log Likelihood: 6.8345\tSigma2 Prior: -348.5613\tRegularization: 0.0007\n",
            "Iter: 3410  \tTraining Loss: -341.8466    \n",
            "    Negative Log Likelihood: 6.8753\tSigma2 Prior: -348.7226\tRegularization: 0.0007\n",
            "Iter: 3420  \tTraining Loss: -342.2561    \n",
            "    Negative Log Likelihood: 6.8688\tSigma2 Prior: -349.1256\tRegularization: 0.0007\n",
            "Iter: 3430  \tTraining Loss: -341.8278    \n",
            "    Negative Log Likelihood: 6.9113\tSigma2 Prior: -348.7397\tRegularization: 0.0007\n",
            "Iter: 3440  \tTraining Loss: -342.3085    \n",
            "    Negative Log Likelihood: 6.9002\tSigma2 Prior: -349.2093\tRegularization: 0.0007\n",
            "Iter: 3450  \tTraining Loss: -343.2077    \n",
            "    Negative Log Likelihood: 6.8696\tSigma2 Prior: -350.0780\tRegularization: 0.0007\n",
            "Iter: 3460  \tTraining Loss: -343.5239    \n",
            "    Negative Log Likelihood: 6.8687\tSigma2 Prior: -350.3932\tRegularization: 0.0007\n",
            "Iter: 3470  \tTraining Loss: -342.4553    \n",
            "    Negative Log Likelihood: 6.9465\tSigma2 Prior: -349.4024\tRegularization: 0.0007\n",
            "Iter: 3480  \tTraining Loss: -343.1790    \n",
            "    Negative Log Likelihood: 6.9614\tSigma2 Prior: -350.1411\tRegularization: 0.0007\n",
            "Iter: 3490  \tTraining Loss: -343.4908    \n",
            "    Negative Log Likelihood: 6.9277\tSigma2 Prior: -350.4192\tRegularization: 0.0007\n",
            "Iter: 3500  \tTraining Loss: -343.5817    \n",
            "    Negative Log Likelihood: 6.9787\tSigma2 Prior: -350.5612\tRegularization: 0.0007\n",
            "Iter: 3510  \tTraining Loss: -342.8459    \n",
            "    Negative Log Likelihood: 7.0334\tSigma2 Prior: -349.8800\tRegularization: 0.0007\n",
            "Iter: 3520  \tTraining Loss: -345.3611    \n",
            "    Negative Log Likelihood: 6.8808\tSigma2 Prior: -352.2426\tRegularization: 0.0007\n",
            "Iter: 3530  \tTraining Loss: -343.9993    \n",
            "    Negative Log Likelihood: 7.0155\tSigma2 Prior: -351.0155\tRegularization: 0.0007\n",
            "Iter: 3540  \tTraining Loss: -344.0522    \n",
            "    Negative Log Likelihood: 7.1144\tSigma2 Prior: -351.1673\tRegularization: 0.0007\n",
            "Iter: 3550  \tTraining Loss: -344.7134    \n",
            "    Negative Log Likelihood: 7.1148\tSigma2 Prior: -351.8289\tRegularization: 0.0007\n",
            "Iter: 3560  \tTraining Loss: -342.5695    \n",
            "    Negative Log Likelihood: 7.2326\tSigma2 Prior: -349.8028\tRegularization: 0.0007\n",
            "Iter: 3570  \tTraining Loss: -345.0102    \n",
            "    Negative Log Likelihood: 7.0470\tSigma2 Prior: -352.0579\tRegularization: 0.0007\n",
            "Iter: 3580  \tTraining Loss: -344.0210    \n",
            "    Negative Log Likelihood: 7.2305\tSigma2 Prior: -351.2522\tRegularization: 0.0007\n",
            "Iter: 3590  \tTraining Loss: -346.3153    \n",
            "    Negative Log Likelihood: 6.9634\tSigma2 Prior: -353.2794\tRegularization: 0.0007\n",
            "Iter: 3600  \tTraining Loss: -345.6449    \n",
            "    Negative Log Likelihood: 7.0696\tSigma2 Prior: -352.7152\tRegularization: 0.0007\n",
            "Iter: 3610  \tTraining Loss: -345.0379    \n",
            "    Negative Log Likelihood: 7.1333\tSigma2 Prior: -352.1719\tRegularization: 0.0007\n",
            "Iter: 3620  \tTraining Loss: -347.1280    \n",
            "    Negative Log Likelihood: 7.0627\tSigma2 Prior: -354.1914\tRegularization: 0.0007\n",
            "Iter: 3630  \tTraining Loss: -346.5643    \n",
            "    Negative Log Likelihood: 7.1187\tSigma2 Prior: -353.6837\tRegularization: 0.0007\n",
            "Iter: 3640  \tTraining Loss: -347.6637    \n",
            "    Negative Log Likelihood: 6.9677\tSigma2 Prior: -354.6320\tRegularization: 0.0007\n",
            "Iter: 3650  \tTraining Loss: -346.6939    \n",
            "    Negative Log Likelihood: 7.2286\tSigma2 Prior: -353.9233\tRegularization: 0.0007\n",
            "Iter: 3660  \tTraining Loss: -348.1099    \n",
            "    Negative Log Likelihood: 6.9900\tSigma2 Prior: -355.1006\tRegularization: 0.0007\n",
            "Iter: 3670  \tTraining Loss: -346.1660    \n",
            "    Negative Log Likelihood: 7.2551\tSigma2 Prior: -353.4218\tRegularization: 0.0007\n",
            "Iter: 3680  \tTraining Loss: -346.5124    \n",
            "    Negative Log Likelihood: 7.2719\tSigma2 Prior: -353.7851\tRegularization: 0.0007\n",
            "Iter: 3690  \tTraining Loss: -348.1473    \n",
            "    Negative Log Likelihood: 7.1285\tSigma2 Prior: -355.2765\tRegularization: 0.0007\n",
            "Iter: 3700  \tTraining Loss: -346.8936    \n",
            "    Negative Log Likelihood: 7.2502\tSigma2 Prior: -354.1445\tRegularization: 0.0007\n",
            "Iter: 3710  \tTraining Loss: -347.1748    \n",
            "    Negative Log Likelihood: 7.3412\tSigma2 Prior: -354.5167\tRegularization: 0.0007\n",
            "Iter: 3720  \tTraining Loss: -348.2281    \n",
            "    Negative Log Likelihood: 7.1648\tSigma2 Prior: -355.3936\tRegularization: 0.0007\n",
            "Iter: 3730  \tTraining Loss: -348.4112    \n",
            "    Negative Log Likelihood: 7.2841\tSigma2 Prior: -355.6960\tRegularization: 0.0007\n",
            "Iter: 3740  \tTraining Loss: -349.2554    \n",
            "    Negative Log Likelihood: 7.1514\tSigma2 Prior: -356.4075\tRegularization: 0.0007\n",
            "Iter: 3750  \tTraining Loss: -348.4477    \n",
            "    Negative Log Likelihood: 7.2835\tSigma2 Prior: -355.7319\tRegularization: 0.0007\n",
            "Iter: 3760  \tTraining Loss: -349.6900    \n",
            "    Negative Log Likelihood: 7.2213\tSigma2 Prior: -356.9120\tRegularization: 0.0007\n",
            "Iter: 3770  \tTraining Loss: -349.6542    \n",
            "    Negative Log Likelihood: 7.2751\tSigma2 Prior: -356.9300\tRegularization: 0.0007\n",
            "Iter: 3780  \tTraining Loss: -349.3665    \n",
            "    Negative Log Likelihood: 7.2542\tSigma2 Prior: -356.6214\tRegularization: 0.0007\n",
            "Iter: 3790  \tTraining Loss: -350.3404    \n",
            "    Negative Log Likelihood: 7.2199\tSigma2 Prior: -357.5610\tRegularization: 0.0007\n",
            "Iter: 3800  \tTraining Loss: -350.1582    \n",
            "    Negative Log Likelihood: 7.2742\tSigma2 Prior: -357.4332\tRegularization: 0.0007\n",
            "Iter: 3810  \tTraining Loss: -350.7298    \n",
            "    Negative Log Likelihood: 7.2515\tSigma2 Prior: -357.9820\tRegularization: 0.0007\n",
            "Iter: 3820  \tTraining Loss: -351.0975    \n",
            "    Negative Log Likelihood: 7.1620\tSigma2 Prior: -358.2602\tRegularization: 0.0007\n",
            "Iter: 3830  \tTraining Loss: -350.7504    \n",
            "    Negative Log Likelihood: 7.2347\tSigma2 Prior: -357.9857\tRegularization: 0.0007\n",
            "Iter: 3840  \tTraining Loss: -350.6661    \n",
            "    Negative Log Likelihood: 7.3325\tSigma2 Prior: -357.9993\tRegularization: 0.0007\n",
            "Iter: 3850  \tTraining Loss: -351.8728    \n",
            "    Negative Log Likelihood: 7.1998\tSigma2 Prior: -359.0733\tRegularization: 0.0007\n",
            "Iter: 3860  \tTraining Loss: -353.0984    \n",
            "    Negative Log Likelihood: 7.0683\tSigma2 Prior: -360.1674\tRegularization: 0.0007\n",
            "Iter: 3870  \tTraining Loss: -351.8274    \n",
            "    Negative Log Likelihood: 7.3655\tSigma2 Prior: -359.1936\tRegularization: 0.0007\n",
            "Iter: 3880  \tTraining Loss: -350.2878    \n",
            "    Negative Log Likelihood: 7.5043\tSigma2 Prior: -357.7928\tRegularization: 0.0007\n",
            "Iter: 3890  \tTraining Loss: -352.0325    \n",
            "    Negative Log Likelihood: 7.3563\tSigma2 Prior: -359.3895\tRegularization: 0.0007\n",
            "Iter: 3900  \tTraining Loss: -352.4648    \n",
            "    Negative Log Likelihood: 7.2501\tSigma2 Prior: -359.7157\tRegularization: 0.0007\n",
            "Iter: 3910  \tTraining Loss: -351.2399    \n",
            "    Negative Log Likelihood: 7.4817\tSigma2 Prior: -358.7223\tRegularization: 0.0007\n",
            "Iter: 3920  \tTraining Loss: -354.2881    \n",
            "    Negative Log Likelihood: 7.2511\tSigma2 Prior: -361.5399\tRegularization: 0.0007\n",
            "Iter: 3930  \tTraining Loss: -352.3458    \n",
            "    Negative Log Likelihood: 7.4655\tSigma2 Prior: -359.8120\tRegularization: 0.0007\n",
            "Iter: 3940  \tTraining Loss: -354.4855    \n",
            "    Negative Log Likelihood: 7.2839\tSigma2 Prior: -361.7702\tRegularization: 0.0007\n",
            "Iter: 3950  \tTraining Loss: -353.6982    \n",
            "    Negative Log Likelihood: 7.4775\tSigma2 Prior: -361.1764\tRegularization: 0.0007\n",
            "Iter: 3960  \tTraining Loss: -353.8279    \n",
            "    Negative Log Likelihood: 7.4724\tSigma2 Prior: -361.3010\tRegularization: 0.0007\n",
            "Iter: 3970  \tTraining Loss: -353.2865    \n",
            "    Negative Log Likelihood: 7.5179\tSigma2 Prior: -360.8050\tRegularization: 0.0007\n",
            "Iter: 3980  \tTraining Loss: -355.7174    \n",
            "    Negative Log Likelihood: 7.2033\tSigma2 Prior: -362.9214\tRegularization: 0.0007\n",
            "Iter: 3990  \tTraining Loss: -355.5627    \n",
            "    Negative Log Likelihood: 7.3249\tSigma2 Prior: -362.8884\tRegularization: 0.0007\n",
            "Iter: 4000  \tTraining Loss: -353.6669    \n",
            "    Negative Log Likelihood: 7.6016\tSigma2 Prior: -361.2692\tRegularization: 0.0007\n",
            "Iter: 4010  \tTraining Loss: -354.4258    \n",
            "    Negative Log Likelihood: 7.6180\tSigma2 Prior: -362.0446\tRegularization: 0.0007\n",
            "Iter: 4020  \tTraining Loss: -356.4863    \n",
            "    Negative Log Likelihood: 7.3929\tSigma2 Prior: -363.8799\tRegularization: 0.0007\n",
            "Iter: 4030  \tTraining Loss: -355.1079    \n",
            "    Negative Log Likelihood: 7.6359\tSigma2 Prior: -362.7444\tRegularization: 0.0007\n",
            "Iter: 4040  \tTraining Loss: -356.2507    \n",
            "    Negative Log Likelihood: 7.4753\tSigma2 Prior: -363.7267\tRegularization: 0.0007\n",
            "Iter: 4050  \tTraining Loss: -355.7450    \n",
            "    Negative Log Likelihood: 7.6242\tSigma2 Prior: -363.3699\tRegularization: 0.0007\n",
            "Iter: 4060  \tTraining Loss: -353.7605    \n",
            "    Negative Log Likelihood: 7.8252\tSigma2 Prior: -361.5864\tRegularization: 0.0007\n",
            "Iter: 4070  \tTraining Loss: -356.9930    \n",
            "    Negative Log Likelihood: 7.4341\tSigma2 Prior: -364.4277\tRegularization: 0.0007\n",
            "Iter: 4080  \tTraining Loss: -356.1770    \n",
            "    Negative Log Likelihood: 7.5408\tSigma2 Prior: -363.7186\tRegularization: 0.0007\n",
            "Iter: 4090  \tTraining Loss: -357.1663    \n",
            "    Negative Log Likelihood: 7.5450\tSigma2 Prior: -364.7120\tRegularization: 0.0007\n",
            "Iter: 4100  \tTraining Loss: -357.5276    \n",
            "    Negative Log Likelihood: 7.5321\tSigma2 Prior: -365.0604\tRegularization: 0.0007\n",
            "Iter: 4110  \tTraining Loss: -356.1029    \n",
            "    Negative Log Likelihood: 7.8001\tSigma2 Prior: -363.9037\tRegularization: 0.0007\n",
            "Iter: 4120  \tTraining Loss: -358.0125    \n",
            "    Negative Log Likelihood: 7.5824\tSigma2 Prior: -365.5956\tRegularization: 0.0007\n",
            "Iter: 4130  \tTraining Loss: -355.3362    \n",
            "    Negative Log Likelihood: 7.8163\tSigma2 Prior: -363.1532\tRegularization: 0.0007\n",
            "Iter: 4140  \tTraining Loss: -357.5884    \n",
            "    Negative Log Likelihood: 7.7156\tSigma2 Prior: -365.3047\tRegularization: 0.0007\n",
            "Iter: 4150  \tTraining Loss: -357.7014    \n",
            "    Negative Log Likelihood: 7.7042\tSigma2 Prior: -365.4062\tRegularization: 0.0007\n",
            "Iter: 4160  \tTraining Loss: -356.1913    \n",
            "    Negative Log Likelihood: 7.9461\tSigma2 Prior: -364.1381\tRegularization: 0.0007\n",
            "Iter: 4170  \tTraining Loss: -358.2063    \n",
            "    Negative Log Likelihood: 7.6919\tSigma2 Prior: -365.8988\tRegularization: 0.0007\n",
            "Iter: 4180  \tTraining Loss: -360.0464    \n",
            "    Negative Log Likelihood: 7.6411\tSigma2 Prior: -367.6883\tRegularization: 0.0007\n",
            "Iter: 4190  \tTraining Loss: -358.0830    \n",
            "    Negative Log Likelihood: 7.8600\tSigma2 Prior: -365.9438\tRegularization: 0.0007\n",
            "Iter: 4200  \tTraining Loss: -360.8627    \n",
            "    Negative Log Likelihood: 7.5827\tSigma2 Prior: -368.4460\tRegularization: 0.0007\n",
            "Iter: 4210  \tTraining Loss: -359.2207    \n",
            "    Negative Log Likelihood: 7.8391\tSigma2 Prior: -367.0605\tRegularization: 0.0007\n",
            "Iter: 4220  \tTraining Loss: -360.6333    \n",
            "    Negative Log Likelihood: 7.6456\tSigma2 Prior: -368.2796\tRegularization: 0.0007\n",
            "Iter: 4230  \tTraining Loss: -360.0040    \n",
            "    Negative Log Likelihood: 7.7406\tSigma2 Prior: -367.7453\tRegularization: 0.0007\n",
            "Iter: 4240  \tTraining Loss: -360.6639    \n",
            "    Negative Log Likelihood: 7.8648\tSigma2 Prior: -368.5294\tRegularization: 0.0007\n",
            "Iter: 4250  \tTraining Loss: -360.9240    \n",
            "    Negative Log Likelihood: 7.7489\tSigma2 Prior: -368.6736\tRegularization: 0.0007\n",
            "Iter: 4260  \tTraining Loss: -360.7773    \n",
            "    Negative Log Likelihood: 7.8124\tSigma2 Prior: -368.5904\tRegularization: 0.0007\n",
            "Iter: 4270  \tTraining Loss: -359.9544    \n",
            "    Negative Log Likelihood: 7.9753\tSigma2 Prior: -367.9304\tRegularization: 0.0007\n",
            "Iter: 4280  \tTraining Loss: -361.3663    \n",
            "    Negative Log Likelihood: 7.8561\tSigma2 Prior: -369.2231\tRegularization: 0.0007\n",
            "Iter: 4290  \tTraining Loss: -362.6931    \n",
            "    Negative Log Likelihood: 7.8160\tSigma2 Prior: -370.5098\tRegularization: 0.0007\n",
            "Iter: 4300  \tTraining Loss: -361.0704    \n",
            "    Negative Log Likelihood: 7.9606\tSigma2 Prior: -369.0318\tRegularization: 0.0007\n",
            "Iter: 4310  \tTraining Loss: -361.9168    \n",
            "    Negative Log Likelihood: 7.9299\tSigma2 Prior: -369.8475\tRegularization: 0.0007\n",
            "Iter: 4320  \tTraining Loss: -362.2395    \n",
            "    Negative Log Likelihood: 7.8604\tSigma2 Prior: -370.1007\tRegularization: 0.0007\n",
            "Iter: 4330  \tTraining Loss: -362.5361    \n",
            "    Negative Log Likelihood: 7.8787\tSigma2 Prior: -370.4156\tRegularization: 0.0007\n",
            "Iter: 4340  \tTraining Loss: -363.3870    \n",
            "    Negative Log Likelihood: 7.8407\tSigma2 Prior: -371.2284\tRegularization: 0.0007\n",
            "Iter: 4350  \tTraining Loss: -362.5212    \n",
            "    Negative Log Likelihood: 7.9587\tSigma2 Prior: -370.4806\tRegularization: 0.0007\n",
            "Iter: 4360  \tTraining Loss: -363.9980    \n",
            "    Negative Log Likelihood: 7.9123\tSigma2 Prior: -371.9109\tRegularization: 0.0007\n",
            "Iter: 4370  \tTraining Loss: -364.3759    \n",
            "    Negative Log Likelihood: 7.8351\tSigma2 Prior: -372.2118\tRegularization: 0.0007\n",
            "Iter: 4380  \tTraining Loss: -365.4026    \n",
            "    Negative Log Likelihood: 7.6519\tSigma2 Prior: -373.0552\tRegularization: 0.0007\n",
            "Iter: 4390  \tTraining Loss: -364.2906    \n",
            "    Negative Log Likelihood: 7.9799\tSigma2 Prior: -372.2712\tRegularization: 0.0007\n",
            "Iter: 4400  \tTraining Loss: -364.8334    \n",
            "    Negative Log Likelihood: 8.0105\tSigma2 Prior: -372.8446\tRegularization: 0.0007\n",
            "Iter: 4410  \tTraining Loss: -363.8895    \n",
            "    Negative Log Likelihood: 8.0130\tSigma2 Prior: -371.9032\tRegularization: 0.0007\n",
            "Iter: 4420  \tTraining Loss: -365.7932    \n",
            "    Negative Log Likelihood: 7.9503\tSigma2 Prior: -373.7443\tRegularization: 0.0007\n",
            "Iter: 4430  \tTraining Loss: -364.2277    \n",
            "    Negative Log Likelihood: 8.1058\tSigma2 Prior: -372.3343\tRegularization: 0.0007\n",
            "Iter: 4440  \tTraining Loss: -363.3644    \n",
            "    Negative Log Likelihood: 8.1996\tSigma2 Prior: -371.5647\tRegularization: 0.0007\n",
            "Iter: 4450  \tTraining Loss: -366.3609    \n",
            "    Negative Log Likelihood: 7.9742\tSigma2 Prior: -374.3358\tRegularization: 0.0007\n",
            "Iter: 4460  \tTraining Loss: -365.5696    \n",
            "    Negative Log Likelihood: 8.1325\tSigma2 Prior: -373.7028\tRegularization: 0.0007\n",
            "Iter: 4470  \tTraining Loss: -364.3977    \n",
            "    Negative Log Likelihood: 8.3497\tSigma2 Prior: -372.7482\tRegularization: 0.0007\n",
            "Iter: 4480  \tTraining Loss: -366.1914    \n",
            "    Negative Log Likelihood: 8.0831\tSigma2 Prior: -374.2751\tRegularization: 0.0007\n",
            "Iter: 4490  \tTraining Loss: -367.7454    \n",
            "    Negative Log Likelihood: 7.9908\tSigma2 Prior: -375.7369\tRegularization: 0.0007\n",
            "Iter: 4500  \tTraining Loss: -368.1145    \n",
            "    Negative Log Likelihood: 7.9671\tSigma2 Prior: -376.0823\tRegularization: 0.0007\n",
            "Iter: 4510  \tTraining Loss: -366.7202    \n",
            "    Negative Log Likelihood: 8.1770\tSigma2 Prior: -374.8979\tRegularization: 0.0007\n",
            "Iter: 4520  \tTraining Loss: -365.9848    \n",
            "    Negative Log Likelihood: 8.2903\tSigma2 Prior: -374.2758\tRegularization: 0.0007\n",
            "Iter: 4530  \tTraining Loss: -368.4742    \n",
            "    Negative Log Likelihood: 8.0033\tSigma2 Prior: -376.4782\tRegularization: 0.0007\n",
            "Iter: 4540  \tTraining Loss: -367.3848    \n",
            "    Negative Log Likelihood: 8.2674\tSigma2 Prior: -375.6530\tRegularization: 0.0007\n",
            "Iter: 4550  \tTraining Loss: -368.1230    \n",
            "    Negative Log Likelihood: 8.1801\tSigma2 Prior: -376.3039\tRegularization: 0.0007\n",
            "Iter: 4560  \tTraining Loss: -367.9919    \n",
            "    Negative Log Likelihood: 8.1618\tSigma2 Prior: -376.1544\tRegularization: 0.0007\n",
            "Iter: 4570  \tTraining Loss: -369.0822    \n",
            "    Negative Log Likelihood: 8.2120\tSigma2 Prior: -377.2950\tRegularization: 0.0007\n",
            "Iter: 4580  \tTraining Loss: -368.3584    \n",
            "    Negative Log Likelihood: 8.3463\tSigma2 Prior: -376.7054\tRegularization: 0.0007\n",
            "Iter: 4590  \tTraining Loss: -368.6785    \n",
            "    Negative Log Likelihood: 8.4250\tSigma2 Prior: -377.1042\tRegularization: 0.0007\n",
            "Iter: 4600  \tTraining Loss: -369.5667    \n",
            "    Negative Log Likelihood: 8.3753\tSigma2 Prior: -377.9427\tRegularization: 0.0007\n",
            "Iter: 4610  \tTraining Loss: -369.3099    \n",
            "    Negative Log Likelihood: 8.4143\tSigma2 Prior: -377.7249\tRegularization: 0.0007\n",
            "Iter: 4620  \tTraining Loss: -370.2334    \n",
            "    Negative Log Likelihood: 8.3653\tSigma2 Prior: -378.5994\tRegularization: 0.0007\n",
            "Iter: 4630  \tTraining Loss: -368.9920    \n",
            "    Negative Log Likelihood: 8.5579\tSigma2 Prior: -377.5506\tRegularization: 0.0007\n",
            "Iter: 4640  \tTraining Loss: -370.1317    \n",
            "    Negative Log Likelihood: 8.3750\tSigma2 Prior: -378.5074\tRegularization: 0.0007\n",
            "Iter: 4650  \tTraining Loss: -371.0686    \n",
            "    Negative Log Likelihood: 8.3839\tSigma2 Prior: -379.4532\tRegularization: 0.0007\n",
            "Iter: 4660  \tTraining Loss: -370.4164    \n",
            "    Negative Log Likelihood: 8.6057\tSigma2 Prior: -379.0229\tRegularization: 0.0007\n",
            "Iter: 4670  \tTraining Loss: -372.2261    \n",
            "    Negative Log Likelihood: 8.2107\tSigma2 Prior: -380.4376\tRegularization: 0.0007\n",
            "Iter: 4680  \tTraining Loss: -372.2382    \n",
            "    Negative Log Likelihood: 8.3631\tSigma2 Prior: -380.6021\tRegularization: 0.0007\n",
            "Iter: 4690  \tTraining Loss: -372.4813    \n",
            "    Negative Log Likelihood: 8.2914\tSigma2 Prior: -380.7735\tRegularization: 0.0007\n",
            "Iter: 4700  \tTraining Loss: -373.4405    \n",
            "    Negative Log Likelihood: 8.3531\tSigma2 Prior: -381.7943\tRegularization: 0.0007\n",
            "Iter: 4710  \tTraining Loss: -372.8898    \n",
            "    Negative Log Likelihood: 8.4664\tSigma2 Prior: -381.3570\tRegularization: 0.0007\n",
            "Iter: 4720  \tTraining Loss: -373.1383    \n",
            "    Negative Log Likelihood: 8.5826\tSigma2 Prior: -381.7217\tRegularization: 0.0007\n",
            "Iter: 4730  \tTraining Loss: -372.1299    \n",
            "    Negative Log Likelihood: 8.5787\tSigma2 Prior: -380.7094\tRegularization: 0.0007\n",
            "Iter: 4740  \tTraining Loss: -375.3569    \n",
            "    Negative Log Likelihood: 8.2194\tSigma2 Prior: -383.5770\tRegularization: 0.0007\n",
            "Iter: 4750  \tTraining Loss: -373.3911    \n",
            "    Negative Log Likelihood: 8.6372\tSigma2 Prior: -382.0291\tRegularization: 0.0007\n",
            "Iter: 4760  \tTraining Loss: -372.7780    \n",
            "    Negative Log Likelihood: 8.7464\tSigma2 Prior: -381.5251\tRegularization: 0.0007\n",
            "Iter: 4770  \tTraining Loss: -374.0862    \n",
            "    Negative Log Likelihood: 8.5211\tSigma2 Prior: -382.6081\tRegularization: 0.0007\n",
            "Iter: 4780  \tTraining Loss: -374.1310    \n",
            "    Negative Log Likelihood: 8.6090\tSigma2 Prior: -382.7407\tRegularization: 0.0007\n",
            "Iter: 4790  \tTraining Loss: -375.7122    \n",
            "    Negative Log Likelihood: 8.4853\tSigma2 Prior: -384.1982\tRegularization: 0.0007\n",
            "Iter: 4800  \tTraining Loss: -375.2843    \n",
            "    Negative Log Likelihood: 8.6127\tSigma2 Prior: -383.8977\tRegularization: 0.0007\n",
            "Iter: 4810  \tTraining Loss: -376.3425    \n",
            "    Negative Log Likelihood: 8.3944\tSigma2 Prior: -384.7376\tRegularization: 0.0007\n",
            "Iter: 4820  \tTraining Loss: -377.2884    \n",
            "    Negative Log Likelihood: 8.3483\tSigma2 Prior: -385.6374\tRegularization: 0.0007\n",
            "Iter: 4830  \tTraining Loss: -375.6486    \n",
            "    Negative Log Likelihood: 8.6210\tSigma2 Prior: -384.2703\tRegularization: 0.0007\n",
            "Iter: 4840  \tTraining Loss: -376.3006    \n",
            "    Negative Log Likelihood: 8.7589\tSigma2 Prior: -385.0602\tRegularization: 0.0007\n",
            "Iter: 4850  \tTraining Loss: -377.1763    \n",
            "    Negative Log Likelihood: 8.4879\tSigma2 Prior: -385.6649\tRegularization: 0.0007\n",
            "Iter: 4860  \tTraining Loss: -376.6113    \n",
            "    Negative Log Likelihood: 8.5924\tSigma2 Prior: -385.2044\tRegularization: 0.0007\n",
            "Iter: 4870  \tTraining Loss: -376.3313    \n",
            "    Negative Log Likelihood: 8.8303\tSigma2 Prior: -385.1623\tRegularization: 0.0007\n",
            "Iter: 4880  \tTraining Loss: -377.9268    \n",
            "    Negative Log Likelihood: 8.8473\tSigma2 Prior: -386.7749\tRegularization: 0.0007\n",
            "Iter: 4890  \tTraining Loss: -375.6857    \n",
            "    Negative Log Likelihood: 9.0133\tSigma2 Prior: -384.6997\tRegularization: 0.0007\n",
            "Iter: 4900  \tTraining Loss: -377.8400    \n",
            "    Negative Log Likelihood: 8.9413\tSigma2 Prior: -386.7820\tRegularization: 0.0007\n",
            "Iter: 4910  \tTraining Loss: -376.9622    \n",
            "    Negative Log Likelihood: 9.1429\tSigma2 Prior: -386.1058\tRegularization: 0.0007\n",
            "Iter: 4920  \tTraining Loss: -379.1977    \n",
            "    Negative Log Likelihood: 8.7025\tSigma2 Prior: -387.9010\tRegularization: 0.0007\n",
            "Iter: 4930  \tTraining Loss: -379.8451    \n",
            "    Negative Log Likelihood: 8.5581\tSigma2 Prior: -388.4040\tRegularization: 0.0007\n",
            "Iter: 4940  \tTraining Loss: -380.1325    \n",
            "    Negative Log Likelihood: 8.7153\tSigma2 Prior: -388.8485\tRegularization: 0.0007\n",
            "Iter: 4950  \tTraining Loss: -379.1023    \n",
            "    Negative Log Likelihood: 8.9934\tSigma2 Prior: -388.0964\tRegularization: 0.0007\n",
            "Iter: 4960  \tTraining Loss: -379.3206    \n",
            "    Negative Log Likelihood: 8.9943\tSigma2 Prior: -388.3157\tRegularization: 0.0007\n",
            "Iter: 4970  \tTraining Loss: -377.9373    \n",
            "    Negative Log Likelihood: 9.1707\tSigma2 Prior: -387.1087\tRegularization: 0.0007\n",
            "Iter: 4980  \tTraining Loss: -381.4373    \n",
            "    Negative Log Likelihood: 8.8440\tSigma2 Prior: -390.2820\tRegularization: 0.0007\n",
            "Iter: 4990  \tTraining Loss: -380.0856    \n",
            "    Negative Log Likelihood: 8.9676\tSigma2 Prior: -389.0540\tRegularization: 0.0007\n",
            "Iter: 5000  \tTraining Loss: -380.9166    \n",
            "    Negative Log Likelihood: 9.0296\tSigma2 Prior: -389.9470\tRegularization: 0.0007\n",
            "Iter: 5010  \tTraining Loss: -381.9248    \n",
            "    Negative Log Likelihood: 8.8122\tSigma2 Prior: -390.7378\tRegularization: 0.0007\n",
            "Iter: 5020  \tTraining Loss: -381.7586    \n",
            "    Negative Log Likelihood: 9.0489\tSigma2 Prior: -390.8083\tRegularization: 0.0007\n",
            "Iter: 5030  \tTraining Loss: -381.1436    \n",
            "    Negative Log Likelihood: 9.0028\tSigma2 Prior: -390.1471\tRegularization: 0.0007\n",
            "Iter: 5040  \tTraining Loss: -380.7190    \n",
            "    Negative Log Likelihood: 9.3378\tSigma2 Prior: -390.0576\tRegularization: 0.0007\n",
            "Iter: 5050  \tTraining Loss: -382.0078    \n",
            "    Negative Log Likelihood: 9.2010\tSigma2 Prior: -391.2094\tRegularization: 0.0007\n",
            "Iter: 5060  \tTraining Loss: -382.6853    \n",
            "    Negative Log Likelihood: 9.2847\tSigma2 Prior: -391.9707\tRegularization: 0.0007\n",
            "Iter: 5070  \tTraining Loss: -380.8205    \n",
            "    Negative Log Likelihood: 9.3101\tSigma2 Prior: -390.1313\tRegularization: 0.0007\n",
            "Iter: 5080  \tTraining Loss: -382.4961    \n",
            "    Negative Log Likelihood: 9.1435\tSigma2 Prior: -391.6403\tRegularization: 0.0007\n",
            "Iter: 5090  \tTraining Loss: -383.2709    \n",
            "    Negative Log Likelihood: 9.2206\tSigma2 Prior: -392.4923\tRegularization: 0.0007\n",
            "Iter: 5100  \tTraining Loss: -384.9627    \n",
            "    Negative Log Likelihood: 9.0124\tSigma2 Prior: -393.9759\tRegularization: 0.0007\n",
            "Iter: 5110  \tTraining Loss: -383.2400    \n",
            "    Negative Log Likelihood: 9.3289\tSigma2 Prior: -392.5696\tRegularization: 0.0007\n",
            "Iter: 5120  \tTraining Loss: -384.5728    \n",
            "    Negative Log Likelihood: 9.1797\tSigma2 Prior: -393.7532\tRegularization: 0.0007\n",
            "Iter: 5130  \tTraining Loss: -385.2114    \n",
            "    Negative Log Likelihood: 9.2253\tSigma2 Prior: -394.4374\tRegularization: 0.0007\n",
            "Iter: 5140  \tTraining Loss: -385.7248    \n",
            "    Negative Log Likelihood: 9.2174\tSigma2 Prior: -394.9430\tRegularization: 0.0007\n",
            "Iter: 5150  \tTraining Loss: -383.1345    \n",
            "    Negative Log Likelihood: 9.4453\tSigma2 Prior: -392.5805\tRegularization: 0.0008\n",
            "Iter: 5160  \tTraining Loss: -384.4917    \n",
            "    Negative Log Likelihood: 9.5188\tSigma2 Prior: -394.0112\tRegularization: 0.0008\n",
            "Iter: 5170  \tTraining Loss: -384.9984    \n",
            "    Negative Log Likelihood: 9.4912\tSigma2 Prior: -394.4904\tRegularization: 0.0008\n",
            "Iter: 5180  \tTraining Loss: -385.0042    \n",
            "    Negative Log Likelihood: 9.4612\tSigma2 Prior: -394.4661\tRegularization: 0.0008\n",
            "Iter: 5190  \tTraining Loss: -387.0176    \n",
            "    Negative Log Likelihood: 9.1742\tSigma2 Prior: -396.1926\tRegularization: 0.0008\n",
            "Iter: 5200  \tTraining Loss: -386.9261    \n",
            "    Negative Log Likelihood: 9.2050\tSigma2 Prior: -396.1318\tRegularization: 0.0008\n",
            "Iter: 5210  \tTraining Loss: -386.8253    \n",
            "    Negative Log Likelihood: 9.3894\tSigma2 Prior: -396.2155\tRegularization: 0.0008\n",
            "Iter: 5220  \tTraining Loss: -387.9471    \n",
            "    Negative Log Likelihood: 9.3377\tSigma2 Prior: -397.2855\tRegularization: 0.0008\n",
            "Iter: 5230  \tTraining Loss: -386.8332    \n",
            "    Negative Log Likelihood: 9.6661\tSigma2 Prior: -396.5001\tRegularization: 0.0008\n",
            "Iter: 5240  \tTraining Loss: -387.9100    \n",
            "    Negative Log Likelihood: 9.6324\tSigma2 Prior: -397.5431\tRegularization: 0.0008\n",
            "Iter: 5250  \tTraining Loss: -387.7305    \n",
            "    Negative Log Likelihood: 9.7352\tSigma2 Prior: -397.4665\tRegularization: 0.0008\n",
            "Iter: 5260  \tTraining Loss: -387.4734    \n",
            "    Negative Log Likelihood: 9.5389\tSigma2 Prior: -397.0131\tRegularization: 0.0008\n",
            "Iter: 5270  \tTraining Loss: -389.8197    \n",
            "    Negative Log Likelihood: 9.3597\tSigma2 Prior: -399.1802\tRegularization: 0.0008\n",
            "Iter: 5280  \tTraining Loss: -389.5290    \n",
            "    Negative Log Likelihood: 9.4714\tSigma2 Prior: -399.0011\tRegularization: 0.0008\n",
            "Iter: 5290  \tTraining Loss: -387.6592    \n",
            "    Negative Log Likelihood: 9.8268\tSigma2 Prior: -397.4867\tRegularization: 0.0008\n",
            "Iter: 5300  \tTraining Loss: -388.8640    \n",
            "    Negative Log Likelihood: 9.7359\tSigma2 Prior: -398.6007\tRegularization: 0.0008\n",
            "Iter: 5310  \tTraining Loss: -390.8466    \n",
            "    Negative Log Likelihood: 9.4826\tSigma2 Prior: -400.3300\tRegularization: 0.0008\n",
            "Iter: 5320  \tTraining Loss: -390.2502    \n",
            "    Negative Log Likelihood: 9.7400\tSigma2 Prior: -399.9910\tRegularization: 0.0008\n",
            "Iter: 5330  \tTraining Loss: -391.7709    \n",
            "    Negative Log Likelihood: 9.5095\tSigma2 Prior: -401.2812\tRegularization: 0.0008\n",
            "Iter: 5340  \tTraining Loss: -390.3275    \n",
            "    Negative Log Likelihood: 9.7397\tSigma2 Prior: -400.0679\tRegularization: 0.0008\n",
            "Iter: 5350  \tTraining Loss: -387.6026    \n",
            "    Negative Log Likelihood: 10.0364\tSigma2 Prior: -397.6397\tRegularization: 0.0008\n",
            "Iter: 5360  \tTraining Loss: -392.5012    \n",
            "    Negative Log Likelihood: 9.7994\tSigma2 Prior: -402.3013\tRegularization: 0.0008\n",
            "Iter: 5370  \tTraining Loss: -391.2769    \n",
            "    Negative Log Likelihood: 9.7026\tSigma2 Prior: -400.9803\tRegularization: 0.0008\n",
            "Iter: 5380  \tTraining Loss: -390.5697    \n",
            "    Negative Log Likelihood: 10.0684\tSigma2 Prior: -400.6389\tRegularization: 0.0008\n",
            "Iter: 5390  \tTraining Loss: -392.2435    \n",
            "    Negative Log Likelihood: 9.6967\tSigma2 Prior: -401.9409\tRegularization: 0.0008\n",
            "Iter: 5400  \tTraining Loss: -393.3644    \n",
            "    Negative Log Likelihood: 9.9673\tSigma2 Prior: -403.3324\tRegularization: 0.0008\n",
            "Iter: 5410  \tTraining Loss: -391.8484    \n",
            "    Negative Log Likelihood: 10.1159\tSigma2 Prior: -401.9651\tRegularization: 0.0008\n",
            "Iter: 5420  \tTraining Loss: -394.1296    \n",
            "    Negative Log Likelihood: 9.8683\tSigma2 Prior: -403.9987\tRegularization: 0.0008\n",
            "Iter: 5430  \tTraining Loss: -390.7971    \n",
            "    Negative Log Likelihood: 10.3633\tSigma2 Prior: -401.1612\tRegularization: 0.0008\n",
            "Iter: 5440  \tTraining Loss: -394.1135    \n",
            "    Negative Log Likelihood: 9.9707\tSigma2 Prior: -404.0849\tRegularization: 0.0008\n",
            "Iter: 5450  \tTraining Loss: -394.5494    \n",
            "    Negative Log Likelihood: 10.0282\tSigma2 Prior: -404.5784\tRegularization: 0.0008\n",
            "Iter: 5460  \tTraining Loss: -395.4637    \n",
            "    Negative Log Likelihood: 9.9440\tSigma2 Prior: -405.4084\tRegularization: 0.0008\n",
            "Iter: 5470  \tTraining Loss: -395.4055    \n",
            "    Negative Log Likelihood: 10.3140\tSigma2 Prior: -405.7202\tRegularization: 0.0008\n",
            "Iter: 5480  \tTraining Loss: -396.8685    \n",
            "    Negative Log Likelihood: 9.9724\tSigma2 Prior: -406.8417\tRegularization: 0.0008\n",
            "Iter: 5490  \tTraining Loss: -395.8051    \n",
            "    Negative Log Likelihood: 10.2719\tSigma2 Prior: -406.0779\tRegularization: 0.0008\n",
            "Iter: 5500  \tTraining Loss: -395.9985    \n",
            "    Negative Log Likelihood: 10.2622\tSigma2 Prior: -406.2614\tRegularization: 0.0008\n",
            "Iter: 5510  \tTraining Loss: -396.3616    \n",
            "    Negative Log Likelihood: 10.2792\tSigma2 Prior: -406.6415\tRegularization: 0.0008\n",
            "Iter: 5520  \tTraining Loss: -396.0542    \n",
            "    Negative Log Likelihood: 10.4034\tSigma2 Prior: -406.4583\tRegularization: 0.0008\n",
            "Iter: 5530  \tTraining Loss: -399.0206    \n",
            "    Negative Log Likelihood: 10.0363\tSigma2 Prior: -409.0577\tRegularization: 0.0008\n",
            "Iter: 5540  \tTraining Loss: -398.0061    \n",
            "    Negative Log Likelihood: 10.3842\tSigma2 Prior: -408.3911\tRegularization: 0.0008\n",
            "Iter: 5550  \tTraining Loss: -396.3118    \n",
            "    Negative Log Likelihood: 10.4432\tSigma2 Prior: -406.7557\tRegularization: 0.0008\n",
            "Iter: 5560  \tTraining Loss: -398.6223    \n",
            "    Negative Log Likelihood: 10.1503\tSigma2 Prior: -408.7734\tRegularization: 0.0008\n",
            "Iter: 5570  \tTraining Loss: -398.6065    \n",
            "    Negative Log Likelihood: 10.1166\tSigma2 Prior: -408.7239\tRegularization: 0.0008\n",
            "Iter: 5580  \tTraining Loss: -401.1833    \n",
            "    Negative Log Likelihood: 9.9065\tSigma2 Prior: -411.0905\tRegularization: 0.0008\n",
            "Iter: 5590  \tTraining Loss: -398.0583    \n",
            "    Negative Log Likelihood: 10.5322\tSigma2 Prior: -408.5913\tRegularization: 0.0008\n",
            "Iter: 5600  \tTraining Loss: -399.8643    \n",
            "    Negative Log Likelihood: 10.4721\tSigma2 Prior: -410.3372\tRegularization: 0.0008\n",
            "Iter: 5610  \tTraining Loss: -399.3531    \n",
            "    Negative Log Likelihood: 10.4824\tSigma2 Prior: -409.8362\tRegularization: 0.0008\n",
            "Iter: 5620  \tTraining Loss: -399.9252    \n",
            "    Negative Log Likelihood: 10.5431\tSigma2 Prior: -410.4690\tRegularization: 0.0008\n",
            "Iter: 5630  \tTraining Loss: -400.6635    \n",
            "    Negative Log Likelihood: 10.4892\tSigma2 Prior: -411.1534\tRegularization: 0.0008\n",
            "Iter: 5640  \tTraining Loss: -401.3833    \n",
            "    Negative Log Likelihood: 10.6527\tSigma2 Prior: -412.0368\tRegularization: 0.0008\n",
            "Iter: 5650  \tTraining Loss: -403.4908    \n",
            "    Negative Log Likelihood: 10.0444\tSigma2 Prior: -413.5359\tRegularization: 0.0008\n",
            "Iter: 5660  \tTraining Loss: -404.6932    \n",
            "    Negative Log Likelihood: 10.3623\tSigma2 Prior: -415.0562\tRegularization: 0.0008\n",
            "Iter: 5670  \tTraining Loss: -400.5909    \n",
            "    Negative Log Likelihood: 10.8761\tSigma2 Prior: -411.4677\tRegularization: 0.0008\n",
            "Iter: 5680  \tTraining Loss: -402.1723    \n",
            "    Negative Log Likelihood: 10.5373\tSigma2 Prior: -412.7104\tRegularization: 0.0008\n",
            "Iter: 5690  \tTraining Loss: -404.3504    \n",
            "    Negative Log Likelihood: 10.3965\tSigma2 Prior: -414.7477\tRegularization: 0.0008\n",
            "Iter: 5700  \tTraining Loss: -403.9278    \n",
            "    Negative Log Likelihood: 10.5860\tSigma2 Prior: -414.5146\tRegularization: 0.0008\n",
            "Iter: 5710  \tTraining Loss: -401.9089    \n",
            "    Negative Log Likelihood: 11.0167\tSigma2 Prior: -412.9264\tRegularization: 0.0008\n",
            "Iter: 5720  \tTraining Loss: -403.5798    \n",
            "    Negative Log Likelihood: 10.8724\tSigma2 Prior: -414.4530\tRegularization: 0.0008\n",
            "Iter: 5730  \tTraining Loss: -403.2699    \n",
            "    Negative Log Likelihood: 11.1324\tSigma2 Prior: -414.4030\tRegularization: 0.0008\n",
            "Iter: 5740  \tTraining Loss: -404.4574    \n",
            "    Negative Log Likelihood: 10.9439\tSigma2 Prior: -415.4020\tRegularization: 0.0008\n",
            "Iter: 5750  \tTraining Loss: -402.8752    \n",
            "    Negative Log Likelihood: 11.2746\tSigma2 Prior: -414.1505\tRegularization: 0.0008\n",
            "Iter: 5760  \tTraining Loss: -407.4216    \n",
            "    Negative Log Likelihood: 10.7555\tSigma2 Prior: -418.1778\tRegularization: 0.0008\n",
            "Iter: 5770  \tTraining Loss: -404.4264    \n",
            "    Negative Log Likelihood: 11.2005\tSigma2 Prior: -415.6277\tRegularization: 0.0008\n",
            "Iter: 5780  \tTraining Loss: -404.6705    \n",
            "    Negative Log Likelihood: 11.2446\tSigma2 Prior: -415.9159\tRegularization: 0.0008\n",
            "Iter: 5790  \tTraining Loss: -406.8834    \n",
            "    Negative Log Likelihood: 10.9146\tSigma2 Prior: -417.7987\tRegularization: 0.0008\n",
            "Iter: 5800  \tTraining Loss: -408.2156    \n",
            "    Negative Log Likelihood: 11.0144\tSigma2 Prior: -419.2307\tRegularization: 0.0008\n",
            "Iter: 5810  \tTraining Loss: -407.1675    \n",
            "    Negative Log Likelihood: 11.0916\tSigma2 Prior: -418.2598\tRegularization: 0.0008\n",
            "Iter: 5820  \tTraining Loss: -406.7649    \n",
            "    Negative Log Likelihood: 11.1081\tSigma2 Prior: -417.8737\tRegularization: 0.0008\n",
            "Iter: 5830  \tTraining Loss: -407.1980    \n",
            "    Negative Log Likelihood: 11.2943\tSigma2 Prior: -418.4931\tRegularization: 0.0008\n",
            "Iter: 5840  \tTraining Loss: -405.4424    \n",
            "    Negative Log Likelihood: 11.5966\tSigma2 Prior: -417.0398\tRegularization: 0.0008\n",
            "Iter: 5850  \tTraining Loss: -408.9995    \n",
            "    Negative Log Likelihood: 11.2228\tSigma2 Prior: -420.2231\tRegularization: 0.0008\n",
            "Iter: 5860  \tTraining Loss: -407.9875    \n",
            "    Negative Log Likelihood: 11.4250\tSigma2 Prior: -419.4133\tRegularization: 0.0008\n",
            "Iter: 5870  \tTraining Loss: -410.1112    \n",
            "    Negative Log Likelihood: 11.2555\tSigma2 Prior: -421.3674\tRegularization: 0.0008\n",
            "Iter: 5880  \tTraining Loss: -409.5327    \n",
            "    Negative Log Likelihood: 11.3848\tSigma2 Prior: -420.9183\tRegularization: 0.0008\n",
            "Iter: 5890  \tTraining Loss: -409.7581    \n",
            "    Negative Log Likelihood: 11.1854\tSigma2 Prior: -420.9443\tRegularization: 0.0008\n",
            "Iter: 5900  \tTraining Loss: -409.8992    \n",
            "    Negative Log Likelihood: 11.6290\tSigma2 Prior: -421.5290\tRegularization: 0.0008\n",
            "Iter: 5910  \tTraining Loss: -409.4746    \n",
            "    Negative Log Likelihood: 11.6130\tSigma2 Prior: -421.0884\tRegularization: 0.0008\n",
            "Iter: 5920  \tTraining Loss: -411.4750    \n",
            "    Negative Log Likelihood: 11.5441\tSigma2 Prior: -423.0199\tRegularization: 0.0008\n",
            "Iter: 5930  \tTraining Loss: -409.5202    \n",
            "    Negative Log Likelihood: 11.6084\tSigma2 Prior: -421.1293\tRegularization: 0.0008\n",
            "Iter: 5940  \tTraining Loss: -413.0000    \n",
            "    Negative Log Likelihood: 11.3617\tSigma2 Prior: -424.3624\tRegularization: 0.0008\n",
            "Iter: 5950  \tTraining Loss: -411.2528    \n",
            "    Negative Log Likelihood: 11.8865\tSigma2 Prior: -423.1400\tRegularization: 0.0008\n",
            "Iter: 5960  \tTraining Loss: -411.3040    \n",
            "    Negative Log Likelihood: 11.7589\tSigma2 Prior: -423.0637\tRegularization: 0.0008\n",
            "Iter: 5970  \tTraining Loss: -412.9189    \n",
            "    Negative Log Likelihood: 11.7458\tSigma2 Prior: -424.6655\tRegularization: 0.0008\n",
            "Iter: 5980  \tTraining Loss: -413.7164    \n",
            "    Negative Log Likelihood: 11.6150\tSigma2 Prior: -425.3321\tRegularization: 0.0008\n",
            "Iter: 5990  \tTraining Loss: -413.8818    \n",
            "    Negative Log Likelihood: 11.8970\tSigma2 Prior: -425.7796\tRegularization: 0.0008\n",
            "Iter: 6000  \tTraining Loss: -412.8607    \n",
            "    Negative Log Likelihood: 11.9308\tSigma2 Prior: -424.7922\tRegularization: 0.0008\n",
            "Iter: 6010  \tTraining Loss: -413.6689    \n",
            "    Negative Log Likelihood: 12.0960\tSigma2 Prior: -425.7657\tRegularization: 0.0008\n",
            "Iter: 6020  \tTraining Loss: -414.9795    \n",
            "    Negative Log Likelihood: 11.8426\tSigma2 Prior: -426.8229\tRegularization: 0.0008\n",
            "Iter: 6030  \tTraining Loss: -414.0981    \n",
            "    Negative Log Likelihood: 12.0725\tSigma2 Prior: -426.1714\tRegularization: 0.0008\n",
            "Iter: 6040  \tTraining Loss: -413.7501    \n",
            "    Negative Log Likelihood: 12.0494\tSigma2 Prior: -425.8003\tRegularization: 0.0008\n",
            "Iter: 6050  \tTraining Loss: -410.9631    \n",
            "    Negative Log Likelihood: 12.4864\tSigma2 Prior: -423.4502\tRegularization: 0.0008\n",
            "Iter: 6060  \tTraining Loss: -416.5101    \n",
            "    Negative Log Likelihood: 12.1993\tSigma2 Prior: -428.7102\tRegularization: 0.0008\n",
            "Iter: 6070  \tTraining Loss: -415.8263    \n",
            "    Negative Log Likelihood: 12.1845\tSigma2 Prior: -428.0116\tRegularization: 0.0008\n",
            "Iter: 6080  \tTraining Loss: -417.0940    \n",
            "    Negative Log Likelihood: 12.1224\tSigma2 Prior: -429.2172\tRegularization: 0.0008\n",
            "Iter: 6090  \tTraining Loss: -418.9612    \n",
            "    Negative Log Likelihood: 12.0727\tSigma2 Prior: -431.0347\tRegularization: 0.0008\n",
            "Iter: 6100  \tTraining Loss: -417.6848    \n",
            "    Negative Log Likelihood: 12.2883\tSigma2 Prior: -429.9740\tRegularization: 0.0008\n",
            "Iter: 6110  \tTraining Loss: -419.4264    \n",
            "    Negative Log Likelihood: 12.1424\tSigma2 Prior: -431.5696\tRegularization: 0.0008\n",
            "Iter: 6120  \tTraining Loss: -417.9900    \n",
            "    Negative Log Likelihood: 12.3461\tSigma2 Prior: -430.3369\tRegularization: 0.0008\n",
            "Iter: 6130  \tTraining Loss: -417.0412    \n",
            "    Negative Log Likelihood: 12.6937\tSigma2 Prior: -429.7357\tRegularization: 0.0008\n",
            "Iter: 6140  \tTraining Loss: -420.4219    \n",
            "    Negative Log Likelihood: 12.3676\tSigma2 Prior: -432.7903\tRegularization: 0.0008\n",
            "Iter: 6150  \tTraining Loss: -420.9168    \n",
            "    Negative Log Likelihood: 12.4222\tSigma2 Prior: -433.3398\tRegularization: 0.0008\n",
            "Iter: 6160  \tTraining Loss: -419.7309    \n",
            "    Negative Log Likelihood: 12.3504\tSigma2 Prior: -432.0821\tRegularization: 0.0008\n",
            "Iter: 6170  \tTraining Loss: -421.8176    \n",
            "    Negative Log Likelihood: 12.3451\tSigma2 Prior: -434.1635\tRegularization: 0.0008\n",
            "Iter: 6180  \tTraining Loss: -419.1259    \n",
            "    Negative Log Likelihood: 12.7778\tSigma2 Prior: -431.9045\tRegularization: 0.0008\n",
            "Iter: 6190  \tTraining Loss: -418.6667    \n",
            "    Negative Log Likelihood: 13.0648\tSigma2 Prior: -431.7322\tRegularization: 0.0008\n",
            "Iter: 6200  \tTraining Loss: -424.4419    \n",
            "    Negative Log Likelihood: 12.2866\tSigma2 Prior: -436.7293\tRegularization: 0.0008\n",
            "Iter: 6210  \tTraining Loss: -420.7793    \n",
            "    Negative Log Likelihood: 13.0733\tSigma2 Prior: -433.8534\tRegularization: 0.0008\n",
            "Iter: 6220  \tTraining Loss: -421.0236    \n",
            "    Negative Log Likelihood: 12.8933\tSigma2 Prior: -433.9177\tRegularization: 0.0008\n",
            "Iter: 6230  \tTraining Loss: -421.4831    \n",
            "    Negative Log Likelihood: 13.0186\tSigma2 Prior: -434.5026\tRegularization: 0.0008\n",
            "Iter: 6240  \tTraining Loss: -424.4173    \n",
            "    Negative Log Likelihood: 12.7595\tSigma2 Prior: -437.1776\tRegularization: 0.0008\n",
            "Iter: 6250  \tTraining Loss: -426.4265    \n",
            "    Negative Log Likelihood: 12.7916\tSigma2 Prior: -439.2189\tRegularization: 0.0008\n",
            "Iter: 6260  \tTraining Loss: -420.2910    \n",
            "    Negative Log Likelihood: 13.4023\tSigma2 Prior: -433.6941\tRegularization: 0.0008\n",
            "Iter: 6270  \tTraining Loss: -424.0731    \n",
            "    Negative Log Likelihood: 13.2303\tSigma2 Prior: -437.3041\tRegularization: 0.0008\n",
            "Iter: 6280  \tTraining Loss: -421.2346    \n",
            "    Negative Log Likelihood: 13.3375\tSigma2 Prior: -434.5729\tRegularization: 0.0008\n",
            "Iter: 6290  \tTraining Loss: -426.4928    \n",
            "    Negative Log Likelihood: 13.0999\tSigma2 Prior: -439.5934\tRegularization: 0.0008\n",
            "Iter: 6300  \tTraining Loss: -425.1454    \n",
            "    Negative Log Likelihood: 13.4046\tSigma2 Prior: -438.5508\tRegularization: 0.0008\n",
            "Iter: 6310  \tTraining Loss: -428.1240    \n",
            "    Negative Log Likelihood: 13.2334\tSigma2 Prior: -441.3582\tRegularization: 0.0008\n",
            "Iter: 6320  \tTraining Loss: -430.2313    \n",
            "    Negative Log Likelihood: 12.8605\tSigma2 Prior: -443.0926\tRegularization: 0.0008\n",
            "Iter: 6330  \tTraining Loss: -426.9388    \n",
            "    Negative Log Likelihood: 13.4395\tSigma2 Prior: -440.3792\tRegularization: 0.0008\n",
            "Iter: 6340  \tTraining Loss: -426.7772    \n",
            "    Negative Log Likelihood: 13.5171\tSigma2 Prior: -440.2950\tRegularization: 0.0008\n",
            "Iter: 6350  \tTraining Loss: -431.1541    \n",
            "    Negative Log Likelihood: 12.9167\tSigma2 Prior: -444.0717\tRegularization: 0.0008\n",
            "Iter: 6360  \tTraining Loss: -430.1082    \n",
            "    Negative Log Likelihood: 13.3376\tSigma2 Prior: -443.4466\tRegularization: 0.0008\n",
            "Iter: 6370  \tTraining Loss: -430.8925    \n",
            "    Negative Log Likelihood: 13.3787\tSigma2 Prior: -444.2720\tRegularization: 0.0008\n",
            "Iter: 6380  \tTraining Loss: -431.4221    \n",
            "    Negative Log Likelihood: 13.2880\tSigma2 Prior: -444.7108\tRegularization: 0.0008\n",
            "Iter: 6390  \tTraining Loss: -428.2906    \n",
            "    Negative Log Likelihood: 13.9390\tSigma2 Prior: -442.2304\tRegularization: 0.0008\n",
            "Iter: 6400  \tTraining Loss: -429.6291    \n",
            "    Negative Log Likelihood: 13.7136\tSigma2 Prior: -443.3434\tRegularization: 0.0008\n",
            "Iter: 6410  \tTraining Loss: -428.2216    \n",
            "    Negative Log Likelihood: 13.9490\tSigma2 Prior: -442.1714\tRegularization: 0.0008\n",
            "Iter: 6420  \tTraining Loss: -429.1358    \n",
            "    Negative Log Likelihood: 13.9133\tSigma2 Prior: -443.0499\tRegularization: 0.0008\n",
            "Iter: 6430  \tTraining Loss: -434.3179    \n",
            "    Negative Log Likelihood: 13.4288\tSigma2 Prior: -447.7475\tRegularization: 0.0008\n",
            "Iter: 6440  \tTraining Loss: -431.3125    \n",
            "    Negative Log Likelihood: 13.9451\tSigma2 Prior: -445.2585\tRegularization: 0.0008\n",
            "Iter: 6450  \tTraining Loss: -429.7793    \n",
            "    Negative Log Likelihood: 14.1582\tSigma2 Prior: -443.9383\tRegularization: 0.0008\n",
            "Iter: 6460  \tTraining Loss: -435.2849    \n",
            "    Negative Log Likelihood: 13.4747\tSigma2 Prior: -448.7603\tRegularization: 0.0008\n",
            "Iter: 6470  \tTraining Loss: -433.9859    \n",
            "    Negative Log Likelihood: 13.9651\tSigma2 Prior: -447.9518\tRegularization: 0.0008\n",
            "Iter: 6480  \tTraining Loss: -432.8212    \n",
            "    Negative Log Likelihood: 14.4229\tSigma2 Prior: -447.2449\tRegularization: 0.0008\n",
            "Iter: 6490  \tTraining Loss: -434.4546    \n",
            "    Negative Log Likelihood: 14.2478\tSigma2 Prior: -448.7031\tRegularization: 0.0008\n",
            "Iter: 6500  \tTraining Loss: -437.4782    \n",
            "    Negative Log Likelihood: 14.0625\tSigma2 Prior: -451.5415\tRegularization: 0.0008\n",
            "Iter: 6510  \tTraining Loss: -436.8997    \n",
            "    Negative Log Likelihood: 14.1742\tSigma2 Prior: -451.0747\tRegularization: 0.0008\n",
            "Iter: 6520  \tTraining Loss: -436.0211    \n",
            "    Negative Log Likelihood: 14.5881\tSigma2 Prior: -450.6100\tRegularization: 0.0008\n",
            "Iter: 6530  \tTraining Loss: -434.6023    \n",
            "    Negative Log Likelihood: 14.7293\tSigma2 Prior: -449.3324\tRegularization: 0.0008\n",
            "Iter: 6540  \tTraining Loss: -440.8199    \n",
            "    Negative Log Likelihood: 14.0191\tSigma2 Prior: -454.8398\tRegularization: 0.0008\n",
            "Iter: 6550  \tTraining Loss: -432.1784    \n",
            "    Negative Log Likelihood: 15.1056\tSigma2 Prior: -447.2848\tRegularization: 0.0008\n",
            "Iter: 6560  \tTraining Loss: -436.5066    \n",
            "    Negative Log Likelihood: 14.9236\tSigma2 Prior: -451.4311\tRegularization: 0.0008\n",
            "Iter: 6570  \tTraining Loss: -439.0529    \n",
            "    Negative Log Likelihood: 14.5216\tSigma2 Prior: -453.5753\tRegularization: 0.0008\n",
            "Iter: 6580  \tTraining Loss: -435.9606    \n",
            "    Negative Log Likelihood: 15.3070\tSigma2 Prior: -451.2684\tRegularization: 0.0008\n",
            "Iter: 6590  \tTraining Loss: -439.6330    \n",
            "    Negative Log Likelihood: 14.8118\tSigma2 Prior: -454.4456\tRegularization: 0.0008\n",
            "Iter: 6600  \tTraining Loss: -441.3935    \n",
            "    Negative Log Likelihood: 14.6590\tSigma2 Prior: -456.0533\tRegularization: 0.0008\n",
            "Iter: 6610  \tTraining Loss: -442.4038    \n",
            "    Negative Log Likelihood: 14.5335\tSigma2 Prior: -456.9382\tRegularization: 0.0008\n",
            "Iter: 6620  \tTraining Loss: -443.0829    \n",
            "    Negative Log Likelihood: 14.9375\tSigma2 Prior: -458.0211\tRegularization: 0.0008\n",
            "Iter: 6630  \tTraining Loss: -442.8047    \n",
            "    Negative Log Likelihood: 14.9748\tSigma2 Prior: -457.7803\tRegularization: 0.0008\n",
            "Iter: 6640  \tTraining Loss: -440.2959    \n",
            "    Negative Log Likelihood: 14.9822\tSigma2 Prior: -455.2790\tRegularization: 0.0008\n",
            "Iter: 6650  \tTraining Loss: -444.0372    \n",
            "    Negative Log Likelihood: 15.2769\tSigma2 Prior: -459.3148\tRegularization: 0.0008\n",
            "Iter: 6660  \tTraining Loss: -440.7534    \n",
            "    Negative Log Likelihood: 15.3479\tSigma2 Prior: -456.1021\tRegularization: 0.0008\n",
            "Iter: 6670  \tTraining Loss: -445.0945    \n",
            "    Negative Log Likelihood: 14.9529\tSigma2 Prior: -460.0482\tRegularization: 0.0008\n",
            "Iter: 6680  \tTraining Loss: -445.1902    \n",
            "    Negative Log Likelihood: 15.1448\tSigma2 Prior: -460.3358\tRegularization: 0.0008\n",
            "Iter: 6690  \tTraining Loss: -447.6270    \n",
            "    Negative Log Likelihood: 14.6326\tSigma2 Prior: -462.2604\tRegularization: 0.0008\n",
            "Iter: 6700  \tTraining Loss: -445.5036    \n",
            "    Negative Log Likelihood: 15.3944\tSigma2 Prior: -460.8988\tRegularization: 0.0008\n",
            "Iter: 6710  \tTraining Loss: -442.3142    \n",
            "    Negative Log Likelihood: 16.0516\tSigma2 Prior: -458.3666\tRegularization: 0.0008\n",
            "Iter: 6720  \tTraining Loss: -446.7054    \n",
            "    Negative Log Likelihood: 15.5640\tSigma2 Prior: -462.2703\tRegularization: 0.0008\n",
            "Iter: 6730  \tTraining Loss: -444.5466    \n",
            "    Negative Log Likelihood: 15.9821\tSigma2 Prior: -460.5295\tRegularization: 0.0008\n",
            "Iter: 6740  \tTraining Loss: -442.3990    \n",
            "    Negative Log Likelihood: 16.4391\tSigma2 Prior: -458.8389\tRegularization: 0.0008\n",
            "Iter: 6750  \tTraining Loss: -445.8374    \n",
            "    Negative Log Likelihood: 16.1344\tSigma2 Prior: -461.9726\tRegularization: 0.0008\n",
            "Iter: 6760  \tTraining Loss: -449.2892    \n",
            "    Negative Log Likelihood: 15.9674\tSigma2 Prior: -465.2574\tRegularization: 0.0008\n",
            "Iter: 6770  \tTraining Loss: -449.3406    \n",
            "    Negative Log Likelihood: 15.8180\tSigma2 Prior: -465.1594\tRegularization: 0.0008\n",
            "Iter: 6780  \tTraining Loss: -450.3041    \n",
            "    Negative Log Likelihood: 16.0270\tSigma2 Prior: -466.3319\tRegularization: 0.0008\n",
            "Iter: 6790  \tTraining Loss: -446.1217    \n",
            "    Negative Log Likelihood: 16.7333\tSigma2 Prior: -462.8558\tRegularization: 0.0008\n",
            "Iter: 6800  \tTraining Loss: -440.4065    \n",
            "    Negative Log Likelihood: 17.4376\tSigma2 Prior: -457.8448\tRegularization: 0.0008\n",
            "Iter: 6810  \tTraining Loss: -453.2474    \n",
            "    Negative Log Likelihood: 16.2822\tSigma2 Prior: -469.5304\tRegularization: 0.0008\n",
            "Iter: 6820  \tTraining Loss: -452.6647    \n",
            "    Negative Log Likelihood: 16.3888\tSigma2 Prior: -469.0542\tRegularization: 0.0008\n",
            "Iter: 6830  \tTraining Loss: -447.7619    \n",
            "    Negative Log Likelihood: 16.8395\tSigma2 Prior: -464.6022\tRegularization: 0.0008\n",
            "Iter: 6840  \tTraining Loss: -453.0515    \n",
            "    Negative Log Likelihood: 16.7375\tSigma2 Prior: -469.7898\tRegularization: 0.0008\n",
            "Iter: 6850  \tTraining Loss: -454.6275    \n",
            "    Negative Log Likelihood: 16.4276\tSigma2 Prior: -471.0559\tRegularization: 0.0008\n",
            "Iter: 6860  \tTraining Loss: -450.1487    \n",
            "    Negative Log Likelihood: 17.5559\tSigma2 Prior: -467.7054\tRegularization: 0.0008\n",
            "Iter: 6870  \tTraining Loss: -456.2667    \n",
            "    Negative Log Likelihood: 16.5193\tSigma2 Prior: -472.7868\tRegularization: 0.0008\n",
            "Iter: 6880  \tTraining Loss: -450.8421    \n",
            "    Negative Log Likelihood: 17.5899\tSigma2 Prior: -468.4328\tRegularization: 0.0008\n",
            "Iter: 6890  \tTraining Loss: -454.5465    \n",
            "    Negative Log Likelihood: 17.3894\tSigma2 Prior: -471.9367\tRegularization: 0.0008\n",
            "Iter: 6900  \tTraining Loss: -451.9824    \n",
            "    Negative Log Likelihood: 17.8157\tSigma2 Prior: -469.7989\tRegularization: 0.0008\n",
            "Iter: 6910  \tTraining Loss: -456.4572    \n",
            "    Negative Log Likelihood: 17.5746\tSigma2 Prior: -474.0325\tRegularization: 0.0008\n",
            "Iter: 6920  \tTraining Loss: -455.5033    \n",
            "    Negative Log Likelihood: 17.7519\tSigma2 Prior: -473.2560\tRegularization: 0.0008\n",
            "Iter: 6930  \tTraining Loss: -451.8847    \n",
            "    Negative Log Likelihood: 18.0387\tSigma2 Prior: -469.9242\tRegularization: 0.0008\n",
            "Iter: 6940  \tTraining Loss: -454.8759    \n",
            "    Negative Log Likelihood: 17.5865\tSigma2 Prior: -472.4632\tRegularization: 0.0008\n",
            "Iter: 6950  \tTraining Loss: -452.9795    \n",
            "    Negative Log Likelihood: 18.0797\tSigma2 Prior: -471.0600\tRegularization: 0.0008\n",
            "Iter: 6960  \tTraining Loss: -459.4375    \n",
            "    Negative Log Likelihood: 17.8945\tSigma2 Prior: -477.3327\tRegularization: 0.0008\n",
            "Iter: 6970  \tTraining Loss: -456.5303    \n",
            "    Negative Log Likelihood: 17.9341\tSigma2 Prior: -474.4652\tRegularization: 0.0008\n",
            "Iter: 6980  \tTraining Loss: -461.0793    \n",
            "    Negative Log Likelihood: 17.8180\tSigma2 Prior: -478.8981\tRegularization: 0.0008\n",
            "Iter: 6990  \tTraining Loss: -463.5779    \n",
            "    Negative Log Likelihood: 17.5515\tSigma2 Prior: -481.1302\tRegularization: 0.0008\n",
            "Iter: 7000  \tTraining Loss: -457.5302    \n",
            "    Negative Log Likelihood: 18.7231\tSigma2 Prior: -476.2541\tRegularization: 0.0008\n",
            "Iter: 7010  \tTraining Loss: -455.7859    \n",
            "    Negative Log Likelihood: 19.0533\tSigma2 Prior: -474.8400\tRegularization: 0.0008\n",
            "Iter: 7020  \tTraining Loss: -461.2970    \n",
            "    Negative Log Likelihood: 18.6007\tSigma2 Prior: -479.8985\tRegularization: 0.0008\n",
            "Iter: 7030  \tTraining Loss: -461.2036    \n",
            "    Negative Log Likelihood: 18.9519\tSigma2 Prior: -480.1563\tRegularization: 0.0008\n",
            "Iter: 7040  \tTraining Loss: -462.1520    \n",
            "    Negative Log Likelihood: 18.7773\tSigma2 Prior: -480.9301\tRegularization: 0.0008\n",
            "Iter: 7050  \tTraining Loss: -461.4865    \n",
            "    Negative Log Likelihood: 19.2905\tSigma2 Prior: -480.7778\tRegularization: 0.0008\n",
            "Iter: 7060  \tTraining Loss: -464.8907    \n",
            "    Negative Log Likelihood: 18.6832\tSigma2 Prior: -483.5747\tRegularization: 0.0008\n",
            "Iter: 7070  \tTraining Loss: -470.1121    \n",
            "    Negative Log Likelihood: 18.2730\tSigma2 Prior: -488.3858\tRegularization: 0.0008\n",
            "Iter: 7080  \tTraining Loss: -466.5193    \n",
            "    Negative Log Likelihood: 19.0580\tSigma2 Prior: -485.5780\tRegularization: 0.0008\n",
            "Iter: 7090  \tTraining Loss: -465.2942    \n",
            "    Negative Log Likelihood: 19.4177\tSigma2 Prior: -484.7126\tRegularization: 0.0008\n",
            "Iter: 7100  \tTraining Loss: -465.2536    \n",
            "    Negative Log Likelihood: 19.4688\tSigma2 Prior: -484.7232\tRegularization: 0.0008\n",
            "Iter: 7110  \tTraining Loss: -464.5887    \n",
            "    Negative Log Likelihood: 19.2686\tSigma2 Prior: -483.8581\tRegularization: 0.0008\n",
            "Iter: 7120  \tTraining Loss: -468.7218    \n",
            "    Negative Log Likelihood: 19.5981\tSigma2 Prior: -488.3207\tRegularization: 0.0008\n",
            "Iter: 7130  \tTraining Loss: -469.9091    \n",
            "    Negative Log Likelihood: 19.3305\tSigma2 Prior: -489.2404\tRegularization: 0.0008\n",
            "Iter: 7140  \tTraining Loss: -468.5904    \n",
            "    Negative Log Likelihood: 19.6299\tSigma2 Prior: -488.2211\tRegularization: 0.0008\n",
            "Iter: 7150  \tTraining Loss: -468.9172    \n",
            "    Negative Log Likelihood: 20.2757\tSigma2 Prior: -489.1937\tRegularization: 0.0008\n",
            "Iter: 7160  \tTraining Loss: -470.8810    \n",
            "    Negative Log Likelihood: 19.9043\tSigma2 Prior: -490.7861\tRegularization: 0.0008\n",
            "Iter: 7170  \tTraining Loss: -466.0813    \n",
            "    Negative Log Likelihood: 20.7942\tSigma2 Prior: -486.8764\tRegularization: 0.0008\n",
            "Iter: 7180  \tTraining Loss: -472.8883    \n",
            "    Negative Log Likelihood: 20.5286\tSigma2 Prior: -493.4178\tRegularization: 0.0008\n",
            "Iter: 7190  \tTraining Loss: -470.2331    \n",
            "    Negative Log Likelihood: 20.9175\tSigma2 Prior: -491.1514\tRegularization: 0.0008\n",
            "Iter: 7200  \tTraining Loss: -473.5451    \n",
            "    Negative Log Likelihood: 20.6155\tSigma2 Prior: -494.1614\tRegularization: 0.0008\n",
            "Iter: 7210  \tTraining Loss: -474.3069    \n",
            "    Negative Log Likelihood: 20.7426\tSigma2 Prior: -495.0503\tRegularization: 0.0008\n",
            "Iter: 7220  \tTraining Loss: -465.9619    \n",
            "    Negative Log Likelihood: 21.7757\tSigma2 Prior: -487.7383\tRegularization: 0.0008\n",
            "Iter: 7230  \tTraining Loss: -478.3715    \n",
            "    Negative Log Likelihood: 20.3045\tSigma2 Prior: -498.6768\tRegularization: 0.0008\n",
            "Iter: 7240  \tTraining Loss: -471.5606    \n",
            "    Negative Log Likelihood: 21.5158\tSigma2 Prior: -493.0772\tRegularization: 0.0008\n",
            "Iter: 7250  \tTraining Loss: -474.9836    \n",
            "    Negative Log Likelihood: 21.5644\tSigma2 Prior: -496.5488\tRegularization: 0.0008\n",
            "Iter: 7260  \tTraining Loss: -480.4693    \n",
            "    Negative Log Likelihood: 21.1632\tSigma2 Prior: -501.6333\tRegularization: 0.0008\n",
            "Iter: 7270  \tTraining Loss: -474.4137    \n",
            "    Negative Log Likelihood: 21.8894\tSigma2 Prior: -496.3039\tRegularization: 0.0008\n",
            "Iter: 7280  \tTraining Loss: -478.8453    \n",
            "    Negative Log Likelihood: 21.8525\tSigma2 Prior: -500.6987\tRegularization: 0.0008\n",
            "Iter: 7290  \tTraining Loss: -480.5016    \n",
            "    Negative Log Likelihood: 21.8719\tSigma2 Prior: -502.3744\tRegularization: 0.0008\n",
            "Iter: 7300  \tTraining Loss: -481.1303    \n",
            "    Negative Log Likelihood: 21.7821\tSigma2 Prior: -502.9133\tRegularization: 0.0008\n",
            "Iter: 7310  \tTraining Loss: -482.5280    \n",
            "    Negative Log Likelihood: 21.7155\tSigma2 Prior: -504.2443\tRegularization: 0.0008\n",
            "Iter: 7320  \tTraining Loss: -479.2830    \n",
            "    Negative Log Likelihood: 22.4428\tSigma2 Prior: -501.7266\tRegularization: 0.0008\n",
            "Iter: 7330  \tTraining Loss: -480.4285    \n",
            "    Negative Log Likelihood: 22.8684\tSigma2 Prior: -503.2978\tRegularization: 0.0008\n",
            "Iter: 7340  \tTraining Loss: -477.5865    \n",
            "    Negative Log Likelihood: 23.0553\tSigma2 Prior: -500.6426\tRegularization: 0.0008\n",
            "Iter: 7350  \tTraining Loss: -476.0153    \n",
            "    Negative Log Likelihood: 23.4353\tSigma2 Prior: -499.4515\tRegularization: 0.0008\n",
            "Iter: 7360  \tTraining Loss: -479.9324    \n",
            "    Negative Log Likelihood: 23.3728\tSigma2 Prior: -503.3061\tRegularization: 0.0008\n",
            "Iter: 7370  \tTraining Loss: -486.1029    \n",
            "    Negative Log Likelihood: 23.2323\tSigma2 Prior: -509.3361\tRegularization: 0.0008\n",
            "Iter: 7380  \tTraining Loss: -485.4034    \n",
            "    Negative Log Likelihood: 23.1250\tSigma2 Prior: -508.5293\tRegularization: 0.0008\n",
            "Iter: 7390  \tTraining Loss: -481.3664    \n",
            "    Negative Log Likelihood: 23.9660\tSigma2 Prior: -505.3332\tRegularization: 0.0008\n",
            "Iter: 7400  \tTraining Loss: -485.0957    \n",
            "    Negative Log Likelihood: 24.0625\tSigma2 Prior: -509.1590\tRegularization: 0.0008\n",
            "Iter: 7410  \tTraining Loss: -490.2633    \n",
            "    Negative Log Likelihood: 23.4557\tSigma2 Prior: -513.7198\tRegularization: 0.0008\n",
            "Iter: 7420  \tTraining Loss: -486.6025    \n",
            "    Negative Log Likelihood: 24.0514\tSigma2 Prior: -510.6547\tRegularization: 0.0008\n",
            "Iter: 7430  \tTraining Loss: -494.3760    \n",
            "    Negative Log Likelihood: 23.4651\tSigma2 Prior: -517.8419\tRegularization: 0.0008\n",
            "Iter: 7440  \tTraining Loss: -487.5594    \n",
            "    Negative Log Likelihood: 24.4130\tSigma2 Prior: -511.9732\tRegularization: 0.0008\n",
            "Iter: 7450  \tTraining Loss: -489.5192    \n",
            "    Negative Log Likelihood: 24.6823\tSigma2 Prior: -514.2023\tRegularization: 0.0008\n",
            "Iter: 7460  \tTraining Loss: -489.1803    \n",
            "    Negative Log Likelihood: 25.2748\tSigma2 Prior: -514.4559\tRegularization: 0.0008\n",
            "Iter: 7470  \tTraining Loss: -490.4802    \n",
            "    Negative Log Likelihood: 25.1030\tSigma2 Prior: -515.5840\tRegularization: 0.0008\n",
            "Iter: 7480  \tTraining Loss: -496.1768    \n",
            "    Negative Log Likelihood: 24.2877\tSigma2 Prior: -520.4653\tRegularization: 0.0008\n",
            "Iter: 7490  \tTraining Loss: -485.9264    \n",
            "    Negative Log Likelihood: 26.1930\tSigma2 Prior: -512.1202\tRegularization: 0.0008\n",
            "Iter: 7500  \tTraining Loss: -488.5247    \n",
            "    Negative Log Likelihood: 26.5649\tSigma2 Prior: -515.0904\tRegularization: 0.0008\n",
            "Iter: 7510  \tTraining Loss: -485.2748    \n",
            "    Negative Log Likelihood: 26.2154\tSigma2 Prior: -511.4910\tRegularization: 0.0008\n",
            "Iter: 7520  \tTraining Loss: -500.6029    \n",
            "    Negative Log Likelihood: 25.2719\tSigma2 Prior: -525.8756\tRegularization: 0.0008\n",
            "Iter: 7530  \tTraining Loss: -491.9801    \n",
            "    Negative Log Likelihood: 26.7071\tSigma2 Prior: -518.6880\tRegularization: 0.0008\n",
            "Iter: 7540  \tTraining Loss: -490.6839    \n",
            "    Negative Log Likelihood: 27.3434\tSigma2 Prior: -518.0281\tRegularization: 0.0008\n",
            "Iter: 7550  \tTraining Loss: -492.5674    \n",
            "    Negative Log Likelihood: 27.0568\tSigma2 Prior: -519.6250\tRegularization: 0.0008\n",
            "Iter: 7560  \tTraining Loss: -493.1431    \n",
            "    Negative Log Likelihood: 27.3498\tSigma2 Prior: -520.4938\tRegularization: 0.0008\n",
            "Iter: 7570  \tTraining Loss: -490.7626    \n",
            "    Negative Log Likelihood: 28.1328\tSigma2 Prior: -518.8962\tRegularization: 0.0008\n",
            "Iter: 7580  \tTraining Loss: -493.2024    \n",
            "    Negative Log Likelihood: 28.2114\tSigma2 Prior: -521.4146\tRegularization: 0.0008\n",
            "Iter: 7590  \tTraining Loss: -497.8982    \n",
            "    Negative Log Likelihood: 28.2930\tSigma2 Prior: -526.1920\tRegularization: 0.0008\n",
            "Iter: 7600  \tTraining Loss: -500.8910    \n",
            "    Negative Log Likelihood: 28.0195\tSigma2 Prior: -528.9113\tRegularization: 0.0008\n",
            "Iter: 7610  \tTraining Loss: -498.2512    \n",
            "    Negative Log Likelihood: 28.7940\tSigma2 Prior: -527.0460\tRegularization: 0.0008\n",
            "Iter: 7620  \tTraining Loss: -499.0174    \n",
            "    Negative Log Likelihood: 29.1791\tSigma2 Prior: -528.1973\tRegularization: 0.0008\n",
            "Iter: 7630  \tTraining Loss: -499.8327    \n",
            "    Negative Log Likelihood: 28.9708\tSigma2 Prior: -528.8043\tRegularization: 0.0008\n",
            "Iter: 7640  \tTraining Loss: -488.0538    \n",
            "    Negative Log Likelihood: 31.1138\tSigma2 Prior: -519.1685\tRegularization: 0.0008\n",
            "Iter: 7650  \tTraining Loss: -504.0057    \n",
            "    Negative Log Likelihood: 28.9578\tSigma2 Prior: -532.9644\tRegularization: 0.0008\n",
            "Iter: 7660  \tTraining Loss: -499.4175    \n",
            "    Negative Log Likelihood: 30.6986\tSigma2 Prior: -530.1169\tRegularization: 0.0008\n",
            "Iter: 7670  \tTraining Loss: -487.5408    \n",
            "    Negative Log Likelihood: 31.0741\tSigma2 Prior: -518.6157\tRegularization: 0.0008\n",
            "Iter: 7680  \tTraining Loss: -508.8037    \n",
            "    Negative Log Likelihood: 29.7932\tSigma2 Prior: -538.5977\tRegularization: 0.0008\n",
            "Iter: 7690  \tTraining Loss: -501.6211    \n",
            "    Negative Log Likelihood: 30.8083\tSigma2 Prior: -532.4302\tRegularization: 0.0008\n",
            "Iter: 7700  \tTraining Loss: -502.4665    \n",
            "    Negative Log Likelihood: 31.4918\tSigma2 Prior: -533.9592\tRegularization: 0.0008\n",
            "Iter: 7710  \tTraining Loss: -506.7112    \n",
            "    Negative Log Likelihood: 31.2030\tSigma2 Prior: -537.9151\tRegularization: 0.0008\n",
            "Iter: 7720  \tTraining Loss: -512.8934    \n",
            "    Negative Log Likelihood: 30.9679\tSigma2 Prior: -543.8621\tRegularization: 0.0008\n",
            "Iter: 7730  \tTraining Loss: -509.0684    \n",
            "    Negative Log Likelihood: 32.1885\tSigma2 Prior: -541.2578\tRegularization: 0.0008\n",
            "Iter: 7740  \tTraining Loss: -518.4615    \n",
            "    Negative Log Likelihood: 31.0434\tSigma2 Prior: -549.5058\tRegularization: 0.0008\n",
            "Iter: 7750  \tTraining Loss: -515.7570    \n",
            "    Negative Log Likelihood: 31.8571\tSigma2 Prior: -547.6149\tRegularization: 0.0008\n",
            "Iter: 7760  \tTraining Loss: -510.0881    \n",
            "    Negative Log Likelihood: 33.2136\tSigma2 Prior: -543.3025\tRegularization: 0.0008\n",
            "Iter: 7770  \tTraining Loss: -504.9599    \n",
            "    Negative Log Likelihood: 34.3069\tSigma2 Prior: -539.2676\tRegularization: 0.0008\n",
            "Iter: 7780  \tTraining Loss: -514.5464    \n",
            "    Negative Log Likelihood: 32.5929\tSigma2 Prior: -547.1401\tRegularization: 0.0008\n",
            "Iter: 7790  \tTraining Loss: -504.4886    \n",
            "    Negative Log Likelihood: 35.3231\tSigma2 Prior: -539.8126\tRegularization: 0.0008\n",
            "Iter: 7800  \tTraining Loss: -516.9836    \n",
            "    Negative Log Likelihood: 34.1639\tSigma2 Prior: -551.1484\tRegularization: 0.0008\n",
            "Iter: 7810  \tTraining Loss: -502.2196    \n",
            "    Negative Log Likelihood: 36.1593\tSigma2 Prior: -538.3797\tRegularization: 0.0008\n",
            "Iter: 7820  \tTraining Loss: -502.3792    \n",
            "    Negative Log Likelihood: 36.1887\tSigma2 Prior: -538.5687\tRegularization: 0.0008\n",
            "Iter: 7830  \tTraining Loss: -521.2157    \n",
            "    Negative Log Likelihood: 34.4253\tSigma2 Prior: -555.6418\tRegularization: 0.0008\n",
            "Iter: 7840  \tTraining Loss: -516.8126    \n",
            "    Negative Log Likelihood: 35.8439\tSigma2 Prior: -552.6574\tRegularization: 0.0008\n",
            "Iter: 7850  \tTraining Loss: -520.9692    \n",
            "    Negative Log Likelihood: 35.4968\tSigma2 Prior: -556.4669\tRegularization: 0.0008\n",
            "Iter: 7860  \tTraining Loss: -515.1060    \n",
            "    Negative Log Likelihood: 37.0450\tSigma2 Prior: -552.1518\tRegularization: 0.0008\n",
            "Iter: 7870  \tTraining Loss: -500.9082    \n",
            "    Negative Log Likelihood: 39.2562\tSigma2 Prior: -540.1652\tRegularization: 0.0008\n",
            "Iter: 7880  \tTraining Loss: -510.6125    \n",
            "    Negative Log Likelihood: 38.2553\tSigma2 Prior: -548.8687\tRegularization: 0.0008\n",
            "Iter: 7890  \tTraining Loss: -513.1735    \n",
            "    Negative Log Likelihood: 39.0368\tSigma2 Prior: -552.2112\tRegularization: 0.0008\n",
            "Iter: 7900  \tTraining Loss: -529.0123    \n",
            "    Negative Log Likelihood: 37.6029\tSigma2 Prior: -566.6161\tRegularization: 0.0008\n",
            "Iter: 7910  \tTraining Loss: -528.2476    \n",
            "    Negative Log Likelihood: 37.6876\tSigma2 Prior: -565.9360\tRegularization: 0.0008\n",
            "Iter: 7920  \tTraining Loss: -512.7294    \n",
            "    Negative Log Likelihood: 40.7715\tSigma2 Prior: -553.5017\tRegularization: 0.0008\n",
            "Iter: 7930  \tTraining Loss: -517.4930    \n",
            "    Negative Log Likelihood: 40.9536\tSigma2 Prior: -558.4474\tRegularization: 0.0008\n",
            "Iter: 7940  \tTraining Loss: -516.7349    \n",
            "    Negative Log Likelihood: 40.5813\tSigma2 Prior: -557.3171\tRegularization: 0.0008\n",
            "Iter: 7950  \tTraining Loss: -509.0350    \n",
            "    Negative Log Likelihood: 42.1961\tSigma2 Prior: -551.2319\tRegularization: 0.0008\n",
            "Iter: 7960  \tTraining Loss: -531.2531    \n",
            "    Negative Log Likelihood: 39.8258\tSigma2 Prior: -571.0798\tRegularization: 0.0008\n",
            "Iter: 7970  \tTraining Loss: -529.0626    \n",
            "    Negative Log Likelihood: 41.3754\tSigma2 Prior: -570.4388\tRegularization: 0.0008\n",
            "Iter: 7980  \tTraining Loss: -515.7064    \n",
            "    Negative Log Likelihood: 43.3330\tSigma2 Prior: -559.0402\tRegularization: 0.0008\n",
            "Iter: 7990  \tTraining Loss: -526.1810    \n",
            "    Negative Log Likelihood: 42.4414\tSigma2 Prior: -568.6232\tRegularization: 0.0008\n",
            "Iter: 8000  \tTraining Loss: -527.4035    \n",
            "    Negative Log Likelihood: 42.3536\tSigma2 Prior: -569.7579\tRegularization: 0.0008\n",
            "Iter: 8010  \tTraining Loss: -523.4365    \n",
            "    Negative Log Likelihood: 44.1629\tSigma2 Prior: -567.6003\tRegularization: 0.0008\n",
            "Iter: 8020  \tTraining Loss: -519.9648    \n",
            "    Negative Log Likelihood: 45.4005\tSigma2 Prior: -565.3662\tRegularization: 0.0008\n",
            "Iter: 8030  \tTraining Loss: -522.9700    \n",
            "    Negative Log Likelihood: 46.8583\tSigma2 Prior: -569.8292\tRegularization: 0.0008\n",
            "Iter: 8040  \tTraining Loss: -518.5062    \n",
            "    Negative Log Likelihood: 45.7228\tSigma2 Prior: -564.2299\tRegularization: 0.0008\n",
            "Iter: 8050  \tTraining Loss: -526.8828    \n",
            "    Negative Log Likelihood: 46.7172\tSigma2 Prior: -573.6009\tRegularization: 0.0008\n",
            "Iter: 8060  \tTraining Loss: -519.0447    \n",
            "    Negative Log Likelihood: 48.0780\tSigma2 Prior: -567.1236\tRegularization: 0.0008\n",
            "Iter: 8070  \tTraining Loss: -533.7771    \n",
            "    Negative Log Likelihood: 46.1942\tSigma2 Prior: -579.9722\tRegularization: 0.0008\n",
            "Iter: 8080  \tTraining Loss: -511.2484    \n",
            "    Negative Log Likelihood: 49.7350\tSigma2 Prior: -560.9842\tRegularization: 0.0008\n",
            "Iter: 8090  \tTraining Loss: -538.2914    \n",
            "    Negative Log Likelihood: 47.2862\tSigma2 Prior: -585.5784\tRegularization: 0.0008\n",
            "Iter: 8100  \tTraining Loss: -533.6226    \n",
            "    Negative Log Likelihood: 47.9194\tSigma2 Prior: -581.5428\tRegularization: 0.0008\n",
            "Iter: 8110  \tTraining Loss: -541.4821    \n",
            "    Negative Log Likelihood: 47.8145\tSigma2 Prior: -589.2974\tRegularization: 0.0008\n",
            "Iter: 8120  \tTraining Loss: -533.3773    \n",
            "    Negative Log Likelihood: 49.6499\tSigma2 Prior: -583.0281\tRegularization: 0.0008\n",
            "Iter: 8130  \tTraining Loss: -525.9591    \n",
            "    Negative Log Likelihood: 50.9629\tSigma2 Prior: -576.9229\tRegularization: 0.0008\n",
            "Iter: 8140  \tTraining Loss: -519.6883    \n",
            "    Negative Log Likelihood: 53.2142\tSigma2 Prior: -572.9033\tRegularization: 0.0008\n",
            "Iter: 8150  \tTraining Loss: -508.4247    \n",
            "    Negative Log Likelihood: 54.0385\tSigma2 Prior: -562.4641\tRegularization: 0.0008\n",
            "Iter: 8160  \tTraining Loss: -524.7924    \n",
            "    Negative Log Likelihood: 52.5549\tSigma2 Prior: -577.3481\tRegularization: 0.0008\n",
            "Iter: 8170  \tTraining Loss: -539.2692    \n",
            "    Negative Log Likelihood: 52.1323\tSigma2 Prior: -591.4023\tRegularization: 0.0008\n",
            "Iter: 8180  \tTraining Loss: -531.8338    \n",
            "    Negative Log Likelihood: 52.7164\tSigma2 Prior: -584.5511\tRegularization: 0.0008\n",
            "Iter: 8190  \tTraining Loss: -530.5826    \n",
            "    Negative Log Likelihood: 53.9884\tSigma2 Prior: -584.5718\tRegularization: 0.0008\n",
            "Iter: 8200  \tTraining Loss: -519.3946    \n",
            "    Negative Log Likelihood: 54.1670\tSigma2 Prior: -573.5624\tRegularization: 0.0008\n",
            "Iter: 8210  \tTraining Loss: -530.8276    \n",
            "    Negative Log Likelihood: 54.5374\tSigma2 Prior: -585.3659\tRegularization: 0.0008\n",
            "Iter: 8220  \tTraining Loss: -534.7195    \n",
            "    Negative Log Likelihood: 54.0397\tSigma2 Prior: -588.7601\tRegularization: 0.0008\n",
            "Iter: 8230  \tTraining Loss: -518.5116    \n",
            "    Negative Log Likelihood: 58.0547\tSigma2 Prior: -576.5671\tRegularization: 0.0008\n",
            "Iter: 8240  \tTraining Loss: -529.7322    \n",
            "    Negative Log Likelihood: 55.9345\tSigma2 Prior: -585.6675\tRegularization: 0.0008\n",
            "Iter: 8250  \tTraining Loss: -531.6122    \n",
            "    Negative Log Likelihood: 55.9772\tSigma2 Prior: -587.5903\tRegularization: 0.0008\n",
            "Iter: 8260  \tTraining Loss: -527.2173    \n",
            "    Negative Log Likelihood: 56.9189\tSigma2 Prior: -584.1370\tRegularization: 0.0008\n",
            "Iter: 8270  \tTraining Loss: -546.6864    \n",
            "    Negative Log Likelihood: 56.0246\tSigma2 Prior: -602.7119\tRegularization: 0.0008\n",
            "Iter: 8280  \tTraining Loss: -520.4838    \n",
            "    Negative Log Likelihood: 59.4435\tSigma2 Prior: -579.9282\tRegularization: 0.0008\n",
            "Iter: 8290  \tTraining Loss: -542.5895    \n",
            "    Negative Log Likelihood: 56.7670\tSigma2 Prior: -599.3574\tRegularization: 0.0008\n",
            "Iter: 8300  \tTraining Loss: -545.0811    \n",
            "    Negative Log Likelihood: 55.8265\tSigma2 Prior: -600.9084\tRegularization: 0.0008\n",
            "Iter: 8310  \tTraining Loss: -520.7667    \n",
            "    Negative Log Likelihood: 60.2558\tSigma2 Prior: -581.0233\tRegularization: 0.0008\n",
            "Iter: 8320  \tTraining Loss: -531.2263    \n",
            "    Negative Log Likelihood: 59.3493\tSigma2 Prior: -590.5765\tRegularization: 0.0008\n",
            "Iter: 8330  \tTraining Loss: -541.6078    \n",
            "    Negative Log Likelihood: 57.7974\tSigma2 Prior: -599.4061\tRegularization: 0.0008\n",
            "Iter: 8340  \tTraining Loss: -532.7677    \n",
            "    Negative Log Likelihood: 59.9733\tSigma2 Prior: -592.7419\tRegularization: 0.0008\n",
            "Iter: 8350  \tTraining Loss: -541.4009    \n",
            "    Negative Log Likelihood: 60.0650\tSigma2 Prior: -601.4668\tRegularization: 0.0008\n",
            "Iter: 8360  \tTraining Loss: -535.8130    \n",
            "    Negative Log Likelihood: 58.9058\tSigma2 Prior: -594.7197\tRegularization: 0.0008\n",
            "Iter: 8370  \tTraining Loss: -537.2331    \n",
            "    Negative Log Likelihood: 60.3048\tSigma2 Prior: -597.5387\tRegularization: 0.0008\n",
            "Iter: 8380  \tTraining Loss: -524.3537    \n",
            "    Negative Log Likelihood: 61.3820\tSigma2 Prior: -585.7365\tRegularization: 0.0008\n",
            "Iter: 8390  \tTraining Loss: -539.1688    \n",
            "    Negative Log Likelihood: 59.9600\tSigma2 Prior: -599.1297\tRegularization: 0.0008\n",
            "Iter: 8400  \tTraining Loss: -530.6634    \n",
            "    Negative Log Likelihood: 60.7933\tSigma2 Prior: -591.4575\tRegularization: 0.0008\n",
            "Iter: 8410  \tTraining Loss: -519.6721    \n",
            "    Negative Log Likelihood: 63.2330\tSigma2 Prior: -582.9060\tRegularization: 0.0008\n",
            "Iter: 8420  \tTraining Loss: -536.8258    \n",
            "    Negative Log Likelihood: 61.5625\tSigma2 Prior: -598.3891\tRegularization: 0.0008\n",
            "Iter: 8430  \tTraining Loss: -549.7315    \n",
            "    Negative Log Likelihood: 60.1599\tSigma2 Prior: -609.8923\tRegularization: 0.0008\n",
            "Iter: 8440  \tTraining Loss: -539.7318    \n",
            "    Negative Log Likelihood: 62.0168\tSigma2 Prior: -601.7495\tRegularization: 0.0008\n",
            "Iter: 8450  \tTraining Loss: -540.9103    \n",
            "    Negative Log Likelihood: 60.1762\tSigma2 Prior: -601.0873\tRegularization: 0.0008\n",
            "Iter: 8460  \tTraining Loss: -530.8264    \n",
            "    Negative Log Likelihood: 63.4672\tSigma2 Prior: -594.2944\tRegularization: 0.0009\n",
            "Iter: 8470  \tTraining Loss: -546.7706    \n",
            "    Negative Log Likelihood: 60.7124\tSigma2 Prior: -607.4839\tRegularization: 0.0009\n",
            "Iter: 8480  \tTraining Loss: -514.1112    \n",
            "    Negative Log Likelihood: 64.0127\tSigma2 Prior: -578.1248\tRegularization: 0.0009\n",
            "Iter: 8490  \tTraining Loss: -554.2316    \n",
            "    Negative Log Likelihood: 58.9720\tSigma2 Prior: -613.2045\tRegularization: 0.0009\n",
            "Iter: 8500  \tTraining Loss: -521.0843    \n",
            "    Negative Log Likelihood: 64.2035\tSigma2 Prior: -585.2887\tRegularization: 0.0009\n",
            "Iter: 8510  \tTraining Loss: -546.1613    \n",
            "    Negative Log Likelihood: 60.8762\tSigma2 Prior: -607.0383\tRegularization: 0.0009\n",
            "Iter: 8520  \tTraining Loss: -535.2225    \n",
            "    Negative Log Likelihood: 61.2805\tSigma2 Prior: -596.5038\tRegularization: 0.0009\n",
            "Iter: 8530  \tTraining Loss: -518.6991    \n",
            "    Negative Log Likelihood: 64.3167\tSigma2 Prior: -583.0167\tRegularization: 0.0009\n",
            "Iter: 8540  \tTraining Loss: -532.1337    \n",
            "    Negative Log Likelihood: 63.1043\tSigma2 Prior: -595.2388\tRegularization: 0.0009\n",
            "Iter: 8550  \tTraining Loss: -532.6230    \n",
            "    Negative Log Likelihood: 63.2471\tSigma2 Prior: -595.8710\tRegularization: 0.0009\n",
            "Iter: 8560  \tTraining Loss: -527.3519    \n",
            "    Negative Log Likelihood: 64.4271\tSigma2 Prior: -591.7798\tRegularization: 0.0009\n",
            "Iter: 8570  \tTraining Loss: -522.9959    \n",
            "    Negative Log Likelihood: 63.5163\tSigma2 Prior: -586.5131\tRegularization: 0.0009\n",
            "Iter: 8580  \tTraining Loss: -543.4218    \n",
            "    Negative Log Likelihood: 62.1482\tSigma2 Prior: -605.5709\tRegularization: 0.0009\n",
            "Iter: 8590  \tTraining Loss: -524.2024    \n",
            "    Negative Log Likelihood: 62.6044\tSigma2 Prior: -586.8076\tRegularization: 0.0009\n",
            "Iter: 8600  \tTraining Loss: -530.8296    \n",
            "    Negative Log Likelihood: 62.9937\tSigma2 Prior: -593.8242\tRegularization: 0.0009\n",
            "Iter: 8610  \tTraining Loss: -515.9440    \n",
            "    Negative Log Likelihood: 66.0571\tSigma2 Prior: -582.0020\tRegularization: 0.0009\n",
            "Iter: 8620  \tTraining Loss: -540.7661    \n",
            "    Negative Log Likelihood: 61.3663\tSigma2 Prior: -602.1332\tRegularization: 0.0009\n",
            "Iter: 8630  \tTraining Loss: -516.3842    \n",
            "    Negative Log Likelihood: 65.5596\tSigma2 Prior: -581.9447\tRegularization: 0.0009\n",
            "Iter: 8640  \tTraining Loss: -523.6813    \n",
            "    Negative Log Likelihood: 64.4746\tSigma2 Prior: -588.1568\tRegularization: 0.0009\n",
            "Iter: 8650  \tTraining Loss: -537.5620    \n",
            "    Negative Log Likelihood: 61.9058\tSigma2 Prior: -599.4686\tRegularization: 0.0009\n",
            "Iter: 8660  \tTraining Loss: -538.2956    \n",
            "    Negative Log Likelihood: 62.0333\tSigma2 Prior: -600.3297\tRegularization: 0.0009\n",
            "Iter: 8670  \tTraining Loss: -521.0984    \n",
            "    Negative Log Likelihood: 64.3139\tSigma2 Prior: -585.4132\tRegularization: 0.0009\n",
            "Iter: 8680  \tTraining Loss: -544.4229    \n",
            "    Negative Log Likelihood: 61.7780\tSigma2 Prior: -606.2018\tRegularization: 0.0009\n",
            "Iter: 8690  \tTraining Loss: -534.3739    \n",
            "    Negative Log Likelihood: 61.2349\tSigma2 Prior: -595.6097\tRegularization: 0.0009\n",
            "Iter: 8700  \tTraining Loss: -519.3795    \n",
            "    Negative Log Likelihood: 65.9573\tSigma2 Prior: -585.3376\tRegularization: 0.0009\n",
            "Iter: 8710  \tTraining Loss: -516.8496    \n",
            "    Negative Log Likelihood: 64.9811\tSigma2 Prior: -581.8315\tRegularization: 0.0009\n",
            "Iter: 8720  \tTraining Loss: -532.5052    \n",
            "    Negative Log Likelihood: 63.8168\tSigma2 Prior: -596.3228\tRegularization: 0.0009\n",
            "Iter: 8730  \tTraining Loss: -532.2579    \n",
            "    Negative Log Likelihood: 63.3292\tSigma2 Prior: -595.5880\tRegularization: 0.0009\n",
            "Iter: 8740  \tTraining Loss: -546.8906    \n",
            "    Negative Log Likelihood: 61.1985\tSigma2 Prior: -608.0900\tRegularization: 0.0009\n",
            "Iter: 8750  \tTraining Loss: -543.9905    \n",
            "    Negative Log Likelihood: 62.4925\tSigma2 Prior: -606.4839\tRegularization: 0.0009\n",
            "Iter: 8760  \tTraining Loss: -511.6199    \n",
            "    Negative Log Likelihood: 64.4869\tSigma2 Prior: -576.1077\tRegularization: 0.0009\n",
            "Iter: 8770  \tTraining Loss: -538.2445    \n",
            "    Negative Log Likelihood: 60.9540\tSigma2 Prior: -599.1993\tRegularization: 0.0009\n",
            "Iter: 8780  \tTraining Loss: -531.9109    \n",
            "    Negative Log Likelihood: 64.3043\tSigma2 Prior: -596.2161\tRegularization: 0.0009\n",
            "Iter: 8790  \tTraining Loss: -533.1946    \n",
            "    Negative Log Likelihood: 63.3933\tSigma2 Prior: -596.5887\tRegularization: 0.0009\n",
            "Iter: 8800  \tTraining Loss: -526.1684    \n",
            "    Negative Log Likelihood: 63.2655\tSigma2 Prior: -589.4348\tRegularization: 0.0009\n",
            "Iter: 8810  \tTraining Loss: -540.4058    \n",
            "    Negative Log Likelihood: 63.0303\tSigma2 Prior: -603.4369\tRegularization: 0.0009\n",
            "Iter: 8820  \tTraining Loss: -530.7663    \n",
            "    Negative Log Likelihood: 63.6204\tSigma2 Prior: -594.3875\tRegularization: 0.0009\n",
            "Iter: 8830  \tTraining Loss: -516.6367    \n",
            "    Negative Log Likelihood: 65.4269\tSigma2 Prior: -582.0644\tRegularization: 0.0009\n",
            "Iter: 8840  \tTraining Loss: -522.4147    \n",
            "    Negative Log Likelihood: 64.9658\tSigma2 Prior: -587.3814\tRegularization: 0.0009\n",
            "Iter: 8850  \tTraining Loss: -540.5479    \n",
            "    Negative Log Likelihood: 62.0937\tSigma2 Prior: -602.6423\tRegularization: 0.0009\n",
            "Iter: 8860  \tTraining Loss: -520.9375    \n",
            "    Negative Log Likelihood: 64.8712\tSigma2 Prior: -585.8096\tRegularization: 0.0009\n",
            "Iter: 8870  \tTraining Loss: -532.4574    \n",
            "    Negative Log Likelihood: 63.1817\tSigma2 Prior: -595.6400\tRegularization: 0.0009\n",
            "Iter: 8880  \tTraining Loss: -502.7388    \n",
            "    Negative Log Likelihood: 65.2366\tSigma2 Prior: -567.9763\tRegularization: 0.0009\n",
            "Iter: 8890  \tTraining Loss: -544.5162    \n",
            "    Negative Log Likelihood: 62.5113\tSigma2 Prior: -607.0283\tRegularization: 0.0009\n",
            "Iter: 8900  \tTraining Loss: -539.3223    \n",
            "    Negative Log Likelihood: 61.9650\tSigma2 Prior: -601.2882\tRegularization: 0.0009\n",
            "Iter: 8910  \tTraining Loss: -526.4785    \n",
            "    Negative Log Likelihood: 63.4988\tSigma2 Prior: -589.9782\tRegularization: 0.0009\n",
            "Iter: 8920  \tTraining Loss: -538.5811    \n",
            "    Negative Log Likelihood: 62.4512\tSigma2 Prior: -601.0332\tRegularization: 0.0009\n",
            "Iter: 8930  \tTraining Loss: -520.8287    \n",
            "    Negative Log Likelihood: 63.5656\tSigma2 Prior: -584.3951\tRegularization: 0.0009\n",
            "Iter: 8940  \tTraining Loss: -508.5564    \n",
            "    Negative Log Likelihood: 65.2889\tSigma2 Prior: -573.8462\tRegularization: 0.0009\n",
            "Iter: 8950  \tTraining Loss: -539.5286    \n",
            "    Negative Log Likelihood: 62.9001\tSigma2 Prior: -602.4295\tRegularization: 0.0009\n",
            "Iter: 8960  \tTraining Loss: -537.5792    \n",
            "    Negative Log Likelihood: 63.8549\tSigma2 Prior: -601.4350\tRegularization: 0.0009\n",
            "Iter: 8970  \tTraining Loss: -531.7887    \n",
            "    Negative Log Likelihood: 64.2251\tSigma2 Prior: -596.0146\tRegularization: 0.0009\n",
            "Iter: 8980  \tTraining Loss: -533.3074    \n",
            "    Negative Log Likelihood: 62.2140\tSigma2 Prior: -595.5223\tRegularization: 0.0009\n",
            "Iter: 8990  \tTraining Loss: -542.3773    \n",
            "    Negative Log Likelihood: 63.2495\tSigma2 Prior: -605.6276\tRegularization: 0.0009\n",
            "Iter: 9000  \tTraining Loss: -518.5292    \n",
            "    Negative Log Likelihood: 66.2119\tSigma2 Prior: -584.7419\tRegularization: 0.0009\n",
            "Iter: 9010  \tTraining Loss: -513.1844    \n",
            "    Negative Log Likelihood: 65.8461\tSigma2 Prior: -579.0314\tRegularization: 0.0009\n",
            "Iter: 9020  \tTraining Loss: -533.4290    \n",
            "    Negative Log Likelihood: 64.7789\tSigma2 Prior: -598.2087\tRegularization: 0.0009\n",
            "Iter: 9030  \tTraining Loss: -534.0610    \n",
            "    Negative Log Likelihood: 64.5783\tSigma2 Prior: -598.6401\tRegularization: 0.0009\n",
            "Iter: 9040  \tTraining Loss: -543.4796    \n",
            "    Negative Log Likelihood: 61.8503\tSigma2 Prior: -605.3307\tRegularization: 0.0009\n",
            "Iter: 9050  \tTraining Loss: -519.7435    \n",
            "    Negative Log Likelihood: 64.2203\tSigma2 Prior: -583.9647\tRegularization: 0.0009\n",
            "Iter: 9060  \tTraining Loss: -527.3745    \n",
            "    Negative Log Likelihood: 65.2468\tSigma2 Prior: -592.6221\tRegularization: 0.0009\n",
            "Iter: 9070  \tTraining Loss: -550.6480    \n",
            "    Negative Log Likelihood: 61.2585\tSigma2 Prior: -611.9073\tRegularization: 0.0009\n",
            "Iter: 9080  \tTraining Loss: -541.6580    \n",
            "    Negative Log Likelihood: 62.3652\tSigma2 Prior: -604.0240\tRegularization: 0.0009\n",
            "Iter: 9090  \tTraining Loss: -532.6019    \n",
            "    Negative Log Likelihood: 64.1097\tSigma2 Prior: -596.7125\tRegularization: 0.0009\n",
            "Iter: 9100  \tTraining Loss: -531.8314    \n",
            "    Negative Log Likelihood: 62.3417\tSigma2 Prior: -594.1740\tRegularization: 0.0009\n",
            "Iter: 9110  \tTraining Loss: -516.1915    \n",
            "    Negative Log Likelihood: 64.5953\tSigma2 Prior: -580.7877\tRegularization: 0.0009\n",
            "Iter: 9120  \tTraining Loss: -548.9700    \n",
            "    Negative Log Likelihood: 59.5289\tSigma2 Prior: -608.4998\tRegularization: 0.0009\n",
            "Iter: 9130  \tTraining Loss: -519.0447    \n",
            "    Negative Log Likelihood: 64.9462\tSigma2 Prior: -583.9917\tRegularization: 0.0009\n",
            "Iter: 9140  \tTraining Loss: -524.1202    \n",
            "    Negative Log Likelihood: 64.4188\tSigma2 Prior: -588.5399\tRegularization: 0.0009\n",
            "Iter: 9150  \tTraining Loss: -538.3361    \n",
            "    Negative Log Likelihood: 61.4351\tSigma2 Prior: -599.7720\tRegularization: 0.0009\n",
            "Iter: 9160  \tTraining Loss: -525.8583    \n",
            "    Negative Log Likelihood: 64.5004\tSigma2 Prior: -590.3596\tRegularization: 0.0009\n",
            "Iter: 9170  \tTraining Loss: -537.6762    \n",
            "    Negative Log Likelihood: 62.9355\tSigma2 Prior: -600.6125\tRegularization: 0.0009\n",
            "Iter: 9180  \tTraining Loss: -542.3231    \n",
            "    Negative Log Likelihood: 61.1171\tSigma2 Prior: -603.4411\tRegularization: 0.0009\n",
            "Iter: 9190  \tTraining Loss: -509.8780    \n",
            "    Negative Log Likelihood: 65.5271\tSigma2 Prior: -575.4060\tRegularization: 0.0009\n",
            "Iter: 9200  \tTraining Loss: -551.8118    \n",
            "    Negative Log Likelihood: 61.7963\tSigma2 Prior: -613.6089\tRegularization: 0.0009\n",
            "Iter: 9210  \tTraining Loss: -528.7496    \n",
            "    Negative Log Likelihood: 63.3852\tSigma2 Prior: -592.1356\tRegularization: 0.0009\n",
            "Iter: 9220  \tTraining Loss: -529.0600    \n",
            "    Negative Log Likelihood: 64.5834\tSigma2 Prior: -593.6443\tRegularization: 0.0009\n",
            "Iter: 9230  \tTraining Loss: -536.9886    \n",
            "    Negative Log Likelihood: 63.3888\tSigma2 Prior: -600.3783\tRegularization: 0.0009\n",
            "Iter: 9240  \tTraining Loss: -546.0358    \n",
            "    Negative Log Likelihood: 61.6718\tSigma2 Prior: -607.7085\tRegularization: 0.0009\n",
            "Iter: 9250  \tTraining Loss: -518.1082    \n",
            "    Negative Log Likelihood: 64.6307\tSigma2 Prior: -582.7397\tRegularization: 0.0009\n",
            "Iter: 9260  \tTraining Loss: -532.8577    \n",
            "    Negative Log Likelihood: 64.5732\tSigma2 Prior: -597.4318\tRegularization: 0.0009\n",
            "Iter: 9270  \tTraining Loss: -539.0520    \n",
            "    Negative Log Likelihood: 62.5195\tSigma2 Prior: -601.5723\tRegularization: 0.0009\n",
            "Iter: 9280  \tTraining Loss: -534.6547    \n",
            "    Negative Log Likelihood: 62.4010\tSigma2 Prior: -597.0566\tRegularization: 0.0009\n",
            "Iter: 9290  \tTraining Loss: -532.8454    \n",
            "    Negative Log Likelihood: 62.5737\tSigma2 Prior: -595.4200\tRegularization: 0.0009\n",
            "Iter: 9300  \tTraining Loss: -527.9765    \n",
            "    Negative Log Likelihood: 63.7101\tSigma2 Prior: -591.6874\tRegularization: 0.0009\n",
            "Iter: 9310  \tTraining Loss: -548.4000    \n",
            "    Negative Log Likelihood: 61.0685\tSigma2 Prior: -609.4693\tRegularization: 0.0009\n",
            "Iter: 9320  \tTraining Loss: -537.4217    \n",
            "    Negative Log Likelihood: 62.0190\tSigma2 Prior: -599.4415\tRegularization: 0.0009\n",
            "Iter: 9330  \tTraining Loss: -550.4129    \n",
            "    Negative Log Likelihood: 61.6670\tSigma2 Prior: -612.0807\tRegularization: 0.0009\n",
            "Iter: 9340  \tTraining Loss: -543.4089    \n",
            "    Negative Log Likelihood: 62.3045\tSigma2 Prior: -605.7143\tRegularization: 0.0009\n",
            "Iter: 9350  \tTraining Loss: -552.7562    \n",
            "    Negative Log Likelihood: 59.8324\tSigma2 Prior: -612.5895\tRegularization: 0.0009\n",
            "Iter: 9360  \tTraining Loss: -522.5807    \n",
            "    Negative Log Likelihood: 64.5507\tSigma2 Prior: -587.1323\tRegularization: 0.0009\n",
            "Iter: 9370  \tTraining Loss: -555.0657    \n",
            "    Negative Log Likelihood: 60.7335\tSigma2 Prior: -615.8000\tRegularization: 0.0009\n",
            "Iter: 9380  \tTraining Loss: -546.0792    \n",
            "    Negative Log Likelihood: 62.3461\tSigma2 Prior: -608.4261\tRegularization: 0.0009\n",
            "Iter: 9390  \tTraining Loss: -508.3143    \n",
            "    Negative Log Likelihood: 65.6335\tSigma2 Prior: -573.9487\tRegularization: 0.0009\n",
            "Iter: 9400  \tTraining Loss: -545.7715    \n",
            "    Negative Log Likelihood: 60.7542\tSigma2 Prior: -606.5266\tRegularization: 0.0009\n",
            "Iter: 9410  \tTraining Loss: -547.9234    \n",
            "    Negative Log Likelihood: 61.0151\tSigma2 Prior: -608.9393\tRegularization: 0.0009\n",
            "Iter: 9420  \tTraining Loss: -537.5054    \n",
            "    Negative Log Likelihood: 62.7930\tSigma2 Prior: -600.2993\tRegularization: 0.0009\n",
            "Iter: 9430  \tTraining Loss: -535.1477    \n",
            "    Negative Log Likelihood: 63.5760\tSigma2 Prior: -598.7245\tRegularization: 0.0009\n",
            "Iter: 9440  \tTraining Loss: -544.0190    \n",
            "    Negative Log Likelihood: 62.3258\tSigma2 Prior: -606.3458\tRegularization: 0.0009\n",
            "Iter: 9450  \tTraining Loss: -534.5753    \n",
            "    Negative Log Likelihood: 62.5951\tSigma2 Prior: -597.1713\tRegularization: 0.0009\n",
            "Iter: 9460  \tTraining Loss: -548.7261    \n",
            "    Negative Log Likelihood: 60.0189\tSigma2 Prior: -608.7458\tRegularization: 0.0009\n",
            "Iter: 9470  \tTraining Loss: -532.4951    \n",
            "    Negative Log Likelihood: 63.1028\tSigma2 Prior: -595.5988\tRegularization: 0.0009\n",
            "Iter: 9480  \tTraining Loss: -544.8370    \n",
            "    Negative Log Likelihood: 62.0624\tSigma2 Prior: -606.9003\tRegularization: 0.0009\n",
            "Iter: 9490  \tTraining Loss: -537.5076    \n",
            "    Negative Log Likelihood: 61.2769\tSigma2 Prior: -598.7854\tRegularization: 0.0009\n",
            "Iter: 9500  \tTraining Loss: -534.0512    \n",
            "    Negative Log Likelihood: 62.6988\tSigma2 Prior: -596.7509\tRegularization: 0.0009\n",
            "Iter: 9510  \tTraining Loss: -537.3692    \n",
            "    Negative Log Likelihood: 62.2812\tSigma2 Prior: -599.6512\tRegularization: 0.0009\n",
            "Iter: 9520  \tTraining Loss: -543.9724    \n",
            "    Negative Log Likelihood: 60.0894\tSigma2 Prior: -604.0626\tRegularization: 0.0009\n",
            "Iter: 9530  \tTraining Loss: -532.5066    \n",
            "    Negative Log Likelihood: 61.9198\tSigma2 Prior: -594.4272\tRegularization: 0.0009\n",
            "Iter: 9540  \tTraining Loss: -517.2080    \n",
            "    Negative Log Likelihood: 64.1747\tSigma2 Prior: -581.3836\tRegularization: 0.0009\n",
            "Iter: 9550  \tTraining Loss: -517.0007    \n",
            "    Negative Log Likelihood: 64.7453\tSigma2 Prior: -581.7469\tRegularization: 0.0009\n",
            "Iter: 9560  \tTraining Loss: -549.4291    \n",
            "    Negative Log Likelihood: 60.6867\tSigma2 Prior: -610.1166\tRegularization: 0.0009\n",
            "Iter: 9570  \tTraining Loss: -530.3197    \n",
            "    Negative Log Likelihood: 62.3902\tSigma2 Prior: -592.7108\tRegularization: 0.0009\n",
            "Iter: 9580  \tTraining Loss: -545.0527    \n",
            "    Negative Log Likelihood: 60.7319\tSigma2 Prior: -605.7855\tRegularization: 0.0009\n",
            "Iter: 9590  \tTraining Loss: -540.2372    \n",
            "    Negative Log Likelihood: 61.3529\tSigma2 Prior: -601.5909\tRegularization: 0.0009\n",
            "Iter: 9600  \tTraining Loss: -551.2875    \n",
            "    Negative Log Likelihood: 61.5828\tSigma2 Prior: -612.8712\tRegularization: 0.0009\n",
            "Iter: 9610  \tTraining Loss: -528.0317    \n",
            "    Negative Log Likelihood: 63.5666\tSigma2 Prior: -591.5992\tRegularization: 0.0009\n",
            "Iter: 9620  \tTraining Loss: -542.9741    \n",
            "    Negative Log Likelihood: 61.6926\tSigma2 Prior: -604.6675\tRegularization: 0.0009\n",
            "Iter: 9630  \tTraining Loss: -537.2202    \n",
            "    Negative Log Likelihood: 62.2156\tSigma2 Prior: -599.4366\tRegularization: 0.0009\n",
            "Iter: 9640  \tTraining Loss: -512.8169    \n",
            "    Negative Log Likelihood: 65.0106\tSigma2 Prior: -577.8284\tRegularization: 0.0009\n",
            "Iter: 9650  \tTraining Loss: -545.8041    \n",
            "    Negative Log Likelihood: 61.4661\tSigma2 Prior: -607.2710\tRegularization: 0.0009\n",
            "Iter: 9660  \tTraining Loss: -535.6776    \n",
            "    Negative Log Likelihood: 62.2890\tSigma2 Prior: -597.9675\tRegularization: 0.0009\n",
            "Iter: 9670  \tTraining Loss: -545.4373    \n",
            "    Negative Log Likelihood: 60.0856\tSigma2 Prior: -605.5238\tRegularization: 0.0009\n",
            "Iter: 9680  \tTraining Loss: -543.6193    \n",
            "    Negative Log Likelihood: 62.3255\tSigma2 Prior: -605.9458\tRegularization: 0.0009\n",
            "Iter: 9690  \tTraining Loss: -530.6968    \n",
            "    Negative Log Likelihood: 63.9967\tSigma2 Prior: -594.6944\tRegularization: 0.0009\n",
            "Iter: 9700  \tTraining Loss: -541.9339    \n",
            "    Negative Log Likelihood: 60.7820\tSigma2 Prior: -602.7168\tRegularization: 0.0009\n",
            "Iter: 9710  \tTraining Loss: -542.2090    \n",
            "    Negative Log Likelihood: 60.2863\tSigma2 Prior: -602.4962\tRegularization: 0.0009\n",
            "Iter: 9720  \tTraining Loss: -526.0228    \n",
            "    Negative Log Likelihood: 62.7963\tSigma2 Prior: -588.8200\tRegularization: 0.0009\n",
            "Iter: 9730  \tTraining Loss: -529.9426    \n",
            "    Negative Log Likelihood: 62.9226\tSigma2 Prior: -592.8661\tRegularization: 0.0009\n",
            "Iter: 9740  \tTraining Loss: -539.0216    \n",
            "    Negative Log Likelihood: 61.7025\tSigma2 Prior: -600.7250\tRegularization: 0.0009\n",
            "Iter: 9750  \tTraining Loss: -516.8129    \n",
            "    Negative Log Likelihood: 64.8678\tSigma2 Prior: -581.6816\tRegularization: 0.0009\n",
            "Iter: 9760  \tTraining Loss: -530.0561    \n",
            "    Negative Log Likelihood: 62.7604\tSigma2 Prior: -592.8174\tRegularization: 0.0009\n",
            "Iter: 9770  \tTraining Loss: -521.3947    \n",
            "    Negative Log Likelihood: 63.3788\tSigma2 Prior: -584.7744\tRegularization: 0.0009\n",
            "Iter: 9780  \tTraining Loss: -522.7565    \n",
            "    Negative Log Likelihood: 64.9983\tSigma2 Prior: -587.7557\tRegularization: 0.0009\n",
            "Iter: 9790  \tTraining Loss: -521.4149    \n",
            "    Negative Log Likelihood: 64.1292\tSigma2 Prior: -585.5450\tRegularization: 0.0009\n",
            "Iter: 9800  \tTraining Loss: -521.1510    \n",
            "    Negative Log Likelihood: 63.7348\tSigma2 Prior: -584.8867\tRegularization: 0.0009\n",
            "Iter: 9810  \tTraining Loss: -546.2375    \n",
            "    Negative Log Likelihood: 61.4513\tSigma2 Prior: -607.6898\tRegularization: 0.0009\n",
            "Iter: 9820  \tTraining Loss: -549.3829    \n",
            "    Negative Log Likelihood: 58.8388\tSigma2 Prior: -608.2225\tRegularization: 0.0009\n",
            "Iter: 9830  \tTraining Loss: -532.2704    \n",
            "    Negative Log Likelihood: 60.3669\tSigma2 Prior: -592.6383\tRegularization: 0.0009\n",
            "Iter: 9840  \tTraining Loss: -542.1062    \n",
            "    Negative Log Likelihood: 61.6468\tSigma2 Prior: -603.7539\tRegularization: 0.0009\n",
            "Iter: 9850  \tTraining Loss: -543.4473    \n",
            "    Negative Log Likelihood: 61.2060\tSigma2 Prior: -604.6542\tRegularization: 0.0009\n",
            "Iter: 9860  \tTraining Loss: -543.8080    \n",
            "    Negative Log Likelihood: 62.6413\tSigma2 Prior: -606.4502\tRegularization: 0.0009\n",
            "Iter: 9870  \tTraining Loss: -516.6389    \n",
            "    Negative Log Likelihood: 64.6355\tSigma2 Prior: -581.2753\tRegularization: 0.0009\n",
            "Iter: 9880  \tTraining Loss: -541.1887    \n",
            "    Negative Log Likelihood: 60.6363\tSigma2 Prior: -601.8259\tRegularization: 0.0009\n",
            "Iter: 9890  \tTraining Loss: -539.0244    \n",
            "    Negative Log Likelihood: 61.9764\tSigma2 Prior: -601.0016\tRegularization: 0.0009\n",
            "Iter: 9900  \tTraining Loss: -543.9155    \n",
            "    Negative Log Likelihood: 61.7274\tSigma2 Prior: -605.6438\tRegularization: 0.0009\n",
            "Iter: 9910  \tTraining Loss: -542.2362    \n",
            "    Negative Log Likelihood: 60.2682\tSigma2 Prior: -602.5053\tRegularization: 0.0009\n",
            "Iter: 9920  \tTraining Loss: -538.7303    \n",
            "    Negative Log Likelihood: 60.4446\tSigma2 Prior: -599.1758\tRegularization: 0.0009\n",
            "Iter: 9930  \tTraining Loss: -533.1666    \n",
            "    Negative Log Likelihood: 63.5709\tSigma2 Prior: -596.7385\tRegularization: 0.0009\n",
            "Iter: 9940  \tTraining Loss: -515.2713    \n",
            "    Negative Log Likelihood: 63.5562\tSigma2 Prior: -578.8284\tRegularization: 0.0009\n",
            "Iter: 9950  \tTraining Loss: -547.5925    \n",
            "    Negative Log Likelihood: 59.7302\tSigma2 Prior: -607.3235\tRegularization: 0.0009\n",
            "Iter: 9960  \tTraining Loss: -547.5272    \n",
            "    Negative Log Likelihood: 59.9142\tSigma2 Prior: -607.4423\tRegularization: 0.0009\n",
            "Iter: 9970  \tTraining Loss: -556.0204    \n",
            "    Negative Log Likelihood: 58.6984\tSigma2 Prior: -614.7198\tRegularization: 0.0009\n",
            "Iter: 9980  \tTraining Loss: -524.2750    \n",
            "    Negative Log Likelihood: 62.7091\tSigma2 Prior: -586.9850\tRegularization: 0.0009\n",
            "Iter: 9990  \tTraining Loss: -535.5720    \n",
            "    Negative Log Likelihood: 61.1980\tSigma2 Prior: -596.7709\tRegularization: 0.0009\n",
            "Iter: 10000  \tTraining Loss: -526.4785    \n",
            "    Negative Log Likelihood: 63.3042\tSigma2 Prior: -589.7836\tRegularization: 0.0009\n",
            "Iter: 10010  \tTraining Loss: -529.8593    \n",
            "    Negative Log Likelihood: 63.2797\tSigma2 Prior: -593.1399\tRegularization: 0.0009\n",
            "Iter: 10020  \tTraining Loss: -532.6094    \n",
            "    Negative Log Likelihood: 64.0063\tSigma2 Prior: -596.6166\tRegularization: 0.0009\n",
            "Iter: 10030  \tTraining Loss: -528.2878    \n",
            "    Negative Log Likelihood: 64.4254\tSigma2 Prior: -592.7141\tRegularization: 0.0009\n",
            "Iter: 10040  \tTraining Loss: -528.5239    \n",
            "    Negative Log Likelihood: 63.9966\tSigma2 Prior: -592.5214\tRegularization: 0.0009\n",
            "Iter: 10050  \tTraining Loss: -537.5078    \n",
            "    Negative Log Likelihood: 61.3356\tSigma2 Prior: -598.8444\tRegularization: 0.0009\n",
            "Iter: 10060  \tTraining Loss: -548.6235    \n",
            "    Negative Log Likelihood: 59.6468\tSigma2 Prior: -608.2712\tRegularization: 0.0009\n",
            "Iter: 10070  \tTraining Loss: -552.8649    \n",
            "    Negative Log Likelihood: 59.7866\tSigma2 Prior: -612.6523\tRegularization: 0.0009\n",
            "Iter: 10080  \tTraining Loss: -529.6360    \n",
            "    Negative Log Likelihood: 62.0929\tSigma2 Prior: -591.7299\tRegularization: 0.0009\n",
            "Iter: 10090  \tTraining Loss: -527.5441    \n",
            "    Negative Log Likelihood: 63.1128\tSigma2 Prior: -590.6578\tRegularization: 0.0009\n",
            "Iter: 10100  \tTraining Loss: -517.6623    \n",
            "    Negative Log Likelihood: 63.4338\tSigma2 Prior: -581.0970\tRegularization: 0.0009\n",
            "Iter: 10110  \tTraining Loss: -524.9647    \n",
            "    Negative Log Likelihood: 64.4258\tSigma2 Prior: -589.3914\tRegularization: 0.0009\n",
            "Iter: 10120  \tTraining Loss: -537.6935    \n",
            "    Negative Log Likelihood: 61.5836\tSigma2 Prior: -599.2780\tRegularization: 0.0009\n",
            "Iter: 10130  \tTraining Loss: -541.1042    \n",
            "    Negative Log Likelihood: 61.5430\tSigma2 Prior: -602.6481\tRegularization: 0.0009\n",
            "Iter: 10140  \tTraining Loss: -543.3325    \n",
            "    Negative Log Likelihood: 59.7072\tSigma2 Prior: -603.0405\tRegularization: 0.0009\n",
            "Iter: 10150  \tTraining Loss: -548.7206    \n",
            "    Negative Log Likelihood: 60.7875\tSigma2 Prior: -609.5090\tRegularization: 0.0009\n",
            "Iter: 10160  \tTraining Loss: -535.7900    \n",
            "    Negative Log Likelihood: 61.0923\tSigma2 Prior: -596.8832\tRegularization: 0.0009\n",
            "Iter: 10170  \tTraining Loss: -535.7617    \n",
            "    Negative Log Likelihood: 61.6664\tSigma2 Prior: -597.4290\tRegularization: 0.0009\n",
            "Iter: 10180  \tTraining Loss: -535.8983    \n",
            "    Negative Log Likelihood: 61.3425\tSigma2 Prior: -597.2417\tRegularization: 0.0009\n",
            "Iter: 10190  \tTraining Loss: -545.5679    \n",
            "    Negative Log Likelihood: 61.3664\tSigma2 Prior: -606.9352\tRegularization: 0.0009\n",
            "Iter: 10200  \tTraining Loss: -530.8563    \n",
            "    Negative Log Likelihood: 62.2100\tSigma2 Prior: -593.0672\tRegularization: 0.0009\n",
            "Iter: 10210  \tTraining Loss: -551.1388    \n",
            "    Negative Log Likelihood: 61.1159\tSigma2 Prior: -612.2556\tRegularization: 0.0009\n",
            "Iter: 10220  \tTraining Loss: -541.0076    \n",
            "    Negative Log Likelihood: 61.4069\tSigma2 Prior: -602.4153\tRegularization: 0.0009\n",
            "Iter: 10230  \tTraining Loss: -524.5543    \n",
            "    Negative Log Likelihood: 61.6030\tSigma2 Prior: -586.1582\tRegularization: 0.0009\n",
            "Iter: 10240  \tTraining Loss: -546.7526    \n",
            "    Negative Log Likelihood: 59.4695\tSigma2 Prior: -606.2231\tRegularization: 0.0009\n",
            "Iter: 10250  \tTraining Loss: -531.7706    \n",
            "    Negative Log Likelihood: 61.9360\tSigma2 Prior: -593.7075\tRegularization: 0.0009\n",
            "Iter: 10260  \tTraining Loss: -532.9953    \n",
            "    Negative Log Likelihood: 63.2596\tSigma2 Prior: -596.2558\tRegularization: 0.0009\n",
            "Iter: 10270  \tTraining Loss: -547.2664    \n",
            "    Negative Log Likelihood: 59.7219\tSigma2 Prior: -606.9891\tRegularization: 0.0009\n",
            "Iter: 10280  \tTraining Loss: -520.3538    \n",
            "    Negative Log Likelihood: 64.3201\tSigma2 Prior: -584.6748\tRegularization: 0.0009\n",
            "Iter: 10290  \tTraining Loss: -512.3190    \n",
            "    Negative Log Likelihood: 65.2217\tSigma2 Prior: -577.5416\tRegularization: 0.0009\n",
            "Iter: 10300  \tTraining Loss: -541.1628    \n",
            "    Negative Log Likelihood: 61.7364\tSigma2 Prior: -602.9001\tRegularization: 0.0009\n",
            "Iter: 10310  \tTraining Loss: -540.7592    \n",
            "    Negative Log Likelihood: 62.2331\tSigma2 Prior: -602.9933\tRegularization: 0.0009\n",
            "Iter: 10320  \tTraining Loss: -543.0294    \n",
            "    Negative Log Likelihood: 61.8659\tSigma2 Prior: -604.8962\tRegularization: 0.0009\n",
            "Iter: 10330  \tTraining Loss: -545.3832    \n",
            "    Negative Log Likelihood: 62.0690\tSigma2 Prior: -607.4531\tRegularization: 0.0009\n",
            "Iter: 10340  \tTraining Loss: -535.9316    \n",
            "    Negative Log Likelihood: 61.7137\tSigma2 Prior: -597.6462\tRegularization: 0.0009\n",
            "Iter: 10350  \tTraining Loss: -531.9063    \n",
            "    Negative Log Likelihood: 61.9159\tSigma2 Prior: -593.8231\tRegularization: 0.0009\n",
            "Iter: 10360  \tTraining Loss: -545.2300    \n",
            "    Negative Log Likelihood: 60.3949\tSigma2 Prior: -605.6258\tRegularization: 0.0009\n",
            "Iter: 10370  \tTraining Loss: -526.8115    \n",
            "    Negative Log Likelihood: 62.4996\tSigma2 Prior: -589.3120\tRegularization: 0.0009\n",
            "Iter: 10380  \tTraining Loss: -525.2684    \n",
            "    Negative Log Likelihood: 63.0610\tSigma2 Prior: -588.3303\tRegularization: 0.0009\n",
            "Iter: 10390  \tTraining Loss: -546.1089    \n",
            "    Negative Log Likelihood: 60.6360\tSigma2 Prior: -606.7458\tRegularization: 0.0009\n",
            "Iter: 10400  \tTraining Loss: -528.6492    \n",
            "    Negative Log Likelihood: 62.8356\tSigma2 Prior: -591.4857\tRegularization: 0.0009\n",
            "Iter: 10410  \tTraining Loss: -546.0854    \n",
            "    Negative Log Likelihood: 61.3616\tSigma2 Prior: -607.4479\tRegularization: 0.0009\n",
            "Iter: 10420  \tTraining Loss: -529.8984    \n",
            "    Negative Log Likelihood: 63.9324\tSigma2 Prior: -593.8317\tRegularization: 0.0009\n",
            "Iter: 10430  \tTraining Loss: -534.3552    \n",
            "    Negative Log Likelihood: 61.0664\tSigma2 Prior: -595.4224\tRegularization: 0.0009\n",
            "Iter: 10440  \tTraining Loss: -555.6657    \n",
            "    Negative Log Likelihood: 59.6530\tSigma2 Prior: -615.3196\tRegularization: 0.0009\n",
            "Iter: 10450  \tTraining Loss: -529.9583    \n",
            "    Negative Log Likelihood: 62.8247\tSigma2 Prior: -592.7839\tRegularization: 0.0009\n",
            "Iter: 10460  \tTraining Loss: -524.6317    \n",
            "    Negative Log Likelihood: 62.8218\tSigma2 Prior: -587.4544\tRegularization: 0.0009\n",
            "Iter: 10470  \tTraining Loss: -543.1453    \n",
            "    Negative Log Likelihood: 62.3136\tSigma2 Prior: -605.4598\tRegularization: 0.0009\n",
            "Iter: 10480  \tTraining Loss: -533.2328    \n",
            "    Negative Log Likelihood: 61.4728\tSigma2 Prior: -594.7066\tRegularization: 0.0009\n",
            "Iter: 10490  \tTraining Loss: -504.4576    \n",
            "    Negative Log Likelihood: 62.2800\tSigma2 Prior: -566.7385\tRegularization: 0.0009\n",
            "Iter: 10500  \tTraining Loss: -532.0010    \n",
            "    Negative Log Likelihood: 61.2339\tSigma2 Prior: -593.2358\tRegularization: 0.0009\n",
            "Iter: 10510  \tTraining Loss: -542.6359    \n",
            "    Negative Log Likelihood: 59.8540\tSigma2 Prior: -602.4908\tRegularization: 0.0009\n",
            "Iter: 10520  \tTraining Loss: -529.7009    \n",
            "    Negative Log Likelihood: 62.1803\tSigma2 Prior: -591.8821\tRegularization: 0.0009\n",
            "Iter: 10530  \tTraining Loss: -535.7341    \n",
            "    Negative Log Likelihood: 61.4868\tSigma2 Prior: -597.2218\tRegularization: 0.0009\n",
            "Iter: 10540  \tTraining Loss: -537.0896    \n",
            "    Negative Log Likelihood: 61.6397\tSigma2 Prior: -598.7302\tRegularization: 0.0009\n",
            "Iter: 10550  \tTraining Loss: -534.2659    \n",
            "    Negative Log Likelihood: 61.5676\tSigma2 Prior: -595.8344\tRegularization: 0.0009\n",
            "Iter: 10560  \tTraining Loss: -531.1390    \n",
            "    Negative Log Likelihood: 61.7873\tSigma2 Prior: -592.9272\tRegularization: 0.0009\n",
            "Iter: 10570  \tTraining Loss: -529.9951    \n",
            "    Negative Log Likelihood: 62.1411\tSigma2 Prior: -592.1371\tRegularization: 0.0009\n",
            "Iter: 10580  \tTraining Loss: -531.6111    \n",
            "    Negative Log Likelihood: 60.0208\tSigma2 Prior: -591.6329\tRegularization: 0.0009\n",
            "Iter: 10590  \tTraining Loss: -530.4139    \n",
            "    Negative Log Likelihood: 60.7892\tSigma2 Prior: -591.2041\tRegularization: 0.0009\n",
            "Iter: 10600  \tTraining Loss: -540.5388    \n",
            "    Negative Log Likelihood: 61.4379\tSigma2 Prior: -601.9776\tRegularization: 0.0009\n",
            "Iter: 10610  \tTraining Loss: -541.4863    \n",
            "    Negative Log Likelihood: 60.4583\tSigma2 Prior: -601.9455\tRegularization: 0.0009\n",
            "Iter: 10620  \tTraining Loss: -547.3276    \n",
            "    Negative Log Likelihood: 59.8027\tSigma2 Prior: -607.1312\tRegularization: 0.0009\n",
            "Iter: 10630  \tTraining Loss: -521.0571    \n",
            "    Negative Log Likelihood: 64.4709\tSigma2 Prior: -585.5289\tRegularization: 0.0009\n",
            "Iter: 10640  \tTraining Loss: -531.1632    \n",
            "    Negative Log Likelihood: 61.3573\tSigma2 Prior: -592.5214\tRegularization: 0.0009\n",
            "Iter: 10650  \tTraining Loss: -550.4359    \n",
            "    Negative Log Likelihood: 59.2477\tSigma2 Prior: -609.6845\tRegularization: 0.0009\n",
            "Iter: 10660  \tTraining Loss: -531.9456    \n",
            "    Negative Log Likelihood: 61.6313\tSigma2 Prior: -593.5779\tRegularization: 0.0009\n",
            "Iter: 10670  \tTraining Loss: -525.9763    \n",
            "    Negative Log Likelihood: 62.2929\tSigma2 Prior: -588.2701\tRegularization: 0.0009\n",
            "Iter: 10680  \tTraining Loss: -552.2830    \n",
            "    Negative Log Likelihood: 59.9641\tSigma2 Prior: -612.2480\tRegularization: 0.0009\n",
            "Iter: 10690  \tTraining Loss: -545.3693    \n",
            "    Negative Log Likelihood: 59.7040\tSigma2 Prior: -605.0742\tRegularization: 0.0009\n",
            "Iter: 10700  \tTraining Loss: -547.2663    \n",
            "    Negative Log Likelihood: 60.8807\tSigma2 Prior: -608.1479\tRegularization: 0.0009\n",
            "Iter: 10710  \tTraining Loss: -557.0706    \n",
            "    Negative Log Likelihood: 59.6361\tSigma2 Prior: -616.7075\tRegularization: 0.0009\n",
            "Iter: 10720  \tTraining Loss: -537.0157    \n",
            "    Negative Log Likelihood: 62.3310\tSigma2 Prior: -599.3477\tRegularization: 0.0009\n",
            "Iter: 10730  \tTraining Loss: -548.0110    \n",
            "    Negative Log Likelihood: 59.3705\tSigma2 Prior: -607.3825\tRegularization: 0.0009\n",
            "Iter: 10740  \tTraining Loss: -542.7617    \n",
            "    Negative Log Likelihood: 62.2849\tSigma2 Prior: -605.0475\tRegularization: 0.0009\n",
            "Iter: 10750  \tTraining Loss: -540.3737    \n",
            "    Negative Log Likelihood: 62.7124\tSigma2 Prior: -603.0870\tRegularization: 0.0009\n",
            "Iter: 10760  \tTraining Loss: -532.5574    \n",
            "    Negative Log Likelihood: 63.1949\tSigma2 Prior: -595.7532\tRegularization: 0.0009\n",
            "Iter: 10770  \tTraining Loss: -546.2359    \n",
            "    Negative Log Likelihood: 61.2774\tSigma2 Prior: -607.5143\tRegularization: 0.0009\n",
            "Iter: 10780  \tTraining Loss: -547.8892    \n",
            "    Negative Log Likelihood: 60.5338\tSigma2 Prior: -608.4239\tRegularization: 0.0009\n",
            "Iter: 10790  \tTraining Loss: -497.1111    \n",
            "    Negative Log Likelihood: 65.9437\tSigma2 Prior: -563.0557\tRegularization: 0.0009\n",
            "Iter: 10800  \tTraining Loss: -544.5121    \n",
            "    Negative Log Likelihood: 60.7071\tSigma2 Prior: -605.2202\tRegularization: 0.0009\n",
            "Iter: 10810  \tTraining Loss: -537.0250    \n",
            "    Negative Log Likelihood: 62.3014\tSigma2 Prior: -599.3273\tRegularization: 0.0009\n",
            "Iter: 10820  \tTraining Loss: -530.3026    \n",
            "    Negative Log Likelihood: 62.0900\tSigma2 Prior: -592.3934\tRegularization: 0.0009\n",
            "Iter: 10830  \tTraining Loss: -518.4581    \n",
            "    Negative Log Likelihood: 63.8631\tSigma2 Prior: -582.3221\tRegularization: 0.0009\n",
            "Iter: 10840  \tTraining Loss: -534.5354    \n",
            "    Negative Log Likelihood: 63.0430\tSigma2 Prior: -597.5793\tRegularization: 0.0009\n",
            "Iter: 10850  \tTraining Loss: -518.9718    \n",
            "    Negative Log Likelihood: 63.5909\tSigma2 Prior: -582.5636\tRegularization: 0.0009\n",
            "Iter: 10860  \tTraining Loss: -502.0624    \n",
            "    Negative Log Likelihood: 65.7177\tSigma2 Prior: -567.7810\tRegularization: 0.0009\n",
            "Iter: 10870  \tTraining Loss: -553.1390    \n",
            "    Negative Log Likelihood: 58.7141\tSigma2 Prior: -611.8539\tRegularization: 0.0009\n",
            "Iter: 10880  \tTraining Loss: -541.0768    \n",
            "    Negative Log Likelihood: 60.5245\tSigma2 Prior: -601.6022\tRegularization: 0.0009\n",
            "Iter: 10890  \tTraining Loss: -536.4016    \n",
            "    Negative Log Likelihood: 63.5789\tSigma2 Prior: -599.9813\tRegularization: 0.0009\n",
            "Iter: 10900  \tTraining Loss: -536.4622    \n",
            "    Negative Log Likelihood: 62.6387\tSigma2 Prior: -599.1018\tRegularization: 0.0009\n",
            "Iter: 10910  \tTraining Loss: -532.0365    \n",
            "    Negative Log Likelihood: 63.8063\tSigma2 Prior: -595.8438\tRegularization: 0.0009\n",
            "Iter: 10920  \tTraining Loss: -545.4481    \n",
            "    Negative Log Likelihood: 61.0235\tSigma2 Prior: -606.4724\tRegularization: 0.0009\n",
            "Iter: 10930  \tTraining Loss: -554.2451    \n",
            "    Negative Log Likelihood: 59.4460\tSigma2 Prior: -613.6920\tRegularization: 0.0009\n",
            "Iter: 10940  \tTraining Loss: -538.7361    \n",
            "    Negative Log Likelihood: 62.9676\tSigma2 Prior: -601.7047\tRegularization: 0.0009\n",
            "Iter: 10950  \tTraining Loss: -532.4515    \n",
            "    Negative Log Likelihood: 61.5957\tSigma2 Prior: -594.0482\tRegularization: 0.0009\n",
            "Iter: 10960  \tTraining Loss: -532.4020    \n",
            "    Negative Log Likelihood: 62.9516\tSigma2 Prior: -595.3545\tRegularization: 0.0009\n",
            "Iter: 10970  \tTraining Loss: -532.0736    \n",
            "    Negative Log Likelihood: 63.6419\tSigma2 Prior: -595.7164\tRegularization: 0.0009\n",
            "Iter: 10980  \tTraining Loss: -534.7507    \n",
            "    Negative Log Likelihood: 63.6915\tSigma2 Prior: -598.4431\tRegularization: 0.0009\n",
            "Iter: 10990  \tTraining Loss: -537.4785    \n",
            "    Negative Log Likelihood: 61.2919\tSigma2 Prior: -598.7712\tRegularization: 0.0009\n",
            "Iter: 11000  \tTraining Loss: -539.4607    \n",
            "    Negative Log Likelihood: 62.3713\tSigma2 Prior: -601.8329\tRegularization: 0.0009\n",
            "Iter: 11010  \tTraining Loss: -531.1092    \n",
            "    Negative Log Likelihood: 61.0450\tSigma2 Prior: -592.1552\tRegularization: 0.0009\n",
            "Iter: 11020  \tTraining Loss: -531.0981    \n",
            "    Negative Log Likelihood: 61.8496\tSigma2 Prior: -592.9485\tRegularization: 0.0009\n",
            "Iter: 11030  \tTraining Loss: -528.8406    \n",
            "    Negative Log Likelihood: 62.6493\tSigma2 Prior: -591.4908\tRegularization: 0.0009\n",
            "Iter: 11040  \tTraining Loss: -537.2368    \n",
            "    Negative Log Likelihood: 61.7657\tSigma2 Prior: -599.0034\tRegularization: 0.0009\n",
            "Iter: 11050  \tTraining Loss: -541.6794    \n",
            "    Negative Log Likelihood: 61.2366\tSigma2 Prior: -602.9169\tRegularization: 0.0009\n",
            "Iter: 11060  \tTraining Loss: -548.5959    \n",
            "    Negative Log Likelihood: 59.0182\tSigma2 Prior: -607.6150\tRegularization: 0.0009\n",
            "Iter: 11070  \tTraining Loss: -535.5275    \n",
            "    Negative Log Likelihood: 61.8535\tSigma2 Prior: -597.3819\tRegularization: 0.0009\n",
            "Iter: 11080  \tTraining Loss: -528.2836    \n",
            "    Negative Log Likelihood: 62.6409\tSigma2 Prior: -590.9254\tRegularization: 0.0009\n",
            "Iter: 11090  \tTraining Loss: -537.6118    \n",
            "    Negative Log Likelihood: 61.2538\tSigma2 Prior: -598.8665\tRegularization: 0.0009\n",
            "Iter: 11100  \tTraining Loss: -537.6362    \n",
            "    Negative Log Likelihood: 60.2422\tSigma2 Prior: -597.8793\tRegularization: 0.0009\n",
            "Iter: 11110  \tTraining Loss: -560.4636    \n",
            "    Negative Log Likelihood: 57.7494\tSigma2 Prior: -618.2139\tRegularization: 0.0009\n",
            "Iter: 11120  \tTraining Loss: -543.2814    \n",
            "    Negative Log Likelihood: 60.8755\tSigma2 Prior: -604.1578\tRegularization: 0.0009\n",
            "Iter: 11130  \tTraining Loss: -533.6605    \n",
            "    Negative Log Likelihood: 62.6721\tSigma2 Prior: -596.3335\tRegularization: 0.0009\n",
            "Iter: 11140  \tTraining Loss: -521.4742    \n",
            "    Negative Log Likelihood: 64.2712\tSigma2 Prior: -585.7463\tRegularization: 0.0009\n",
            "Iter: 11150  \tTraining Loss: -547.8489    \n",
            "    Negative Log Likelihood: 59.8682\tSigma2 Prior: -607.7180\tRegularization: 0.0009\n",
            "Iter: 11160  \tTraining Loss: -534.1230    \n",
            "    Negative Log Likelihood: 62.7959\tSigma2 Prior: -596.9198\tRegularization: 0.0009\n",
            "Iter: 11170  \tTraining Loss: -544.3100    \n",
            "    Negative Log Likelihood: 60.0701\tSigma2 Prior: -604.3810\tRegularization: 0.0009\n",
            "Iter: 11180  \tTraining Loss: -534.2415    \n",
            "    Negative Log Likelihood: 61.6003\tSigma2 Prior: -595.8427\tRegularization: 0.0009\n",
            "Iter: 11190  \tTraining Loss: -538.8931    \n",
            "    Negative Log Likelihood: 59.8769\tSigma2 Prior: -598.7709\tRegularization: 0.0009\n",
            "Iter: 11200  \tTraining Loss: -530.6609    \n",
            "    Negative Log Likelihood: 61.8967\tSigma2 Prior: -592.5585\tRegularization: 0.0009\n",
            "Iter: 11210  \tTraining Loss: -546.8993    \n",
            "    Negative Log Likelihood: 60.4913\tSigma2 Prior: -607.3915\tRegularization: 0.0009\n",
            "Iter: 11220  \tTraining Loss: -530.5635    \n",
            "    Negative Log Likelihood: 62.1826\tSigma2 Prior: -592.7471\tRegularization: 0.0009\n",
            "Iter: 11230  \tTraining Loss: -538.6565    \n",
            "    Negative Log Likelihood: 60.7795\tSigma2 Prior: -599.4369\tRegularization: 0.0009\n",
            "Iter: 11240  \tTraining Loss: -535.0759    \n",
            "    Negative Log Likelihood: 62.0240\tSigma2 Prior: -597.1008\tRegularization: 0.0009\n",
            "Iter: 11250  \tTraining Loss: -554.1885    \n",
            "    Negative Log Likelihood: 59.3938\tSigma2 Prior: -613.5833\tRegularization: 0.0009\n",
            "Iter: 11260  \tTraining Loss: -518.7892    \n",
            "    Negative Log Likelihood: 63.7437\tSigma2 Prior: -582.5338\tRegularization: 0.0009\n",
            "Iter: 11270  \tTraining Loss: -544.2368    \n",
            "    Negative Log Likelihood: 60.2687\tSigma2 Prior: -604.5064\tRegularization: 0.0009\n",
            "Iter: 11280  \tTraining Loss: -507.4507    \n",
            "    Negative Log Likelihood: 62.9058\tSigma2 Prior: -570.3575\tRegularization: 0.0009\n",
            "Iter: 11290  \tTraining Loss: -528.4937    \n",
            "    Negative Log Likelihood: 61.6820\tSigma2 Prior: -590.1766\tRegularization: 0.0009\n",
            "Iter: 11300  \tTraining Loss: -535.5001    \n",
            "    Negative Log Likelihood: 60.5041\tSigma2 Prior: -596.0051\tRegularization: 0.0009\n",
            "Iter: 11310  \tTraining Loss: -532.3627    \n",
            "    Negative Log Likelihood: 60.2268\tSigma2 Prior: -592.5903\tRegularization: 0.0009\n",
            "Iter: 11320  \tTraining Loss: -529.9196    \n",
            "    Negative Log Likelihood: 60.4997\tSigma2 Prior: -590.4202\tRegularization: 0.0009\n",
            "Iter: 11330  \tTraining Loss: -521.5771    \n",
            "    Negative Log Likelihood: 61.6959\tSigma2 Prior: -583.2739\tRegularization: 0.0009\n",
            "Iter: 11340  \tTraining Loss: -538.4455    \n",
            "    Negative Log Likelihood: 60.7463\tSigma2 Prior: -599.1927\tRegularization: 0.0009\n",
            "Iter: 11350  \tTraining Loss: -532.8104    \n",
            "    Negative Log Likelihood: 60.4726\tSigma2 Prior: -593.2839\tRegularization: 0.0009\n",
            "Iter: 11360  \tTraining Loss: -549.0623    \n",
            "    Negative Log Likelihood: 58.2177\tSigma2 Prior: -607.2809\tRegularization: 0.0009\n",
            "Iter: 11370  \tTraining Loss: -544.6281    \n",
            "    Negative Log Likelihood: 59.0288\tSigma2 Prior: -603.6578\tRegularization: 0.0009\n",
            "Iter: 11380  \tTraining Loss: -549.7716    \n",
            "    Negative Log Likelihood: 58.0507\tSigma2 Prior: -607.8232\tRegularization: 0.0009\n",
            "Iter: 11390  \tTraining Loss: -542.5287    \n",
            "    Negative Log Likelihood: 60.7620\tSigma2 Prior: -603.2917\tRegularization: 0.0009\n",
            "Iter: 11400  \tTraining Loss: -519.6290    \n",
            "    Negative Log Likelihood: 61.6012\tSigma2 Prior: -581.2311\tRegularization: 0.0009\n",
            "Iter: 11410  \tTraining Loss: -538.2963    \n",
            "    Negative Log Likelihood: 60.6601\tSigma2 Prior: -598.9573\tRegularization: 0.0009\n",
            "Iter: 11420  \tTraining Loss: -547.4883    \n",
            "    Negative Log Likelihood: 60.5453\tSigma2 Prior: -608.0345\tRegularization: 0.0009\n",
            "Iter: 11430  \tTraining Loss: -554.4648    \n",
            "    Negative Log Likelihood: 58.2284\tSigma2 Prior: -612.6942\tRegularization: 0.0009\n",
            "Iter: 11440  \tTraining Loss: -533.2643    \n",
            "    Negative Log Likelihood: 62.1858\tSigma2 Prior: -595.4510\tRegularization: 0.0009\n",
            "Iter: 11450  \tTraining Loss: -517.4275    \n",
            "    Negative Log Likelihood: 63.6861\tSigma2 Prior: -581.1145\tRegularization: 0.0009\n",
            "Iter: 11460  \tTraining Loss: -535.7853    \n",
            "    Negative Log Likelihood: 62.5708\tSigma2 Prior: -598.3571\tRegularization: 0.0009\n",
            "Iter: 11470  \tTraining Loss: -549.3217    \n",
            "    Negative Log Likelihood: 60.4852\tSigma2 Prior: -609.8078\tRegularization: 0.0009\n",
            "Iter: 11480  \tTraining Loss: -528.3322    \n",
            "    Negative Log Likelihood: 63.6166\tSigma2 Prior: -591.9496\tRegularization: 0.0009\n",
            "Iter: 11490  \tTraining Loss: -527.3815    \n",
            "    Negative Log Likelihood: 62.7939\tSigma2 Prior: -590.1763\tRegularization: 0.0009\n",
            "Iter: 11500  \tTraining Loss: -529.9302    \n",
            "    Negative Log Likelihood: 61.9627\tSigma2 Prior: -591.8938\tRegularization: 0.0009\n",
            "Iter: 11510  \tTraining Loss: -537.7249    \n",
            "    Negative Log Likelihood: 61.5923\tSigma2 Prior: -599.3181\tRegularization: 0.0009\n",
            "Iter: 11520  \tTraining Loss: -527.5498    \n",
            "    Negative Log Likelihood: 63.1120\tSigma2 Prior: -590.6627\tRegularization: 0.0009\n",
            "Iter: 11530  \tTraining Loss: -524.1146    \n",
            "    Negative Log Likelihood: 62.9478\tSigma2 Prior: -587.0632\tRegularization: 0.0009\n",
            "Iter: 11540  \tTraining Loss: -536.9982    \n",
            "    Negative Log Likelihood: 61.3105\tSigma2 Prior: -598.3096\tRegularization: 0.0009\n",
            "Iter: 11550  \tTraining Loss: -537.7426    \n",
            "    Negative Log Likelihood: 62.9406\tSigma2 Prior: -600.6841\tRegularization: 0.0009\n",
            "Iter: 11560  \tTraining Loss: -529.4878    \n",
            "    Negative Log Likelihood: 62.4266\tSigma2 Prior: -591.9153\tRegularization: 0.0009\n",
            "Iter: 11570  \tTraining Loss: -536.8694    \n",
            "    Negative Log Likelihood: 61.6905\tSigma2 Prior: -598.5609\tRegularization: 0.0009\n",
            "Iter: 11580  \tTraining Loss: -541.4179    \n",
            "    Negative Log Likelihood: 60.2755\tSigma2 Prior: -601.6943\tRegularization: 0.0009\n",
            "Iter: 11590  \tTraining Loss: -485.8332    \n",
            "    Negative Log Likelihood: 66.7868\tSigma2 Prior: -552.6210\tRegularization: 0.0009\n",
            "Iter: 11600  \tTraining Loss: -547.0506    \n",
            "    Negative Log Likelihood: 59.4653\tSigma2 Prior: -606.5168\tRegularization: 0.0009\n",
            "Iter: 11610  \tTraining Loss: -537.9105    \n",
            "    Negative Log Likelihood: 61.2446\tSigma2 Prior: -599.1560\tRegularization: 0.0009\n",
            "Iter: 11620  \tTraining Loss: -530.4048    \n",
            "    Negative Log Likelihood: 62.6069\tSigma2 Prior: -593.0126\tRegularization: 0.0009\n",
            "Iter: 11630  \tTraining Loss: -543.4685    \n",
            "    Negative Log Likelihood: 60.4966\tSigma2 Prior: -603.9661\tRegularization: 0.0009\n",
            "Iter: 11640  \tTraining Loss: -529.3267    \n",
            "    Negative Log Likelihood: 60.7317\tSigma2 Prior: -590.0593\tRegularization: 0.0009\n",
            "Iter: 11650  \tTraining Loss: -541.7141    \n",
            "    Negative Log Likelihood: 60.2045\tSigma2 Prior: -601.9195\tRegularization: 0.0009\n",
            "Iter: 11660  \tTraining Loss: -523.2853    \n",
            "    Negative Log Likelihood: 63.8207\tSigma2 Prior: -587.1069\tRegularization: 0.0009\n",
            "Iter: 11670  \tTraining Loss: -542.6605    \n",
            "    Negative Log Likelihood: 61.2927\tSigma2 Prior: -603.9542\tRegularization: 0.0009\n",
            "Iter: 11680  \tTraining Loss: -523.6042    \n",
            "    Negative Log Likelihood: 63.1619\tSigma2 Prior: -586.7670\tRegularization: 0.0009\n",
            "Iter: 11690  \tTraining Loss: -536.7401    \n",
            "    Negative Log Likelihood: 61.0916\tSigma2 Prior: -597.8326\tRegularization: 0.0009\n",
            "Iter: 11700  \tTraining Loss: -524.5511    \n",
            "    Negative Log Likelihood: 61.5232\tSigma2 Prior: -586.0753\tRegularization: 0.0009\n",
            "Iter: 11710  \tTraining Loss: -538.5626    \n",
            "    Negative Log Likelihood: 61.8854\tSigma2 Prior: -600.4490\tRegularization: 0.0009\n",
            "Iter: 11720  \tTraining Loss: -550.2311    \n",
            "    Negative Log Likelihood: 58.0141\tSigma2 Prior: -608.2461\tRegularization: 0.0009\n",
            "Iter: 11730  \tTraining Loss: -518.2662    \n",
            "    Negative Log Likelihood: 63.7643\tSigma2 Prior: -582.0314\tRegularization: 0.0009\n",
            "Iter: 11740  \tTraining Loss: -541.9224    \n",
            "    Negative Log Likelihood: 61.7069\tSigma2 Prior: -603.6302\tRegularization: 0.0009\n",
            "Iter: 11750  \tTraining Loss: -550.4901    \n",
            "    Negative Log Likelihood: 59.9011\tSigma2 Prior: -610.3922\tRegularization: 0.0009\n",
            "Iter: 11760  \tTraining Loss: -544.7378    \n",
            "    Negative Log Likelihood: 61.7881\tSigma2 Prior: -606.5267\tRegularization: 0.0009\n",
            "Iter: 11770  \tTraining Loss: -534.7612    \n",
            "    Negative Log Likelihood: 61.9393\tSigma2 Prior: -596.7014\tRegularization: 0.0009\n",
            "Iter: 11780  \tTraining Loss: -549.5131    \n",
            "    Negative Log Likelihood: 61.8908\tSigma2 Prior: -611.4048\tRegularization: 0.0009\n",
            "Iter: 11790  \tTraining Loss: -506.0027    \n",
            "    Negative Log Likelihood: 64.5154\tSigma2 Prior: -570.5190\tRegularization: 0.0009\n",
            "Iter: 11800  \tTraining Loss: -543.1817    \n",
            "    Negative Log Likelihood: 61.9007\tSigma2 Prior: -605.0833\tRegularization: 0.0009\n",
            "Iter: 11810  \tTraining Loss: -546.8942    \n",
            "    Negative Log Likelihood: 60.7604\tSigma2 Prior: -607.6556\tRegularization: 0.0009\n",
            "Iter: 11820  \tTraining Loss: -539.9915    \n",
            "    Negative Log Likelihood: 59.5790\tSigma2 Prior: -599.5714\tRegularization: 0.0009\n",
            "Iter: 11830  \tTraining Loss: -517.1058    \n",
            "    Negative Log Likelihood: 63.8040\tSigma2 Prior: -580.9107\tRegularization: 0.0009\n",
            "Iter: 11840  \tTraining Loss: -533.1888    \n",
            "    Negative Log Likelihood: 61.8109\tSigma2 Prior: -595.0007\tRegularization: 0.0009\n",
            "Iter: 11850  \tTraining Loss: -529.8884    \n",
            "    Negative Log Likelihood: 61.4961\tSigma2 Prior: -591.3855\tRegularization: 0.0009\n",
            "Iter: 11860  \tTraining Loss: -543.6194    \n",
            "    Negative Log Likelihood: 59.2744\tSigma2 Prior: -602.8948\tRegularization: 0.0009\n",
            "Iter: 11870  \tTraining Loss: -558.0151    \n",
            "    Negative Log Likelihood: 55.7660\tSigma2 Prior: -613.7821\tRegularization: 0.0009\n",
            "Iter: 11880  \tTraining Loss: -539.4459    \n",
            "    Negative Log Likelihood: 60.6092\tSigma2 Prior: -600.0560\tRegularization: 0.0009\n",
            "Iter: 11890  \tTraining Loss: -535.1739    \n",
            "    Negative Log Likelihood: 60.6133\tSigma2 Prior: -595.7881\tRegularization: 0.0009\n",
            "Iter: 11900  \tTraining Loss: -533.6793    \n",
            "    Negative Log Likelihood: 60.7398\tSigma2 Prior: -594.4200\tRegularization: 0.0009\n",
            "Iter: 11910  \tTraining Loss: -518.4014    \n",
            "    Negative Log Likelihood: 62.5666\tSigma2 Prior: -580.9691\tRegularization: 0.0009\n",
            "Iter: 11920  \tTraining Loss: -514.6694    \n",
            "    Negative Log Likelihood: 62.4830\tSigma2 Prior: -577.1534\tRegularization: 0.0009\n",
            "Iter: 11930  \tTraining Loss: -548.7595    \n",
            "    Negative Log Likelihood: 59.6136\tSigma2 Prior: -608.3741\tRegularization: 0.0009\n",
            "Iter: 11940  \tTraining Loss: -539.0058    \n",
            "    Negative Log Likelihood: 59.7786\tSigma2 Prior: -598.7854\tRegularization: 0.0009\n",
            "Iter: 11950  \tTraining Loss: -545.0939    \n",
            "    Negative Log Likelihood: 60.4763\tSigma2 Prior: -605.5712\tRegularization: 0.0009\n",
            "Iter: 11960  \tTraining Loss: -534.9912    \n",
            "    Negative Log Likelihood: 60.6426\tSigma2 Prior: -595.6348\tRegularization: 0.0009\n",
            "Iter: 11970  \tTraining Loss: -526.7617    \n",
            "    Negative Log Likelihood: 60.9887\tSigma2 Prior: -587.7515\tRegularization: 0.0009\n",
            "Iter: 11980  \tTraining Loss: -546.0955    \n",
            "    Negative Log Likelihood: 58.9718\tSigma2 Prior: -605.0683\tRegularization: 0.0010\n",
            "Iter: 11990  \tTraining Loss: -547.9868    \n",
            "    Negative Log Likelihood: 58.3032\tSigma2 Prior: -606.2909\tRegularization: 0.0010\n",
            "Iter: 12000  \tTraining Loss: -545.9080    \n",
            "    Negative Log Likelihood: 59.0052\tSigma2 Prior: -604.9142\tRegularization: 0.0010\n",
            "Iter: 12010  \tTraining Loss: -556.8123    \n",
            "    Negative Log Likelihood: 56.3190\tSigma2 Prior: -613.1322\tRegularization: 0.0010\n",
            "Iter: 12020  \tTraining Loss: -538.2122    \n",
            "    Negative Log Likelihood: 59.0543\tSigma2 Prior: -597.2675\tRegularization: 0.0010\n",
            "Iter: 12030  \tTraining Loss: -545.1796    \n",
            "    Negative Log Likelihood: 59.2505\tSigma2 Prior: -604.4311\tRegularization: 0.0010\n",
            "Iter: 12040  \tTraining Loss: -551.2770    \n",
            "    Negative Log Likelihood: 57.6306\tSigma2 Prior: -608.9085\tRegularization: 0.0010\n",
            "Iter: 12050  \tTraining Loss: -554.3271    \n",
            "    Negative Log Likelihood: 58.7219\tSigma2 Prior: -613.0500\tRegularization: 0.0010\n",
            "Iter: 12060  \tTraining Loss: -518.5089    \n",
            "    Negative Log Likelihood: 62.4382\tSigma2 Prior: -580.9481\tRegularization: 0.0010\n",
            "Iter: 12070  \tTraining Loss: -536.7106    \n",
            "    Negative Log Likelihood: 59.6705\tSigma2 Prior: -596.3820\tRegularization: 0.0010\n",
            "Iter: 12080  \tTraining Loss: -536.2971    \n",
            "    Negative Log Likelihood: 60.7896\tSigma2 Prior: -597.0876\tRegularization: 0.0010\n",
            "Iter: 12090  \tTraining Loss: -551.1903    \n",
            "    Negative Log Likelihood: 60.1339\tSigma2 Prior: -611.3252\tRegularization: 0.0010\n",
            "Iter: 12100  \tTraining Loss: -549.3719    \n",
            "    Negative Log Likelihood: 59.3519\tSigma2 Prior: -608.7248\tRegularization: 0.0010\n",
            "Iter: 12110  \tTraining Loss: -523.8419    \n",
            "    Negative Log Likelihood: 62.3540\tSigma2 Prior: -586.1968\tRegularization: 0.0010\n",
            "Iter: 12120  \tTraining Loss: -553.5078    \n",
            "    Negative Log Likelihood: 58.9854\tSigma2 Prior: -612.4941\tRegularization: 0.0010\n",
            "Iter: 12130  \tTraining Loss: -532.1945    \n",
            "    Negative Log Likelihood: 61.4417\tSigma2 Prior: -593.6372\tRegularization: 0.0010\n",
            "Iter: 12140  \tTraining Loss: -542.5208    \n",
            "    Negative Log Likelihood: 61.0797\tSigma2 Prior: -603.6015\tRegularization: 0.0010\n",
            "Iter: 12150  \tTraining Loss: -543.2963    \n",
            "    Negative Log Likelihood: 60.8357\tSigma2 Prior: -604.1329\tRegularization: 0.0010\n",
            "Iter: 12160  \tTraining Loss: -539.3659    \n",
            "    Negative Log Likelihood: 61.3510\tSigma2 Prior: -600.7179\tRegularization: 0.0010\n",
            "Iter: 12170  \tTraining Loss: -533.1279    \n",
            "    Negative Log Likelihood: 61.2876\tSigma2 Prior: -594.4165\tRegularization: 0.0010\n",
            "Iter: 12180  \tTraining Loss: -528.8747    \n",
            "    Negative Log Likelihood: 61.8920\tSigma2 Prior: -590.7676\tRegularization: 0.0010\n",
            "Iter: 12190  \tTraining Loss: -532.8742    \n",
            "    Negative Log Likelihood: 60.9081\tSigma2 Prior: -593.7833\tRegularization: 0.0010\n",
            "Iter: 12200  \tTraining Loss: -539.7532    \n",
            "    Negative Log Likelihood: 61.0761\tSigma2 Prior: -600.8303\tRegularization: 0.0010\n",
            "Iter: 12210  \tTraining Loss: -542.8697    \n",
            "    Negative Log Likelihood: 61.5245\tSigma2 Prior: -604.3952\tRegularization: 0.0010\n",
            "Iter: 12220  \tTraining Loss: -521.6371    \n",
            "    Negative Log Likelihood: 62.1973\tSigma2 Prior: -583.8353\tRegularization: 0.0010\n",
            "Iter: 12230  \tTraining Loss: -519.9008    \n",
            "    Negative Log Likelihood: 62.2743\tSigma2 Prior: -582.1760\tRegularization: 0.0010\n",
            "Iter: 12240  \tTraining Loss: -523.1340    \n",
            "    Negative Log Likelihood: 63.1249\tSigma2 Prior: -586.2599\tRegularization: 0.0010\n",
            "Iter: 12250  \tTraining Loss: -538.4775    \n",
            "    Negative Log Likelihood: 62.8031\tSigma2 Prior: -601.2816\tRegularization: 0.0010\n",
            "Iter: 12260  \tTraining Loss: -552.6994    \n",
            "    Negative Log Likelihood: 59.2843\tSigma2 Prior: -611.9847\tRegularization: 0.0010\n",
            "Iter: 12270  \tTraining Loss: -510.5829    \n",
            "    Negative Log Likelihood: 65.0645\tSigma2 Prior: -575.6484\tRegularization: 0.0010\n",
            "Iter: 12280  \tTraining Loss: -550.2723    \n",
            "    Negative Log Likelihood: 60.0281\tSigma2 Prior: -610.3015\tRegularization: 0.0010\n",
            "Iter: 12290  \tTraining Loss: -538.2632    \n",
            "    Negative Log Likelihood: 61.2220\tSigma2 Prior: -599.4862\tRegularization: 0.0010\n",
            "Iter: 12300  \tTraining Loss: -530.7131    \n",
            "    Negative Log Likelihood: 60.6764\tSigma2 Prior: -591.3904\tRegularization: 0.0010\n",
            "Iter: 12310  \tTraining Loss: -556.4607    \n",
            "    Negative Log Likelihood: 58.1693\tSigma2 Prior: -614.6309\tRegularization: 0.0010\n",
            "Iter: 12320  \tTraining Loss: -527.9967    \n",
            "    Negative Log Likelihood: 60.9285\tSigma2 Prior: -588.9262\tRegularization: 0.0010\n",
            "Iter: 12330  \tTraining Loss: -543.2584    \n",
            "    Negative Log Likelihood: 58.7170\tSigma2 Prior: -601.9764\tRegularization: 0.0010\n",
            "Iter: 12340  \tTraining Loss: -551.7939    \n",
            "    Negative Log Likelihood: 59.0219\tSigma2 Prior: -610.8168\tRegularization: 0.0010\n",
            "Iter: 12350  \tTraining Loss: -538.7952    \n",
            "    Negative Log Likelihood: 60.6138\tSigma2 Prior: -599.4100\tRegularization: 0.0010\n",
            "Iter: 12360  \tTraining Loss: -544.1163    \n",
            "    Negative Log Likelihood: 60.8627\tSigma2 Prior: -604.9800\tRegularization: 0.0010\n",
            "Iter: 12370  \tTraining Loss: -531.3315    \n",
            "    Negative Log Likelihood: 61.5831\tSigma2 Prior: -592.9156\tRegularization: 0.0010\n",
            "Iter: 12380  \tTraining Loss: -548.3079    \n",
            "    Negative Log Likelihood: 58.4002\tSigma2 Prior: -606.7091\tRegularization: 0.0010\n",
            "Iter: 12390  \tTraining Loss: -546.3857    \n",
            "    Negative Log Likelihood: 59.7311\tSigma2 Prior: -606.1178\tRegularization: 0.0010\n",
            "Iter: 12400  \tTraining Loss: -530.4546    \n",
            "    Negative Log Likelihood: 60.3075\tSigma2 Prior: -590.7631\tRegularization: 0.0010\n",
            "Iter: 12410  \tTraining Loss: -539.0884    \n",
            "    Negative Log Likelihood: 60.2622\tSigma2 Prior: -599.3515\tRegularization: 0.0010\n",
            "Iter: 12420  \tTraining Loss: -527.9911    \n",
            "    Negative Log Likelihood: 62.3928\tSigma2 Prior: -590.3848\tRegularization: 0.0010\n",
            "Iter: 12430  \tTraining Loss: -542.8706    \n",
            "    Negative Log Likelihood: 59.4066\tSigma2 Prior: -602.2781\tRegularization: 0.0010\n",
            "Iter: 12440  \tTraining Loss: -524.9445    \n",
            "    Negative Log Likelihood: 62.6673\tSigma2 Prior: -587.6128\tRegularization: 0.0010\n",
            "Iter: 12450  \tTraining Loss: -533.9144    \n",
            "    Negative Log Likelihood: 60.7753\tSigma2 Prior: -594.6906\tRegularization: 0.0010\n",
            "Iter: 12460  \tTraining Loss: -516.9116    \n",
            "    Negative Log Likelihood: 62.3278\tSigma2 Prior: -579.2404\tRegularization: 0.0010\n",
            "Iter: 12470  \tTraining Loss: -552.0028    \n",
            "    Negative Log Likelihood: 59.9936\tSigma2 Prior: -611.9974\tRegularization: 0.0010\n",
            "Iter: 12480  \tTraining Loss: -536.5657    \n",
            "    Negative Log Likelihood: 60.7622\tSigma2 Prior: -597.3289\tRegularization: 0.0010\n",
            "Iter: 12490  \tTraining Loss: -539.7343    \n",
            "    Negative Log Likelihood: 60.4285\tSigma2 Prior: -600.1637\tRegularization: 0.0010\n",
            "Iter: 12500  \tTraining Loss: -523.8372    \n",
            "    Negative Log Likelihood: 61.3485\tSigma2 Prior: -585.1866\tRegularization: 0.0010\n",
            "Iter: 12510  \tTraining Loss: -541.4333    \n",
            "    Negative Log Likelihood: 59.5479\tSigma2 Prior: -600.9822\tRegularization: 0.0010\n",
            "Iter: 12520  \tTraining Loss: -509.7775    \n",
            "    Negative Log Likelihood: 61.8408\tSigma2 Prior: -571.6192\tRegularization: 0.0010\n",
            "Iter: 12530  \tTraining Loss: -553.3967    \n",
            "    Negative Log Likelihood: 58.3032\tSigma2 Prior: -611.7009\tRegularization: 0.0010\n",
            "Iter: 12540  \tTraining Loss: -551.0677    \n",
            "    Negative Log Likelihood: 57.5577\tSigma2 Prior: -608.6264\tRegularization: 0.0010\n",
            "Iter: 12550  \tTraining Loss: -504.5877    \n",
            "    Negative Log Likelihood: 63.4503\tSigma2 Prior: -568.0389\tRegularization: 0.0010\n",
            "Iter: 12560  \tTraining Loss: -544.7755    \n",
            "    Negative Log Likelihood: 59.0366\tSigma2 Prior: -603.8131\tRegularization: 0.0010\n",
            "Iter: 12570  \tTraining Loss: -546.0364    \n",
            "    Negative Log Likelihood: 58.5942\tSigma2 Prior: -604.6315\tRegularization: 0.0010\n",
            "Iter: 12580  \tTraining Loss: -544.3220    \n",
            "    Negative Log Likelihood: 59.4576\tSigma2 Prior: -603.7806\tRegularization: 0.0010\n",
            "Iter: 12590  \tTraining Loss: -549.3346    \n",
            "    Negative Log Likelihood: 57.4661\tSigma2 Prior: -606.8017\tRegularization: 0.0010\n",
            "Iter: 12600  \tTraining Loss: -533.1152    \n",
            "    Negative Log Likelihood: 61.7495\tSigma2 Prior: -594.8657\tRegularization: 0.0010\n",
            "Iter: 12610  \tTraining Loss: -545.1284    \n",
            "    Negative Log Likelihood: 61.0399\tSigma2 Prior: -606.1693\tRegularization: 0.0010\n",
            "Iter: 12620  \tTraining Loss: -539.3480    \n",
            "    Negative Log Likelihood: 61.1784\tSigma2 Prior: -600.5274\tRegularization: 0.0010\n",
            "Iter: 12630  \tTraining Loss: -536.0379    \n",
            "    Negative Log Likelihood: 61.4693\tSigma2 Prior: -597.5082\tRegularization: 0.0010\n",
            "Iter: 12640  \tTraining Loss: -548.0668    \n",
            "    Negative Log Likelihood: 59.9098\tSigma2 Prior: -607.9777\tRegularization: 0.0010\n",
            "Iter: 12650  \tTraining Loss: -538.1926    \n",
            "    Negative Log Likelihood: 59.6182\tSigma2 Prior: -597.8117\tRegularization: 0.0010\n",
            "Iter: 12660  \tTraining Loss: -538.1875    \n",
            "    Negative Log Likelihood: 61.0586\tSigma2 Prior: -599.2471\tRegularization: 0.0010\n",
            "Iter: 12670  \tTraining Loss: -534.6538    \n",
            "    Negative Log Likelihood: 60.9786\tSigma2 Prior: -595.6334\tRegularization: 0.0010\n",
            "Iter: 12680  \tTraining Loss: -541.9626    \n",
            "    Negative Log Likelihood: 61.1521\tSigma2 Prior: -603.1157\tRegularization: 0.0010\n",
            "Iter: 12690  \tTraining Loss: -539.1143    \n",
            "    Negative Log Likelihood: 60.6838\tSigma2 Prior: -599.7991\tRegularization: 0.0010\n",
            "Iter: 12700  \tTraining Loss: -542.5609    \n",
            "    Negative Log Likelihood: 59.0329\tSigma2 Prior: -601.5948\tRegularization: 0.0010\n",
            "Iter: 12710  \tTraining Loss: -537.5747    \n",
            "    Negative Log Likelihood: 60.8491\tSigma2 Prior: -598.4248\tRegularization: 0.0010\n",
            "Iter: 12720  \tTraining Loss: -533.2753    \n",
            "    Negative Log Likelihood: 60.1649\tSigma2 Prior: -593.4412\tRegularization: 0.0010\n",
            "Iter: 12730  \tTraining Loss: -535.0050    \n",
            "    Negative Log Likelihood: 59.3153\tSigma2 Prior: -594.3213\tRegularization: 0.0010\n",
            "Iter: 12740  \tTraining Loss: -547.1747    \n",
            "    Negative Log Likelihood: 59.3038\tSigma2 Prior: -606.4795\tRegularization: 0.0010\n",
            "Iter: 12750  \tTraining Loss: -523.6290    \n",
            "    Negative Log Likelihood: 60.9841\tSigma2 Prior: -584.6141\tRegularization: 0.0010\n",
            "Iter: 12760  \tTraining Loss: -520.8539    \n",
            "    Negative Log Likelihood: 61.5439\tSigma2 Prior: -582.3987\tRegularization: 0.0010\n",
            "Iter: 12770  \tTraining Loss: -517.8239    \n",
            "    Negative Log Likelihood: 63.2949\tSigma2 Prior: -581.1198\tRegularization: 0.0010\n",
            "Iter: 12780  \tTraining Loss: -542.6696    \n",
            "    Negative Log Likelihood: 59.0705\tSigma2 Prior: -601.7410\tRegularization: 0.0010\n",
            "Iter: 12790  \tTraining Loss: -528.3093    \n",
            "    Negative Log Likelihood: 62.8786\tSigma2 Prior: -591.1889\tRegularization: 0.0010\n",
            "Iter: 12800  \tTraining Loss: -521.3124    \n",
            "    Negative Log Likelihood: 62.9773\tSigma2 Prior: -584.2906\tRegularization: 0.0010\n",
            "Iter: 12810  \tTraining Loss: -533.8466    \n",
            "    Negative Log Likelihood: 58.5130\tSigma2 Prior: -592.3605\tRegularization: 0.0010\n",
            "Iter: 12820  \tTraining Loss: -542.3995    \n",
            "    Negative Log Likelihood: 60.4096\tSigma2 Prior: -602.8101\tRegularization: 0.0010\n",
            "Iter: 12830  \tTraining Loss: -535.4340    \n",
            "    Negative Log Likelihood: 61.5433\tSigma2 Prior: -596.9782\tRegularization: 0.0010\n",
            "Iter: 12840  \tTraining Loss: -543.1157    \n",
            "    Negative Log Likelihood: 58.5046\tSigma2 Prior: -601.6213\tRegularization: 0.0010\n",
            "Iter: 12850  \tTraining Loss: -549.2584    \n",
            "    Negative Log Likelihood: 58.5476\tSigma2 Prior: -607.8069\tRegularization: 0.0010\n",
            "Iter: 12860  \tTraining Loss: -520.5775    \n",
            "    Negative Log Likelihood: 62.2420\tSigma2 Prior: -582.8205\tRegularization: 0.0010\n",
            "Iter: 12870  \tTraining Loss: -553.8281    \n",
            "    Negative Log Likelihood: 57.6478\tSigma2 Prior: -611.4769\tRegularization: 0.0010\n",
            "Iter: 12880  \tTraining Loss: -533.8155    \n",
            "    Negative Log Likelihood: 61.2504\tSigma2 Prior: -595.0668\tRegularization: 0.0010\n",
            "Iter: 12890  \tTraining Loss: -533.5845    \n",
            "    Negative Log Likelihood: 61.3697\tSigma2 Prior: -594.9552\tRegularization: 0.0010\n",
            "Iter: 12900  \tTraining Loss: -539.8533    \n",
            "    Negative Log Likelihood: 59.3258\tSigma2 Prior: -599.1801\tRegularization: 0.0010\n",
            "Iter: 12910  \tTraining Loss: -529.6896    \n",
            "    Negative Log Likelihood: 61.5664\tSigma2 Prior: -591.2570\tRegularization: 0.0010\n",
            "Iter: 12920  \tTraining Loss: -519.4919    \n",
            "    Negative Log Likelihood: 61.7901\tSigma2 Prior: -581.2830\tRegularization: 0.0010\n",
            "Iter: 12930  \tTraining Loss: -543.4619    \n",
            "    Negative Log Likelihood: 59.0628\tSigma2 Prior: -602.5257\tRegularization: 0.0010\n",
            "Iter: 12940  \tTraining Loss: -550.5906    \n",
            "    Negative Log Likelihood: 58.2309\tSigma2 Prior: -608.8224\tRegularization: 0.0010\n",
            "Iter: 12950  \tTraining Loss: -543.2635    \n",
            "    Negative Log Likelihood: 59.4324\tSigma2 Prior: -602.6970\tRegularization: 0.0010\n",
            "Iter: 12960  \tTraining Loss: -541.1875    \n",
            "    Negative Log Likelihood: 60.2457\tSigma2 Prior: -601.4341\tRegularization: 0.0010\n",
            "Iter: 12970  \tTraining Loss: -536.0956    \n",
            "    Negative Log Likelihood: 60.4622\tSigma2 Prior: -596.5587\tRegularization: 0.0010\n",
            "Iter: 12980  \tTraining Loss: -547.8670    \n",
            "    Negative Log Likelihood: 58.2710\tSigma2 Prior: -606.1390\tRegularization: 0.0010\n",
            "Iter: 12990  \tTraining Loss: -560.6266    \n",
            "    Negative Log Likelihood: 56.9362\tSigma2 Prior: -617.5638\tRegularization: 0.0010\n",
            "Iter: 13000  \tTraining Loss: -504.7202    \n",
            "    Negative Log Likelihood: 64.1830\tSigma2 Prior: -568.9041\tRegularization: 0.0010\n",
            "Iter: 13010  \tTraining Loss: -531.5452    \n",
            "    Negative Log Likelihood: 61.8539\tSigma2 Prior: -593.4000\tRegularization: 0.0010\n",
            "Iter: 13020  \tTraining Loss: -535.4595    \n",
            "    Negative Log Likelihood: 61.1003\tSigma2 Prior: -596.5608\tRegularization: 0.0010\n",
            "Iter: 13030  \tTraining Loss: -537.7585    \n",
            "    Negative Log Likelihood: 58.9126\tSigma2 Prior: -596.6721\tRegularization: 0.0010\n",
            "Iter: 13040  \tTraining Loss: -528.1771    \n",
            "    Negative Log Likelihood: 62.4722\tSigma2 Prior: -590.6502\tRegularization: 0.0010\n",
            "Iter: 13050  \tTraining Loss: -518.5106    \n",
            "    Negative Log Likelihood: 60.2852\tSigma2 Prior: -578.7968\tRegularization: 0.0010\n",
            "Iter: 13060  \tTraining Loss: -525.0772    \n",
            "    Negative Log Likelihood: 62.3212\tSigma2 Prior: -587.3994\tRegularization: 0.0010\n",
            "Iter: 13070  \tTraining Loss: -529.7419    \n",
            "    Negative Log Likelihood: 62.5502\tSigma2 Prior: -592.2931\tRegularization: 0.0010\n",
            "Iter: 13080  \tTraining Loss: -534.7162    \n",
            "    Negative Log Likelihood: 60.4392\tSigma2 Prior: -595.1563\tRegularization: 0.0010\n",
            "Iter: 13090  \tTraining Loss: -541.3961    \n",
            "    Negative Log Likelihood: 58.8774\tSigma2 Prior: -600.2745\tRegularization: 0.0010\n",
            "Iter: 13100  \tTraining Loss: -552.0543    \n",
            "    Negative Log Likelihood: 58.3862\tSigma2 Prior: -610.4415\tRegularization: 0.0010\n",
            "Iter: 13110  \tTraining Loss: -542.2144    \n",
            "    Negative Log Likelihood: 59.2649\tSigma2 Prior: -601.4802\tRegularization: 0.0010\n",
            "Iter: 13120  \tTraining Loss: -529.5738    \n",
            "    Negative Log Likelihood: 59.8937\tSigma2 Prior: -589.4685\tRegularization: 0.0010\n",
            "Iter: 13130  \tTraining Loss: -541.0549    \n",
            "    Negative Log Likelihood: 58.0770\tSigma2 Prior: -599.1329\tRegularization: 0.0010\n",
            "Iter: 13140  \tTraining Loss: -548.0433    \n",
            "    Negative Log Likelihood: 57.9808\tSigma2 Prior: -606.0251\tRegularization: 0.0010\n",
            "Iter: 13150  \tTraining Loss: -525.4000    \n",
            "    Negative Log Likelihood: 61.3479\tSigma2 Prior: -586.7489\tRegularization: 0.0010\n",
            "Iter: 13160  \tTraining Loss: -536.6981    \n",
            "    Negative Log Likelihood: 59.3186\tSigma2 Prior: -596.0177\tRegularization: 0.0010\n",
            "Iter: 13170  \tTraining Loss: -542.3766    \n",
            "    Negative Log Likelihood: 58.1703\tSigma2 Prior: -600.5479\tRegularization: 0.0010\n",
            "Iter: 13180  \tTraining Loss: -538.4380    \n",
            "    Negative Log Likelihood: 59.5904\tSigma2 Prior: -598.0294\tRegularization: 0.0010\n",
            "Iter: 13190  \tTraining Loss: -531.6825    \n",
            "    Negative Log Likelihood: 62.2564\tSigma2 Prior: -593.9399\tRegularization: 0.0010\n",
            "Iter: 13200  \tTraining Loss: -520.1880    \n",
            "    Negative Log Likelihood: 61.4194\tSigma2 Prior: -581.6083\tRegularization: 0.0010\n",
            "Iter: 13210  \tTraining Loss: -545.1824    \n",
            "    Negative Log Likelihood: 58.5237\tSigma2 Prior: -603.7070\tRegularization: 0.0010\n",
            "Iter: 13220  \tTraining Loss: -536.8095    \n",
            "    Negative Log Likelihood: 59.7250\tSigma2 Prior: -596.5355\tRegularization: 0.0010\n",
            "Iter: 13230  \tTraining Loss: -523.5869    \n",
            "    Negative Log Likelihood: 60.6274\tSigma2 Prior: -584.2153\tRegularization: 0.0010\n",
            "Iter: 13240  \tTraining Loss: -555.5184    \n",
            "    Negative Log Likelihood: 56.0312\tSigma2 Prior: -611.5505\tRegularization: 0.0010\n",
            "Iter: 13250  \tTraining Loss: -538.1188    \n",
            "    Negative Log Likelihood: 59.2534\tSigma2 Prior: -597.3732\tRegularization: 0.0010\n",
            "Iter: 13260  \tTraining Loss: -530.1422    \n",
            "    Negative Log Likelihood: 59.9106\tSigma2 Prior: -590.0537\tRegularization: 0.0010\n",
            "Iter: 13270  \tTraining Loss: -535.0111    \n",
            "    Negative Log Likelihood: 59.1769\tSigma2 Prior: -594.1890\tRegularization: 0.0010\n",
            "Iter: 13280  \tTraining Loss: -526.5754    \n",
            "    Negative Log Likelihood: 59.8165\tSigma2 Prior: -586.3928\tRegularization: 0.0010\n",
            "Iter: 13290  \tTraining Loss: -524.4016    \n",
            "    Negative Log Likelihood: 61.5552\tSigma2 Prior: -585.9578\tRegularization: 0.0010\n",
            "Iter: 13300  \tTraining Loss: -533.3843    \n",
            "    Negative Log Likelihood: 60.6657\tSigma2 Prior: -594.0510\tRegularization: 0.0010\n",
            "Iter: 13310  \tTraining Loss: -540.6402    \n",
            "    Negative Log Likelihood: 59.8445\tSigma2 Prior: -600.4857\tRegularization: 0.0010\n",
            "Iter: 13320  \tTraining Loss: -531.1135    \n",
            "    Negative Log Likelihood: 59.4010\tSigma2 Prior: -590.5155\tRegularization: 0.0010\n",
            "Iter: 13330  \tTraining Loss: -541.9235    \n",
            "    Negative Log Likelihood: 56.8699\tSigma2 Prior: -598.7944\tRegularization: 0.0010\n",
            "Iter: 13340  \tTraining Loss: -521.3707    \n",
            "    Negative Log Likelihood: 60.3384\tSigma2 Prior: -581.7101\tRegularization: 0.0010\n",
            "Iter: 13350  \tTraining Loss: -536.9219    \n",
            "    Negative Log Likelihood: 60.2098\tSigma2 Prior: -597.1327\tRegularization: 0.0010\n",
            "Iter: 13360  \tTraining Loss: -532.8879    \n",
            "    Negative Log Likelihood: 61.5023\tSigma2 Prior: -594.3912\tRegularization: 0.0010\n",
            "Iter: 13370  \tTraining Loss: -522.1297    \n",
            "    Negative Log Likelihood: 62.5832\tSigma2 Prior: -584.7139\tRegularization: 0.0010\n",
            "Iter: 13380  \tTraining Loss: -545.8035    \n",
            "    Negative Log Likelihood: 59.1904\tSigma2 Prior: -604.9948\tRegularization: 0.0010\n",
            "Iter: 13390  \tTraining Loss: -553.6010    \n",
            "    Negative Log Likelihood: 58.5708\tSigma2 Prior: -612.1727\tRegularization: 0.0010\n",
            "Iter: 13400  \tTraining Loss: -539.3042    \n",
            "    Negative Log Likelihood: 59.4573\tSigma2 Prior: -598.7625\tRegularization: 0.0010\n",
            "Iter: 13410  \tTraining Loss: -531.3490    \n",
            "    Negative Log Likelihood: 62.3084\tSigma2 Prior: -593.6583\tRegularization: 0.0010\n",
            "Iter: 13420  \tTraining Loss: -549.3853    \n",
            "    Negative Log Likelihood: 59.5392\tSigma2 Prior: -608.9254\tRegularization: 0.0010\n",
            "Iter: 13430  \tTraining Loss: -516.3360    \n",
            "    Negative Log Likelihood: 63.9618\tSigma2 Prior: -580.2988\tRegularization: 0.0010\n",
            "Iter: 13440  \tTraining Loss: -527.4625    \n",
            "    Negative Log Likelihood: 61.3952\tSigma2 Prior: -588.8586\tRegularization: 0.0010\n",
            "Iter: 13450  \tTraining Loss: -529.9727    \n",
            "    Negative Log Likelihood: 60.8856\tSigma2 Prior: -590.8593\tRegularization: 0.0010\n",
            "Iter: 13460  \tTraining Loss: -543.9724    \n",
            "    Negative Log Likelihood: 58.5146\tSigma2 Prior: -602.4880\tRegularization: 0.0010\n",
            "Iter: 13470  \tTraining Loss: -538.8619    \n",
            "    Negative Log Likelihood: 61.1286\tSigma2 Prior: -599.9914\tRegularization: 0.0010\n",
            "Iter: 13480  \tTraining Loss: -545.5433    \n",
            "    Negative Log Likelihood: 59.4837\tSigma2 Prior: -605.0279\tRegularization: 0.0010\n",
            "Iter: 13490  \tTraining Loss: -528.8237    \n",
            "    Negative Log Likelihood: 60.0629\tSigma2 Prior: -588.8876\tRegularization: 0.0010\n",
            "Iter: 13500  \tTraining Loss: -532.0913    \n",
            "    Negative Log Likelihood: 60.9222\tSigma2 Prior: -593.0145\tRegularization: 0.0010\n",
            "Iter: 13510  \tTraining Loss: -554.7037    \n",
            "    Negative Log Likelihood: 58.5611\tSigma2 Prior: -613.2657\tRegularization: 0.0010\n",
            "Iter: 13520  \tTraining Loss: -530.4852    \n",
            "    Negative Log Likelihood: 60.8748\tSigma2 Prior: -591.3610\tRegularization: 0.0010\n",
            "Iter: 13530  \tTraining Loss: -539.8685    \n",
            "    Negative Log Likelihood: 60.9836\tSigma2 Prior: -600.8531\tRegularization: 0.0010\n",
            "Iter: 13540  \tTraining Loss: -542.4492    \n",
            "    Negative Log Likelihood: 59.8515\tSigma2 Prior: -602.3017\tRegularization: 0.0010\n",
            "Iter: 13550  \tTraining Loss: -523.3246    \n",
            "    Negative Log Likelihood: 62.4816\tSigma2 Prior: -585.8073\tRegularization: 0.0010\n",
            "Iter: 13560  \tTraining Loss: -531.1394    \n",
            "    Negative Log Likelihood: 61.6966\tSigma2 Prior: -592.8370\tRegularization: 0.0010\n",
            "Iter: 13570  \tTraining Loss: -510.5483    \n",
            "    Negative Log Likelihood: 62.1200\tSigma2 Prior: -572.6693\tRegularization: 0.0010\n",
            "Iter: 13580  \tTraining Loss: -546.1667    \n",
            "    Negative Log Likelihood: 58.8900\tSigma2 Prior: -605.0577\tRegularization: 0.0010\n",
            "Iter: 13590  \tTraining Loss: -528.0925    \n",
            "    Negative Log Likelihood: 60.3366\tSigma2 Prior: -588.4301\tRegularization: 0.0010\n",
            "Iter: 13600  \tTraining Loss: -545.4355    \n",
            "    Negative Log Likelihood: 59.5151\tSigma2 Prior: -604.9516\tRegularization: 0.0010\n",
            "Iter: 13610  \tTraining Loss: -546.8250    \n",
            "    Negative Log Likelihood: 58.0017\tSigma2 Prior: -604.8276\tRegularization: 0.0010\n",
            "Iter: 13620  \tTraining Loss: -546.0321    \n",
            "    Negative Log Likelihood: 59.4944\tSigma2 Prior: -605.5275\tRegularization: 0.0010\n",
            "Iter: 13630  \tTraining Loss: -525.3511    \n",
            "    Negative Log Likelihood: 61.1974\tSigma2 Prior: -586.5495\tRegularization: 0.0010\n",
            "Iter: 13640  \tTraining Loss: -546.6311    \n",
            "    Negative Log Likelihood: 58.6998\tSigma2 Prior: -605.3319\tRegularization: 0.0010\n",
            "Iter: 13650  \tTraining Loss: -522.5808    \n",
            "    Negative Log Likelihood: 60.5525\tSigma2 Prior: -583.1343\tRegularization: 0.0010\n",
            "Iter: 13660  \tTraining Loss: -535.0847    \n",
            "    Negative Log Likelihood: 60.9357\tSigma2 Prior: -596.0214\tRegularization: 0.0010\n",
            "Iter: 13670  \tTraining Loss: -542.8662    \n",
            "    Negative Log Likelihood: 59.6984\tSigma2 Prior: -602.5656\tRegularization: 0.0010\n",
            "Iter: 13680  \tTraining Loss: -539.8935    \n",
            "    Negative Log Likelihood: 61.7534\tSigma2 Prior: -601.6479\tRegularization: 0.0010\n",
            "Iter: 13690  \tTraining Loss: -544.1180    \n",
            "    Negative Log Likelihood: 59.6281\tSigma2 Prior: -603.7471\tRegularization: 0.0010\n",
            "Iter: 13700  \tTraining Loss: -547.3946    \n",
            "    Negative Log Likelihood: 60.1326\tSigma2 Prior: -607.5282\tRegularization: 0.0010\n",
            "Iter: 13710  \tTraining Loss: -530.9912    \n",
            "    Negative Log Likelihood: 62.6027\tSigma2 Prior: -593.5948\tRegularization: 0.0010\n",
            "Iter: 13720  \tTraining Loss: -557.5413    \n",
            "    Negative Log Likelihood: 58.5339\tSigma2 Prior: -616.0762\tRegularization: 0.0010\n",
            "Iter: 13730  \tTraining Loss: -554.1988    \n",
            "    Negative Log Likelihood: 59.3533\tSigma2 Prior: -613.5530\tRegularization: 0.0010\n",
            "Iter: 13740  \tTraining Loss: -544.1952    \n",
            "    Negative Log Likelihood: 61.0315\tSigma2 Prior: -605.2277\tRegularization: 0.0010\n",
            "Iter: 13750  \tTraining Loss: -516.6333    \n",
            "    Negative Log Likelihood: 64.3463\tSigma2 Prior: -580.9806\tRegularization: 0.0010\n",
            "Iter: 13760  \tTraining Loss: -532.5416    \n",
            "    Negative Log Likelihood: 62.3863\tSigma2 Prior: -594.9288\tRegularization: 0.0010\n",
            "Iter: 13770  \tTraining Loss: -546.1238    \n",
            "    Negative Log Likelihood: 58.5637\tSigma2 Prior: -604.6885\tRegularization: 0.0010\n",
            "Iter: 13780  \tTraining Loss: -532.5803    \n",
            "    Negative Log Likelihood: 62.0381\tSigma2 Prior: -594.6194\tRegularization: 0.0010\n",
            "Iter: 13790  \tTraining Loss: -551.2483    \n",
            "    Negative Log Likelihood: 58.9192\tSigma2 Prior: -610.1685\tRegularization: 0.0010\n",
            "Iter: 13800  \tTraining Loss: -559.0769    \n",
            "    Negative Log Likelihood: 58.3974\tSigma2 Prior: -617.4752\tRegularization: 0.0010\n",
            "Iter: 13810  \tTraining Loss: -539.8331    \n",
            "    Negative Log Likelihood: 59.1539\tSigma2 Prior: -598.9880\tRegularization: 0.0010\n",
            "Iter: 13820  \tTraining Loss: -534.0084    \n",
            "    Negative Log Likelihood: 60.6823\tSigma2 Prior: -594.6917\tRegularization: 0.0010\n",
            "Iter: 13830  \tTraining Loss: -549.3251    \n",
            "    Negative Log Likelihood: 59.6702\tSigma2 Prior: -608.9962\tRegularization: 0.0010\n",
            "Iter: 13840  \tTraining Loss: -528.4656    \n",
            "    Negative Log Likelihood: 61.3688\tSigma2 Prior: -589.8354\tRegularization: 0.0010\n",
            "Iter: 13850  \tTraining Loss: -519.1816    \n",
            "    Negative Log Likelihood: 63.3055\tSigma2 Prior: -582.4881\tRegularization: 0.0010\n",
            "Iter: 13860  \tTraining Loss: -533.1338    \n",
            "    Negative Log Likelihood: 61.3946\tSigma2 Prior: -594.5293\tRegularization: 0.0010\n",
            "Iter: 13870  \tTraining Loss: -535.8380    \n",
            "    Negative Log Likelihood: 60.3275\tSigma2 Prior: -596.1664\tRegularization: 0.0010\n",
            "Iter: 13880  \tTraining Loss: -542.6397    \n",
            "    Negative Log Likelihood: 59.7804\tSigma2 Prior: -602.4211\tRegularization: 0.0010\n",
            "Iter: 13890  \tTraining Loss: -525.6193    \n",
            "    Negative Log Likelihood: 60.8289\tSigma2 Prior: -586.4491\tRegularization: 0.0010\n",
            "Iter: 13900  \tTraining Loss: -542.3885    \n",
            "    Negative Log Likelihood: 58.0517\tSigma2 Prior: -600.4412\tRegularization: 0.0010\n",
            "Iter: 13910  \tTraining Loss: -534.1867    \n",
            "    Negative Log Likelihood: 59.9339\tSigma2 Prior: -594.1216\tRegularization: 0.0010\n",
            "Iter: 13920  \tTraining Loss: -556.8318    \n",
            "    Negative Log Likelihood: 57.1452\tSigma2 Prior: -613.9781\tRegularization: 0.0010\n",
            "Iter: 13930  \tTraining Loss: -549.2182    \n",
            "    Negative Log Likelihood: 57.8376\tSigma2 Prior: -607.0568\tRegularization: 0.0010\n",
            "Iter: 13940  \tTraining Loss: -544.7651    \n",
            "    Negative Log Likelihood: 59.6045\tSigma2 Prior: -604.3707\tRegularization: 0.0010\n",
            "Iter: 13950  \tTraining Loss: -522.8286    \n",
            "    Negative Log Likelihood: 61.9555\tSigma2 Prior: -584.7850\tRegularization: 0.0010\n",
            "Iter: 13960  \tTraining Loss: -537.0506    \n",
            "    Negative Log Likelihood: 59.7302\tSigma2 Prior: -596.7818\tRegularization: 0.0010\n",
            "Iter: 13970  \tTraining Loss: -542.1403    \n",
            "    Negative Log Likelihood: 60.1635\tSigma2 Prior: -602.3048\tRegularization: 0.0010\n",
            "Iter: 13980  \tTraining Loss: -540.5458    \n",
            "    Negative Log Likelihood: 59.3876\tSigma2 Prior: -599.9343\tRegularization: 0.0010\n",
            "Iter: 13990  \tTraining Loss: -559.3026    \n",
            "    Negative Log Likelihood: 57.2281\tSigma2 Prior: -616.5317\tRegularization: 0.0010\n",
            "Iter: 14000  \tTraining Loss: -552.2026    \n",
            "    Negative Log Likelihood: 58.5340\tSigma2 Prior: -610.7375\tRegularization: 0.0010\n",
            "Iter: 14010  \tTraining Loss: -539.7288    \n",
            "    Negative Log Likelihood: 60.6411\tSigma2 Prior: -600.3708\tRegularization: 0.0010\n",
            "Iter: 14020  \tTraining Loss: -525.0613    \n",
            "    Negative Log Likelihood: 62.2195\tSigma2 Prior: -587.2818\tRegularization: 0.0010\n",
            "Iter: 14030  \tTraining Loss: -556.9355    \n",
            "    Negative Log Likelihood: 59.0775\tSigma2 Prior: -616.0140\tRegularization: 0.0010\n",
            "Iter: 14040  \tTraining Loss: -547.2863    \n",
            "    Negative Log Likelihood: 59.7307\tSigma2 Prior: -607.0179\tRegularization: 0.0010\n",
            "Iter: 14050  \tTraining Loss: -528.8821    \n",
            "    Negative Log Likelihood: 62.0852\tSigma2 Prior: -590.9684\tRegularization: 0.0010\n",
            "Iter: 14060  \tTraining Loss: -548.3523    \n",
            "    Negative Log Likelihood: 59.0531\tSigma2 Prior: -607.4064\tRegularization: 0.0010\n",
            "Iter: 14070  \tTraining Loss: -530.6025    \n",
            "    Negative Log Likelihood: 61.9377\tSigma2 Prior: -592.5411\tRegularization: 0.0010\n",
            "Iter: 14080  \tTraining Loss: -510.2165    \n",
            "    Negative Log Likelihood: 63.6343\tSigma2 Prior: -573.8518\tRegularization: 0.0010\n",
            "Iter: 14090  \tTraining Loss: -531.9177    \n",
            "    Negative Log Likelihood: 60.9309\tSigma2 Prior: -592.8495\tRegularization: 0.0010\n",
            "Iter: 14100  \tTraining Loss: -537.4109    \n",
            "    Negative Log Likelihood: 61.5452\tSigma2 Prior: -598.9571\tRegularization: 0.0010\n",
            "Iter: 14110  \tTraining Loss: -543.5977    \n",
            "    Negative Log Likelihood: 59.2339\tSigma2 Prior: -602.8325\tRegularization: 0.0010\n",
            "Iter: 14120  \tTraining Loss: -546.3199    \n",
            "    Negative Log Likelihood: 60.8965\tSigma2 Prior: -607.2174\tRegularization: 0.0010\n",
            "Iter: 14130  \tTraining Loss: -540.8447    \n",
            "    Negative Log Likelihood: 59.7582\tSigma2 Prior: -600.6039\tRegularization: 0.0010\n",
            "Iter: 14140  \tTraining Loss: -546.5778    \n",
            "    Negative Log Likelihood: 58.8773\tSigma2 Prior: -605.4561\tRegularization: 0.0010\n",
            "Iter: 14150  \tTraining Loss: -556.0538    \n",
            "    Negative Log Likelihood: 57.9074\tSigma2 Prior: -613.9622\tRegularization: 0.0010\n",
            "Iter: 14160  \tTraining Loss: -548.4221    \n",
            "    Negative Log Likelihood: 60.1699\tSigma2 Prior: -608.5930\tRegularization: 0.0010\n",
            "Iter: 14170  \tTraining Loss: -550.1790    \n",
            "    Negative Log Likelihood: 60.3597\tSigma2 Prior: -610.5397\tRegularization: 0.0010\n",
            "Iter: 14180  \tTraining Loss: -523.7355    \n",
            "    Negative Log Likelihood: 61.9383\tSigma2 Prior: -585.6747\tRegularization: 0.0010\n",
            "Iter: 14190  \tTraining Loss: -543.2837    \n",
            "    Negative Log Likelihood: 60.0190\tSigma2 Prior: -603.3036\tRegularization: 0.0010\n",
            "Iter: 14200  \tTraining Loss: -535.8895    \n",
            "    Negative Log Likelihood: 61.0395\tSigma2 Prior: -596.9299\tRegularization: 0.0010\n",
            "Iter: 14210  \tTraining Loss: -558.4370    \n",
            "    Negative Log Likelihood: 58.8428\tSigma2 Prior: -617.2808\tRegularization: 0.0010\n",
            "Iter: 14220  \tTraining Loss: -535.7299    \n",
            "    Negative Log Likelihood: 61.0370\tSigma2 Prior: -596.7679\tRegularization: 0.0010\n",
            "Iter: 14230  \tTraining Loss: -555.3088    \n",
            "    Negative Log Likelihood: 58.0699\tSigma2 Prior: -613.3798\tRegularization: 0.0010\n",
            "Iter: 14240  \tTraining Loss: -520.7824    \n",
            "    Negative Log Likelihood: 62.9831\tSigma2 Prior: -583.7665\tRegularization: 0.0010\n",
            "Iter: 14250  \tTraining Loss: -533.7872    \n",
            "    Negative Log Likelihood: 60.6913\tSigma2 Prior: -594.4796\tRegularization: 0.0010\n",
            "Iter: 14260  \tTraining Loss: -534.6147    \n",
            "    Negative Log Likelihood: 60.6784\tSigma2 Prior: -595.2941\tRegularization: 0.0010\n",
            "Iter: 14270  \tTraining Loss: -539.9623    \n",
            "    Negative Log Likelihood: 59.7189\tSigma2 Prior: -599.6823\tRegularization: 0.0010\n",
            "Iter: 14280  \tTraining Loss: -548.0966    \n",
            "    Negative Log Likelihood: 58.4817\tSigma2 Prior: -606.5793\tRegularization: 0.0010\n",
            "Iter: 14290  \tTraining Loss: -524.0614    \n",
            "    Negative Log Likelihood: 60.7943\tSigma2 Prior: -584.8568\tRegularization: 0.0010\n",
            "Iter: 14300  \tTraining Loss: -550.6097    \n",
            "    Negative Log Likelihood: 58.8785\tSigma2 Prior: -609.4893\tRegularization: 0.0010\n",
            "Iter: 14310  \tTraining Loss: -543.5515    \n",
            "    Negative Log Likelihood: 59.2030\tSigma2 Prior: -602.7555\tRegularization: 0.0010\n",
            "Iter: 14320  \tTraining Loss: -527.8229    \n",
            "    Negative Log Likelihood: 61.2811\tSigma2 Prior: -589.1050\tRegularization: 0.0010\n",
            "Iter: 14330  \tTraining Loss: -524.8165    \n",
            "    Negative Log Likelihood: 60.7827\tSigma2 Prior: -585.6002\tRegularization: 0.0010\n",
            "Iter: 14340  \tTraining Loss: -540.9013    \n",
            "    Negative Log Likelihood: 59.7361\tSigma2 Prior: -600.6385\tRegularization: 0.0010\n",
            "Iter: 14350  \tTraining Loss: -544.4812    \n",
            "    Negative Log Likelihood: 59.4565\tSigma2 Prior: -603.9387\tRegularization: 0.0010\n",
            "Iter: 14360  \tTraining Loss: -533.6025    \n",
            "    Negative Log Likelihood: 59.7522\tSigma2 Prior: -593.3557\tRegularization: 0.0010\n",
            "Iter: 14370  \tTraining Loss: -530.8354    \n",
            "    Negative Log Likelihood: 60.9143\tSigma2 Prior: -591.7507\tRegularization: 0.0010\n",
            "Iter: 14380  \tTraining Loss: -533.7416    \n",
            "    Negative Log Likelihood: 61.2550\tSigma2 Prior: -594.9976\tRegularization: 0.0010\n",
            "Iter: 14390  \tTraining Loss: -551.2094    \n",
            "    Negative Log Likelihood: 57.2092\tSigma2 Prior: -608.4196\tRegularization: 0.0010\n",
            "Iter: 14400  \tTraining Loss: -538.3141    \n",
            "    Negative Log Likelihood: 60.0644\tSigma2 Prior: -598.3796\tRegularization: 0.0010\n",
            "Iter: 14410  \tTraining Loss: -537.6603    \n",
            "    Negative Log Likelihood: 60.9675\tSigma2 Prior: -598.6289\tRegularization: 0.0010\n",
            "Iter: 14420  \tTraining Loss: -559.6227    \n",
            "    Negative Log Likelihood: 57.1250\tSigma2 Prior: -616.7488\tRegularization: 0.0010\n",
            "Iter: 14430  \tTraining Loss: -532.1290    \n",
            "    Negative Log Likelihood: 60.6618\tSigma2 Prior: -592.7918\tRegularization: 0.0010\n",
            "Iter: 14440  \tTraining Loss: -517.7436    \n",
            "    Negative Log Likelihood: 62.8930\tSigma2 Prior: -580.6376\tRegularization: 0.0010\n",
            "Iter: 14450  \tTraining Loss: -542.7220    \n",
            "    Negative Log Likelihood: 60.3292\tSigma2 Prior: -603.0522\tRegularization: 0.0010\n",
            "Iter: 14460  \tTraining Loss: -543.7820    \n",
            "    Negative Log Likelihood: 59.9850\tSigma2 Prior: -603.7681\tRegularization: 0.0010\n",
            "Iter: 14470  \tTraining Loss: -543.6824    \n",
            "    Negative Log Likelihood: 59.6511\tSigma2 Prior: -603.3345\tRegularization: 0.0010\n",
            "Iter: 14480  \tTraining Loss: -544.3705    \n",
            "    Negative Log Likelihood: 59.1997\tSigma2 Prior: -603.5713\tRegularization: 0.0010\n",
            "Iter: 14490  \tTraining Loss: -537.9793    \n",
            "    Negative Log Likelihood: 60.1770\tSigma2 Prior: -598.1573\tRegularization: 0.0010\n",
            "Iter: 14500  \tTraining Loss: -550.5173    \n",
            "    Negative Log Likelihood: 58.2965\tSigma2 Prior: -608.8149\tRegularization: 0.0010\n",
            "Iter: 14510  \tTraining Loss: -542.0467    \n",
            "    Negative Log Likelihood: 60.3049\tSigma2 Prior: -602.3527\tRegularization: 0.0010\n",
            "Iter: 14520  \tTraining Loss: -550.2436    \n",
            "    Negative Log Likelihood: 60.2596\tSigma2 Prior: -610.5042\tRegularization: 0.0010\n",
            "Iter: 14530  \tTraining Loss: -526.6528    \n",
            "    Negative Log Likelihood: 62.9556\tSigma2 Prior: -589.6094\tRegularization: 0.0010\n",
            "Iter: 14540  \tTraining Loss: -506.9767    \n",
            "    Negative Log Likelihood: 65.0376\tSigma2 Prior: -572.0153\tRegularization: 0.0010\n",
            "Iter: 14550  \tTraining Loss: -541.4150    \n",
            "    Negative Log Likelihood: 61.2077\tSigma2 Prior: -602.6238\tRegularization: 0.0010\n",
            "Iter: 14560  \tTraining Loss: -527.2006    \n",
            "    Negative Log Likelihood: 62.4187\tSigma2 Prior: -589.6204\tRegularization: 0.0010\n",
            "Iter: 14570  \tTraining Loss: -501.5011    \n",
            "    Negative Log Likelihood: 64.4367\tSigma2 Prior: -565.9388\tRegularization: 0.0010\n",
            "Iter: 14580  \tTraining Loss: -551.8165    \n",
            "    Negative Log Likelihood: 58.9970\tSigma2 Prior: -610.8145\tRegularization: 0.0010\n",
            "Iter: 14590  \tTraining Loss: -538.2539    \n",
            "    Negative Log Likelihood: 59.7338\tSigma2 Prior: -597.9887\tRegularization: 0.0010\n",
            "Iter: 14600  \tTraining Loss: -546.0825    \n",
            "    Negative Log Likelihood: 59.5136\tSigma2 Prior: -605.5971\tRegularization: 0.0010\n",
            "Iter: 14610  \tTraining Loss: -560.4078    \n",
            "    Negative Log Likelihood: 58.7885\tSigma2 Prior: -619.1973\tRegularization: 0.0010\n",
            "Iter: 14620  \tTraining Loss: -551.4323    \n",
            "    Negative Log Likelihood: 59.2330\tSigma2 Prior: -610.6664\tRegularization: 0.0010\n",
            "Iter: 14630  \tTraining Loss: -538.6067    \n",
            "    Negative Log Likelihood: 59.8799\tSigma2 Prior: -598.4876\tRegularization: 0.0010\n",
            "Iter: 14640  \tTraining Loss: -533.2350    \n",
            "    Negative Log Likelihood: 62.1486\tSigma2 Prior: -595.3846\tRegularization: 0.0010\n",
            "Iter: 14650  \tTraining Loss: -540.0369    \n",
            "    Negative Log Likelihood: 61.2057\tSigma2 Prior: -601.2437\tRegularization: 0.0010\n",
            "Iter: 14660  \tTraining Loss: -555.0438    \n",
            "    Negative Log Likelihood: 59.1126\tSigma2 Prior: -614.1574\tRegularization: 0.0010\n",
            "Iter: 14670  \tTraining Loss: -546.7789    \n",
            "    Negative Log Likelihood: 58.9028\tSigma2 Prior: -605.6827\tRegularization: 0.0010\n",
            "Iter: 14680  \tTraining Loss: -549.3760    \n",
            "    Negative Log Likelihood: 58.6902\tSigma2 Prior: -608.0673\tRegularization: 0.0010\n",
            "Iter: 14690  \tTraining Loss: -539.6742    \n",
            "    Negative Log Likelihood: 58.9447\tSigma2 Prior: -598.6199\tRegularization: 0.0010\n",
            "Iter: 14700  \tTraining Loss: -535.4584    \n",
            "    Negative Log Likelihood: 59.2989\tSigma2 Prior: -594.7583\tRegularization: 0.0010\n",
            "Iter: 14710  \tTraining Loss: -547.0210    \n",
            "    Negative Log Likelihood: 59.3448\tSigma2 Prior: -606.3668\tRegularization: 0.0010\n",
            "Iter: 14720  \tTraining Loss: -549.8203    \n",
            "    Negative Log Likelihood: 58.3524\tSigma2 Prior: -608.1738\tRegularization: 0.0010\n",
            "Iter: 14730  \tTraining Loss: -549.1386    \n",
            "    Negative Log Likelihood: 60.5137\tSigma2 Prior: -609.6533\tRegularization: 0.0010\n",
            "Iter: 14740  \tTraining Loss: -529.5408    \n",
            "    Negative Log Likelihood: 60.9454\tSigma2 Prior: -590.4872\tRegularization: 0.0010\n",
            "Iter: 14750  \tTraining Loss: -537.1956    \n",
            "    Negative Log Likelihood: 61.2025\tSigma2 Prior: -598.3992\tRegularization: 0.0010\n",
            "Iter: 14760  \tTraining Loss: -535.1940    \n",
            "    Negative Log Likelihood: 61.9732\tSigma2 Prior: -597.1683\tRegularization: 0.0010\n",
            "Iter: 14770  \tTraining Loss: -502.1288    \n",
            "    Negative Log Likelihood: 64.8833\tSigma2 Prior: -567.0131\tRegularization: 0.0010\n",
            "Iter: 14780  \tTraining Loss: -538.0854    \n",
            "    Negative Log Likelihood: 60.7240\tSigma2 Prior: -598.8105\tRegularization: 0.0010\n",
            "Iter: 14790  \tTraining Loss: -537.0818    \n",
            "    Negative Log Likelihood: 60.3389\tSigma2 Prior: -597.4218\tRegularization: 0.0010\n",
            "Iter: 14800  \tTraining Loss: -533.5377    \n",
            "    Negative Log Likelihood: 59.7257\tSigma2 Prior: -593.2644\tRegularization: 0.0010\n",
            "Iter: 14810  \tTraining Loss: -547.1180    \n",
            "    Negative Log Likelihood: 57.9506\tSigma2 Prior: -605.0697\tRegularization: 0.0010\n",
            "Iter: 14820  \tTraining Loss: -553.7360    \n",
            "    Negative Log Likelihood: 58.3891\tSigma2 Prior: -612.1262\tRegularization: 0.0010\n",
            "Iter: 14830  \tTraining Loss: -539.7123    \n",
            "    Negative Log Likelihood: 57.8566\tSigma2 Prior: -597.5699\tRegularization: 0.0010\n",
            "Iter: 14840  \tTraining Loss: -518.0646    \n",
            "    Negative Log Likelihood: 62.0828\tSigma2 Prior: -580.1484\tRegularization: 0.0010\n",
            "Iter: 14850  \tTraining Loss: -543.9380    \n",
            "    Negative Log Likelihood: 59.5493\tSigma2 Prior: -603.4884\tRegularization: 0.0010\n",
            "Iter: 14860  \tTraining Loss: -528.5752    \n",
            "    Negative Log Likelihood: 62.2969\tSigma2 Prior: -590.8732\tRegularization: 0.0010\n",
            "Iter: 14870  \tTraining Loss: -533.2070    \n",
            "    Negative Log Likelihood: 61.3050\tSigma2 Prior: -594.5129\tRegularization: 0.0010\n",
            "Iter: 14880  \tTraining Loss: -537.8560    \n",
            "    Negative Log Likelihood: 59.5488\tSigma2 Prior: -597.4058\tRegularization: 0.0010\n",
            "Iter: 14890  \tTraining Loss: -536.8474    \n",
            "    Negative Log Likelihood: 60.2941\tSigma2 Prior: -597.1425\tRegularization: 0.0010\n",
            "Iter: 14900  \tTraining Loss: -542.0256    \n",
            "    Negative Log Likelihood: 58.8580\tSigma2 Prior: -600.8846\tRegularization: 0.0010\n",
            "Iter: 14910  \tTraining Loss: -554.0276    \n",
            "    Negative Log Likelihood: 57.2673\tSigma2 Prior: -611.2959\tRegularization: 0.0010\n",
            "Iter: 14920  \tTraining Loss: -553.2324    \n",
            "    Negative Log Likelihood: 56.7593\tSigma2 Prior: -609.9927\tRegularization: 0.0010\n",
            "Iter: 14930  \tTraining Loss: -529.0963    \n",
            "    Negative Log Likelihood: 61.5144\tSigma2 Prior: -590.6117\tRegularization: 0.0010\n",
            "Iter: 14940  \tTraining Loss: -549.7443    \n",
            "    Negative Log Likelihood: 58.7808\tSigma2 Prior: -608.5261\tRegularization: 0.0010\n",
            "Iter: 14950  \tTraining Loss: -539.5642    \n",
            "    Negative Log Likelihood: 59.6717\tSigma2 Prior: -599.2369\tRegularization: 0.0010\n",
            "Iter: 14960  \tTraining Loss: -531.0582    \n",
            "    Negative Log Likelihood: 61.4217\tSigma2 Prior: -592.4810\tRegularization: 0.0010\n",
            "Iter: 14970  \tTraining Loss: -526.4777    \n",
            "    Negative Log Likelihood: 60.4812\tSigma2 Prior: -586.9600\tRegularization: 0.0010\n",
            "Iter: 14980  \tTraining Loss: -539.2552    \n",
            "    Negative Log Likelihood: 58.2916\tSigma2 Prior: -597.5479\tRegularization: 0.0010\n",
            "Iter: 14990  \tTraining Loss: -547.8499    \n",
            "    Negative Log Likelihood: 59.1425\tSigma2 Prior: -606.9934\tRegularization: 0.0010\n",
            "Iter: 15000  \tTraining Loss: -544.7811    \n",
            "    Negative Log Likelihood: 58.3277\tSigma2 Prior: -603.1099\tRegularization: 0.0010\n",
            "Iter: 15010  \tTraining Loss: -538.3109    \n",
            "    Negative Log Likelihood: 59.4146\tSigma2 Prior: -597.7265\tRegularization: 0.0010\n",
            "Iter: 15020  \tTraining Loss: -549.6794    \n",
            "    Negative Log Likelihood: 56.6336\tSigma2 Prior: -606.3141\tRegularization: 0.0010\n",
            "Iter: 15030  \tTraining Loss: -549.7019    \n",
            "    Negative Log Likelihood: 57.2111\tSigma2 Prior: -606.9140\tRegularization: 0.0010\n",
            "Iter: 15040  \tTraining Loss: -535.2290    \n",
            "    Negative Log Likelihood: 58.5728\tSigma2 Prior: -593.8028\tRegularization: 0.0010\n",
            "Iter: 15050  \tTraining Loss: -556.1111    \n",
            "    Negative Log Likelihood: 57.6501\tSigma2 Prior: -613.7623\tRegularization: 0.0010\n",
            "Iter: 15060  \tTraining Loss: -512.4432    \n",
            "    Negative Log Likelihood: 61.6159\tSigma2 Prior: -574.0602\tRegularization: 0.0010\n",
            "Iter: 15070  \tTraining Loss: -542.8113    \n",
            "    Negative Log Likelihood: 59.0158\tSigma2 Prior: -601.8281\tRegularization: 0.0010\n",
            "Iter: 15080  \tTraining Loss: -536.3671    \n",
            "    Negative Log Likelihood: 60.0377\tSigma2 Prior: -596.4058\tRegularization: 0.0010\n",
            "Iter: 15090  \tTraining Loss: -542.0344    \n",
            "    Negative Log Likelihood: 59.1681\tSigma2 Prior: -601.2035\tRegularization: 0.0010\n",
            "Iter: 15100  \tTraining Loss: -545.1426    \n",
            "    Negative Log Likelihood: 58.9921\tSigma2 Prior: -604.1358\tRegularization: 0.0010\n",
            "Iter: 15110  \tTraining Loss: -558.2189    \n",
            "    Negative Log Likelihood: 56.7025\tSigma2 Prior: -614.9224\tRegularization: 0.0010\n",
            "Iter: 15120  \tTraining Loss: -525.7900    \n",
            "    Negative Log Likelihood: 60.6262\tSigma2 Prior: -586.4173\tRegularization: 0.0010\n",
            "Iter: 15130  \tTraining Loss: -545.4443    \n",
            "    Negative Log Likelihood: 58.9436\tSigma2 Prior: -604.3889\tRegularization: 0.0010\n",
            "Iter: 15140  \tTraining Loss: -542.6993    \n",
            "    Negative Log Likelihood: 59.7929\tSigma2 Prior: -602.4933\tRegularization: 0.0010\n",
            "Iter: 15150  \tTraining Loss: -539.7068    \n",
            "    Negative Log Likelihood: 60.3547\tSigma2 Prior: -600.0626\tRegularization: 0.0010\n",
            "Iter: 15160  \tTraining Loss: -541.9324    \n",
            "    Negative Log Likelihood: 60.4032\tSigma2 Prior: -602.3367\tRegularization: 0.0010\n",
            "Iter: 15170  \tTraining Loss: -557.3710    \n",
            "    Negative Log Likelihood: 57.2071\tSigma2 Prior: -614.5792\tRegularization: 0.0010\n",
            "Iter: 15180  \tTraining Loss: -535.9231    \n",
            "    Negative Log Likelihood: 59.0659\tSigma2 Prior: -594.9901\tRegularization: 0.0010\n",
            "Iter: 15190  \tTraining Loss: -541.9503    \n",
            "    Negative Log Likelihood: 57.8925\tSigma2 Prior: -599.8439\tRegularization: 0.0010\n",
            "Iter: 15200  \tTraining Loss: -528.5917    \n",
            "    Negative Log Likelihood: 59.2500\tSigma2 Prior: -587.8427\tRegularization: 0.0010\n",
            "Iter: 15210  \tTraining Loss: -537.3676    \n",
            "    Negative Log Likelihood: 59.2497\tSigma2 Prior: -596.6183\tRegularization: 0.0010\n",
            "Iter: 15220  \tTraining Loss: -548.8675    \n",
            "    Negative Log Likelihood: 58.4336\tSigma2 Prior: -607.3022\tRegularization: 0.0010\n",
            "Iter: 15230  \tTraining Loss: -553.4627    \n",
            "    Negative Log Likelihood: 57.1819\tSigma2 Prior: -610.6456\tRegularization: 0.0010\n",
            "Iter: 15240  \tTraining Loss: -527.7043    \n",
            "    Negative Log Likelihood: 60.1292\tSigma2 Prior: -587.8345\tRegularization: 0.0010\n",
            "Iter: 15250  \tTraining Loss: -542.4225    \n",
            "    Negative Log Likelihood: 58.3107\tSigma2 Prior: -600.7343\tRegularization: 0.0010\n",
            "Iter: 15260  \tTraining Loss: -533.9736    \n",
            "    Negative Log Likelihood: 59.7323\tSigma2 Prior: -593.7070\tRegularization: 0.0010\n",
            "Iter: 15270  \tTraining Loss: -554.2386    \n",
            "    Negative Log Likelihood: 57.0757\tSigma2 Prior: -611.3153\tRegularization: 0.0010\n",
            "Iter: 15280  \tTraining Loss: -538.3083    \n",
            "    Negative Log Likelihood: 59.9984\tSigma2 Prior: -598.3078\tRegularization: 0.0010\n",
            "Iter: 15290  \tTraining Loss: -532.4492    \n",
            "    Negative Log Likelihood: 61.5778\tSigma2 Prior: -594.0281\tRegularization: 0.0010\n",
            "Iter: 15300  \tTraining Loss: -545.0746    \n",
            "    Negative Log Likelihood: 60.2881\tSigma2 Prior: -605.3636\tRegularization: 0.0010\n",
            "Iter: 15310  \tTraining Loss: -530.4108    \n",
            "    Negative Log Likelihood: 61.3136\tSigma2 Prior: -591.7255\tRegularization: 0.0010\n",
            "Iter: 15320  \tTraining Loss: -550.6015    \n",
            "    Negative Log Likelihood: 58.8554\tSigma2 Prior: -609.4579\tRegularization: 0.0010\n",
            "Iter: 15330  \tTraining Loss: -546.1414    \n",
            "    Negative Log Likelihood: 59.1690\tSigma2 Prior: -605.3115\tRegularization: 0.0010\n",
            "Iter: 15340  \tTraining Loss: -548.8460    \n",
            "    Negative Log Likelihood: 59.2394\tSigma2 Prior: -608.0864\tRegularization: 0.0010\n",
            "Iter: 15350  \tTraining Loss: -550.2622    \n",
            "    Negative Log Likelihood: 57.8729\tSigma2 Prior: -608.1362\tRegularization: 0.0010\n",
            "Iter: 15360  \tTraining Loss: -545.1160    \n",
            "    Negative Log Likelihood: 59.6953\tSigma2 Prior: -604.8123\tRegularization: 0.0010\n",
            "Iter: 15370  \tTraining Loss: -532.7963    \n",
            "    Negative Log Likelihood: 59.5127\tSigma2 Prior: -592.3101\tRegularization: 0.0010\n",
            "Iter: 15380  \tTraining Loss: -537.3562    \n",
            "    Negative Log Likelihood: 59.5224\tSigma2 Prior: -596.8796\tRegularization: 0.0010\n",
            "Iter: 15390  \tTraining Loss: -550.7363    \n",
            "    Negative Log Likelihood: 59.4814\tSigma2 Prior: -610.2188\tRegularization: 0.0010\n",
            "Iter: 15400  \tTraining Loss: -534.7809    \n",
            "    Negative Log Likelihood: 60.3220\tSigma2 Prior: -595.1039\tRegularization: 0.0010\n",
            "Iter: 15410  \tTraining Loss: -554.7979    \n",
            "    Negative Log Likelihood: 58.2439\tSigma2 Prior: -613.0428\tRegularization: 0.0010\n",
            "Iter: 15420  \tTraining Loss: -556.6064    \n",
            "    Negative Log Likelihood: 58.1539\tSigma2 Prior: -614.7614\tRegularization: 0.0010\n",
            "Iter: 15430  \tTraining Loss: -548.7820    \n",
            "    Negative Log Likelihood: 59.0849\tSigma2 Prior: -607.8679\tRegularization: 0.0010\n",
            "Iter: 15440  \tTraining Loss: -544.7851    \n",
            "    Negative Log Likelihood: 59.5984\tSigma2 Prior: -604.3845\tRegularization: 0.0010\n",
            "Iter: 15450  \tTraining Loss: -542.3987    \n",
            "    Negative Log Likelihood: 60.3189\tSigma2 Prior: -602.7186\tRegularization: 0.0010\n",
            "Iter: 15460  \tTraining Loss: -536.6916    \n",
            "    Negative Log Likelihood: 61.7432\tSigma2 Prior: -598.4358\tRegularization: 0.0010\n",
            "Iter: 15470  \tTraining Loss: -500.7540    \n",
            "    Negative Log Likelihood: 63.9557\tSigma2 Prior: -564.7108\tRegularization: 0.0010\n",
            "Iter: 15480  \tTraining Loss: -526.9834    \n",
            "    Negative Log Likelihood: 61.7435\tSigma2 Prior: -588.7280\tRegularization: 0.0010\n",
            "Iter: 15490  \tTraining Loss: -551.6117    \n",
            "    Negative Log Likelihood: 58.0881\tSigma2 Prior: -609.7008\tRegularization: 0.0010\n",
            "Iter: 15500  \tTraining Loss: -544.7966    \n",
            "    Negative Log Likelihood: 59.9545\tSigma2 Prior: -604.7522\tRegularization: 0.0010\n",
            "Iter: 15510  \tTraining Loss: -539.5666    \n",
            "    Negative Log Likelihood: 59.7905\tSigma2 Prior: -599.3581\tRegularization: 0.0010\n",
            "Iter: 15520  \tTraining Loss: -553.1772    \n",
            "    Negative Log Likelihood: 58.0002\tSigma2 Prior: -611.1785\tRegularization: 0.0010\n",
            "Iter: 15530  \tTraining Loss: -537.4288    \n",
            "    Negative Log Likelihood: 59.8544\tSigma2 Prior: -597.2842\tRegularization: 0.0010\n",
            "Iter: 15540  \tTraining Loss: -546.0083    \n",
            "    Negative Log Likelihood: 57.6265\tSigma2 Prior: -603.6358\tRegularization: 0.0010\n",
            "Iter: 15550  \tTraining Loss: -523.2233    \n",
            "    Negative Log Likelihood: 60.1709\tSigma2 Prior: -583.3953\tRegularization: 0.0010\n",
            "Iter: 15560  \tTraining Loss: -523.3388    \n",
            "    Negative Log Likelihood: 60.7030\tSigma2 Prior: -584.0428\tRegularization: 0.0010\n",
            "Iter: 15570  \tTraining Loss: -522.4330    \n",
            "    Negative Log Likelihood: 59.4361\tSigma2 Prior: -581.8701\tRegularization: 0.0010\n",
            "Iter: 15580  \tTraining Loss: -549.9337    \n",
            "    Negative Log Likelihood: 58.5140\tSigma2 Prior: -608.4487\tRegularization: 0.0010\n",
            "Iter: 15590  \tTraining Loss: -544.0617    \n",
            "    Negative Log Likelihood: 59.5318\tSigma2 Prior: -603.5945\tRegularization: 0.0010\n",
            "Iter: 15600  \tTraining Loss: -550.2534    \n",
            "    Negative Log Likelihood: 57.9728\tSigma2 Prior: -608.2272\tRegularization: 0.0010\n",
            "Iter: 15610  \tTraining Loss: -538.8097    \n",
            "    Negative Log Likelihood: 60.0096\tSigma2 Prior: -598.8203\tRegularization: 0.0010\n",
            "Iter: 15620  \tTraining Loss: -532.9769    \n",
            "    Negative Log Likelihood: 60.2339\tSigma2 Prior: -593.2118\tRegularization: 0.0010\n",
            "Iter: 15630  \tTraining Loss: -542.0002    \n",
            "    Negative Log Likelihood: 58.1771\tSigma2 Prior: -600.1783\tRegularization: 0.0010\n",
            "Iter: 15640  \tTraining Loss: -536.5095    \n",
            "    Negative Log Likelihood: 59.3002\tSigma2 Prior: -595.8107\tRegularization: 0.0010\n",
            "Iter: 15650  \tTraining Loss: -536.5260    \n",
            "    Negative Log Likelihood: 59.7578\tSigma2 Prior: -596.2849\tRegularization: 0.0010\n",
            "Iter: 15660  \tTraining Loss: -527.2291    \n",
            "    Negative Log Likelihood: 60.7346\tSigma2 Prior: -587.9647\tRegularization: 0.0010\n",
            "Iter: 15670  \tTraining Loss: -538.9648    \n",
            "    Negative Log Likelihood: 58.9629\tSigma2 Prior: -597.9287\tRegularization: 0.0010\n",
            "Iter: 15680  \tTraining Loss: -544.3148    \n",
            "    Negative Log Likelihood: 57.2711\tSigma2 Prior: -601.5870\tRegularization: 0.0010\n",
            "Iter: 15690  \tTraining Loss: -527.4188    \n",
            "    Negative Log Likelihood: 59.5782\tSigma2 Prior: -586.9980\tRegularization: 0.0010\n",
            "Iter: 15700  \tTraining Loss: -530.8147    \n",
            "    Negative Log Likelihood: 58.7597\tSigma2 Prior: -589.5754\tRegularization: 0.0010\n",
            "Iter: 15710  \tTraining Loss: -524.3583    \n",
            "    Negative Log Likelihood: 58.1015\tSigma2 Prior: -582.4608\tRegularization: 0.0010\n",
            "Iter: 15720  \tTraining Loss: -553.5546    \n",
            "    Negative Log Likelihood: 55.1945\tSigma2 Prior: -608.7502\tRegularization: 0.0010\n",
            "Iter: 15730  \tTraining Loss: -538.7330    \n",
            "    Negative Log Likelihood: 59.0288\tSigma2 Prior: -597.7629\tRegularization: 0.0010\n",
            "Iter: 15740  \tTraining Loss: -540.2604    \n",
            "    Negative Log Likelihood: 57.0684\tSigma2 Prior: -597.3298\tRegularization: 0.0010\n",
            "Iter: 15750  \tTraining Loss: -525.6288    \n",
            "    Negative Log Likelihood: 58.7924\tSigma2 Prior: -584.4223\tRegularization: 0.0010\n",
            "Iter: 15760  \tTraining Loss: -532.2448    \n",
            "    Negative Log Likelihood: 59.9309\tSigma2 Prior: -592.1768\tRegularization: 0.0010\n",
            "Iter: 15770  \tTraining Loss: -541.4099    \n",
            "    Negative Log Likelihood: 57.7613\tSigma2 Prior: -599.1722\tRegularization: 0.0010\n",
            "Iter: 15780  \tTraining Loss: -550.9873    \n",
            "    Negative Log Likelihood: 57.4729\tSigma2 Prior: -608.4612\tRegularization: 0.0010\n",
            "Iter: 15790  \tTraining Loss: -531.1998    \n",
            "    Negative Log Likelihood: 59.0642\tSigma2 Prior: -590.2651\tRegularization: 0.0010\n",
            "Iter: 15800  \tTraining Loss: -504.5286    \n",
            "    Negative Log Likelihood: 60.5356\tSigma2 Prior: -565.0653\tRegularization: 0.0010\n",
            "Iter: 15810  \tTraining Loss: -543.6477    \n",
            "    Negative Log Likelihood: 57.3443\tSigma2 Prior: -600.9931\tRegularization: 0.0010\n",
            "Iter: 15820  \tTraining Loss: -525.3353    \n",
            "    Negative Log Likelihood: 58.6724\tSigma2 Prior: -584.0087\tRegularization: 0.0010\n",
            "Iter: 15830  \tTraining Loss: -543.1887    \n",
            "    Negative Log Likelihood: 58.6701\tSigma2 Prior: -601.8599\tRegularization: 0.0010\n",
            "Iter: 15840  \tTraining Loss: -547.5805    \n",
            "    Negative Log Likelihood: 57.5332\tSigma2 Prior: -605.1147\tRegularization: 0.0010\n",
            "Iter: 15850  \tTraining Loss: -552.4222    \n",
            "    Negative Log Likelihood: 57.2195\tSigma2 Prior: -609.6428\tRegularization: 0.0010\n",
            "Iter: 15860  \tTraining Loss: -507.4932    \n",
            "    Negative Log Likelihood: 62.4119\tSigma2 Prior: -569.9062\tRegularization: 0.0010\n",
            "Iter: 15870  \tTraining Loss: -529.4613    \n",
            "    Negative Log Likelihood: 60.4479\tSigma2 Prior: -589.9102\tRegularization: 0.0010\n",
            "Iter: 15880  \tTraining Loss: -539.0403    \n",
            "    Negative Log Likelihood: 59.1624\tSigma2 Prior: -598.2038\tRegularization: 0.0010\n",
            "Iter: 15890  \tTraining Loss: -544.1241    \n",
            "    Negative Log Likelihood: 59.1268\tSigma2 Prior: -603.2520\tRegularization: 0.0010\n",
            "Iter: 15900  \tTraining Loss: -544.9568    \n",
            "    Negative Log Likelihood: 58.7040\tSigma2 Prior: -603.6619\tRegularization: 0.0010\n",
            "Iter: 15910  \tTraining Loss: -550.6602    \n",
            "    Negative Log Likelihood: 57.4606\tSigma2 Prior: -608.1219\tRegularization: 0.0010\n",
            "Iter: 15920  \tTraining Loss: -554.1919    \n",
            "    Negative Log Likelihood: 56.5835\tSigma2 Prior: -610.7764\tRegularization: 0.0010\n",
            "Iter: 15930  \tTraining Loss: -540.6912    \n",
            "    Negative Log Likelihood: 59.7931\tSigma2 Prior: -600.4854\tRegularization: 0.0010\n",
            "Iter: 15940  \tTraining Loss: -552.9144    \n",
            "    Negative Log Likelihood: 58.1166\tSigma2 Prior: -611.0320\tRegularization: 0.0010\n",
            "Iter: 15950  \tTraining Loss: -553.4224    \n",
            "    Negative Log Likelihood: 57.8220\tSigma2 Prior: -611.2455\tRegularization: 0.0010\n",
            "Iter: 15960  \tTraining Loss: -549.4401    \n",
            "    Negative Log Likelihood: 57.7909\tSigma2 Prior: -607.2320\tRegularization: 0.0010\n",
            "Iter: 15970  \tTraining Loss: -504.2697    \n",
            "    Negative Log Likelihood: 63.4720\tSigma2 Prior: -567.7427\tRegularization: 0.0010\n",
            "Iter: 15980  \tTraining Loss: -546.3977    \n",
            "    Negative Log Likelihood: 59.4277\tSigma2 Prior: -605.8264\tRegularization: 0.0010\n",
            "Iter: 15990  \tTraining Loss: -554.2571    \n",
            "    Negative Log Likelihood: 57.9139\tSigma2 Prior: -612.1720\tRegularization: 0.0010\n",
            "Iter: 16000  \tTraining Loss: -530.3986    \n",
            "    Negative Log Likelihood: 61.1828\tSigma2 Prior: -591.5825\tRegularization: 0.0010\n",
            "Iter: 16010  \tTraining Loss: -546.7846    \n",
            "    Negative Log Likelihood: 58.7020\tSigma2 Prior: -605.4876\tRegularization: 0.0010\n",
            "Iter: 16020  \tTraining Loss: -541.2483    \n",
            "    Negative Log Likelihood: 59.5208\tSigma2 Prior: -600.7701\tRegularization: 0.0010\n",
            "Iter: 16030  \tTraining Loss: -552.8303    \n",
            "    Negative Log Likelihood: 57.4368\tSigma2 Prior: -610.2681\tRegularization: 0.0011\n",
            "Iter: 16040  \tTraining Loss: -541.7070    \n",
            "    Negative Log Likelihood: 58.4613\tSigma2 Prior: -600.1694\tRegularization: 0.0011\n",
            "Iter: 16050  \tTraining Loss: -552.0206    \n",
            "    Negative Log Likelihood: 57.9740\tSigma2 Prior: -609.9957\tRegularization: 0.0011\n",
            "Iter: 16060  \tTraining Loss: -528.6027    \n",
            "    Negative Log Likelihood: 59.9307\tSigma2 Prior: -588.5344\tRegularization: 0.0011\n",
            "Iter: 16070  \tTraining Loss: -542.2591    \n",
            "    Negative Log Likelihood: 59.6758\tSigma2 Prior: -601.9360\tRegularization: 0.0011\n",
            "Iter: 16080  \tTraining Loss: -560.8459    \n",
            "    Negative Log Likelihood: 56.6208\tSigma2 Prior: -617.4677\tRegularization: 0.0011\n",
            "Iter: 16090  \tTraining Loss: -548.2327    \n",
            "    Negative Log Likelihood: 57.7273\tSigma2 Prior: -605.9611\tRegularization: 0.0011\n",
            "Iter: 16100  \tTraining Loss: -552.8111    \n",
            "    Negative Log Likelihood: 58.3338\tSigma2 Prior: -611.1460\tRegularization: 0.0011\n",
            "Iter: 16110  \tTraining Loss: -552.8316    \n",
            "    Negative Log Likelihood: 57.7131\tSigma2 Prior: -610.5458\tRegularization: 0.0011\n",
            "Iter: 16120  \tTraining Loss: -540.0033    \n",
            "    Negative Log Likelihood: 59.0661\tSigma2 Prior: -599.0704\tRegularization: 0.0011\n",
            "Iter: 16130  \tTraining Loss: -543.2568    \n",
            "    Negative Log Likelihood: 59.4509\tSigma2 Prior: -602.7087\tRegularization: 0.0011\n",
            "Iter: 16140  \tTraining Loss: -537.4044    \n",
            "    Negative Log Likelihood: 60.1274\tSigma2 Prior: -597.5328\tRegularization: 0.0011\n",
            "Iter: 16150  \tTraining Loss: -553.9713    \n",
            "    Negative Log Likelihood: 57.7858\tSigma2 Prior: -611.7581\tRegularization: 0.0011\n",
            "Iter: 16160  \tTraining Loss: -542.4205    \n",
            "    Negative Log Likelihood: 59.7661\tSigma2 Prior: -602.1876\tRegularization: 0.0011\n",
            "Iter: 16170  \tTraining Loss: -541.7162    \n",
            "    Negative Log Likelihood: 58.7235\tSigma2 Prior: -600.4408\tRegularization: 0.0011\n",
            "Iter: 16180  \tTraining Loss: -531.1302    \n",
            "    Negative Log Likelihood: 59.7533\tSigma2 Prior: -590.8845\tRegularization: 0.0011\n",
            "Iter: 16190  \tTraining Loss: -522.8783    \n",
            "    Negative Log Likelihood: 59.1758\tSigma2 Prior: -582.0552\tRegularization: 0.0011\n",
            "Iter: 16200  \tTraining Loss: -546.4917    \n",
            "    Negative Log Likelihood: 59.0212\tSigma2 Prior: -605.5139\tRegularization: 0.0011\n",
            "Iter: 16210  \tTraining Loss: -552.5059    \n",
            "    Negative Log Likelihood: 57.5771\tSigma2 Prior: -610.0840\tRegularization: 0.0011\n",
            "Iter: 16220  \tTraining Loss: -526.6943    \n",
            "    Negative Log Likelihood: 59.6037\tSigma2 Prior: -586.2991\tRegularization: 0.0011\n",
            "Iter: 16230  \tTraining Loss: -550.2751    \n",
            "    Negative Log Likelihood: 57.8889\tSigma2 Prior: -608.1650\tRegularization: 0.0011\n",
            "Iter: 16240  \tTraining Loss: -557.1878    \n",
            "    Negative Log Likelihood: 56.7124\tSigma2 Prior: -613.9013\tRegularization: 0.0011\n",
            "Iter: 16250  \tTraining Loss: -546.8104    \n",
            "    Negative Log Likelihood: 58.3783\tSigma2 Prior: -605.1898\tRegularization: 0.0011\n",
            "Iter: 16260  \tTraining Loss: -522.4181    \n",
            "    Negative Log Likelihood: 60.8149\tSigma2 Prior: -583.2340\tRegularization: 0.0011\n",
            "Iter: 16270  \tTraining Loss: -536.4153    \n",
            "    Negative Log Likelihood: 58.0899\tSigma2 Prior: -594.5062\tRegularization: 0.0011\n",
            "Iter: 16280  \tTraining Loss: -535.2971    \n",
            "    Negative Log Likelihood: 59.6737\tSigma2 Prior: -594.9718\tRegularization: 0.0011\n",
            "Iter: 16290  \tTraining Loss: -524.3647    \n",
            "    Negative Log Likelihood: 60.5729\tSigma2 Prior: -584.9387\tRegularization: 0.0011\n",
            "Iter: 16300  \tTraining Loss: -544.3060    \n",
            "    Negative Log Likelihood: 58.3265\tSigma2 Prior: -602.6335\tRegularization: 0.0011\n",
            "Iter: 16310  \tTraining Loss: -542.7102    \n",
            "    Negative Log Likelihood: 58.7178\tSigma2 Prior: -601.4291\tRegularization: 0.0011\n",
            "Iter: 16320  \tTraining Loss: -537.9780    \n",
            "    Negative Log Likelihood: 60.4428\tSigma2 Prior: -598.4218\tRegularization: 0.0011\n",
            "Iter: 16330  \tTraining Loss: -551.9533    \n",
            "    Negative Log Likelihood: 57.2641\tSigma2 Prior: -609.2184\tRegularization: 0.0011\n",
            "Iter: 16340  \tTraining Loss: -512.6876    \n",
            "    Negative Log Likelihood: 60.8626\tSigma2 Prior: -573.5512\tRegularization: 0.0011\n",
            "Iter: 16350  \tTraining Loss: -559.2650    \n",
            "    Negative Log Likelihood: 56.2311\tSigma2 Prior: -615.4971\tRegularization: 0.0011\n",
            "Iter: 16360  \tTraining Loss: -542.1176    \n",
            "    Negative Log Likelihood: 58.2515\tSigma2 Prior: -600.3701\tRegularization: 0.0011\n",
            "Iter: 16370  \tTraining Loss: -529.3383    \n",
            "    Negative Log Likelihood: 60.7130\tSigma2 Prior: -590.0524\tRegularization: 0.0011\n",
            "Iter: 16380  \tTraining Loss: -533.1988    \n",
            "    Negative Log Likelihood: 59.0187\tSigma2 Prior: -592.2185\tRegularization: 0.0011\n",
            "Iter: 16390  \tTraining Loss: -535.7120    \n",
            "    Negative Log Likelihood: 59.5802\tSigma2 Prior: -595.2932\tRegularization: 0.0011\n",
            "Iter: 16400  \tTraining Loss: -555.7994    \n",
            "    Negative Log Likelihood: 57.6842\tSigma2 Prior: -613.4846\tRegularization: 0.0011\n",
            "Iter: 16410  \tTraining Loss: -540.3159    \n",
            "    Negative Log Likelihood: 59.6081\tSigma2 Prior: -599.9250\tRegularization: 0.0011\n",
            "Iter: 16420  \tTraining Loss: -551.3308    \n",
            "    Negative Log Likelihood: 57.9373\tSigma2 Prior: -609.2692\tRegularization: 0.0011\n",
            "Iter: 16430  \tTraining Loss: -538.4192    \n",
            "    Negative Log Likelihood: 59.8078\tSigma2 Prior: -598.2281\tRegularization: 0.0011\n",
            "Iter: 16440  \tTraining Loss: -540.8847    \n",
            "    Negative Log Likelihood: 58.5849\tSigma2 Prior: -599.4706\tRegularization: 0.0011\n",
            "Iter: 16450  \tTraining Loss: -528.5355    \n",
            "    Negative Log Likelihood: 60.3488\tSigma2 Prior: -588.8853\tRegularization: 0.0011\n",
            "Iter: 16460  \tTraining Loss: -530.8022    \n",
            "    Negative Log Likelihood: 60.0470\tSigma2 Prior: -590.8502\tRegularization: 0.0011\n",
            "Iter: 16470  \tTraining Loss: -541.4147    \n",
            "    Negative Log Likelihood: 59.4773\tSigma2 Prior: -600.8931\tRegularization: 0.0011\n",
            "Iter: 16480  \tTraining Loss: -552.0344    \n",
            "    Negative Log Likelihood: 56.1592\tSigma2 Prior: -608.1946\tRegularization: 0.0011\n",
            "Iter: 16490  \tTraining Loss: -547.1070    \n",
            "    Negative Log Likelihood: 58.0934\tSigma2 Prior: -605.2015\tRegularization: 0.0011\n",
            "Iter: 16500  \tTraining Loss: -549.8704    \n",
            "    Negative Log Likelihood: 57.1968\tSigma2 Prior: -607.0682\tRegularization: 0.0011\n",
            "Iter: 16510  \tTraining Loss: -529.6358    \n",
            "    Negative Log Likelihood: 59.6346\tSigma2 Prior: -589.2714\tRegularization: 0.0011\n",
            "Iter: 16520  \tTraining Loss: -548.2017    \n",
            "    Negative Log Likelihood: 57.6614\tSigma2 Prior: -605.8641\tRegularization: 0.0011\n",
            "Iter: 16530  \tTraining Loss: -552.5265    \n",
            "    Negative Log Likelihood: 57.8196\tSigma2 Prior: -610.3472\tRegularization: 0.0011\n",
            "Iter: 16540  \tTraining Loss: -545.2880    \n",
            "    Negative Log Likelihood: 59.0976\tSigma2 Prior: -604.3867\tRegularization: 0.0011\n",
            "Iter: 16550  \tTraining Loss: -549.9572    \n",
            "    Negative Log Likelihood: 58.6731\tSigma2 Prior: -608.6313\tRegularization: 0.0011\n",
            "Iter: 16560  \tTraining Loss: -527.9174    \n",
            "    Negative Log Likelihood: 60.9594\tSigma2 Prior: -588.8779\tRegularization: 0.0011\n",
            "Iter: 16570  \tTraining Loss: -547.5963    \n",
            "    Negative Log Likelihood: 59.1057\tSigma2 Prior: -606.7029\tRegularization: 0.0011\n",
            "Iter: 16580  \tTraining Loss: -539.7124    \n",
            "    Negative Log Likelihood: 59.9101\tSigma2 Prior: -599.6235\tRegularization: 0.0011\n",
            "Iter: 16590  \tTraining Loss: -516.7986    \n",
            "    Negative Log Likelihood: 62.0322\tSigma2 Prior: -578.8318\tRegularization: 0.0011\n",
            "Iter: 16600  \tTraining Loss: -535.9557    \n",
            "    Negative Log Likelihood: 59.4821\tSigma2 Prior: -595.4389\tRegularization: 0.0011\n",
            "Iter: 16610  \tTraining Loss: -529.9978    \n",
            "    Negative Log Likelihood: 60.1835\tSigma2 Prior: -590.1824\tRegularization: 0.0011\n",
            "Iter: 16620  \tTraining Loss: -547.0457    \n",
            "    Negative Log Likelihood: 58.9778\tSigma2 Prior: -606.0245\tRegularization: 0.0011\n",
            "Iter: 16630  \tTraining Loss: -548.1038    \n",
            "    Negative Log Likelihood: 59.3501\tSigma2 Prior: -607.4550\tRegularization: 0.0011\n",
            "Iter: 16640  \tTraining Loss: -553.1048    \n",
            "    Negative Log Likelihood: 58.6576\tSigma2 Prior: -611.7634\tRegularization: 0.0011\n",
            "Iter: 16650  \tTraining Loss: -553.5051    \n",
            "    Negative Log Likelihood: 57.6768\tSigma2 Prior: -611.1830\tRegularization: 0.0011\n",
            "Iter: 16660  \tTraining Loss: -540.5870    \n",
            "    Negative Log Likelihood: 59.7062\tSigma2 Prior: -600.2942\tRegularization: 0.0011\n",
            "Iter: 16670  \tTraining Loss: -562.3690    \n",
            "    Negative Log Likelihood: 56.4642\tSigma2 Prior: -618.8342\tRegularization: 0.0011\n",
            "Iter: 16680  \tTraining Loss: -548.6036    \n",
            "    Negative Log Likelihood: 58.2659\tSigma2 Prior: -606.8705\tRegularization: 0.0011\n",
            "Iter: 16690  \tTraining Loss: -531.8113    \n",
            "    Negative Log Likelihood: 60.7274\tSigma2 Prior: -592.5397\tRegularization: 0.0011\n",
            "Iter: 16700  \tTraining Loss: -539.8812    \n",
            "    Negative Log Likelihood: 59.6653\tSigma2 Prior: -599.5475\tRegularization: 0.0011\n",
            "Iter: 16710  \tTraining Loss: -540.2524    \n",
            "    Negative Log Likelihood: 60.1834\tSigma2 Prior: -600.4369\tRegularization: 0.0011\n",
            "Iter: 16720  \tTraining Loss: -540.3288    \n",
            "    Negative Log Likelihood: 59.3665\tSigma2 Prior: -599.6963\tRegularization: 0.0011\n",
            "Iter: 16730  \tTraining Loss: -536.8047    \n",
            "    Negative Log Likelihood: 60.0878\tSigma2 Prior: -596.8936\tRegularization: 0.0011\n",
            "Iter: 16740  \tTraining Loss: -537.8259    \n",
            "    Negative Log Likelihood: 60.6198\tSigma2 Prior: -598.4467\tRegularization: 0.0011\n",
            "Iter: 16750  \tTraining Loss: -518.0336    \n",
            "    Negative Log Likelihood: 61.4679\tSigma2 Prior: -579.5026\tRegularization: 0.0011\n",
            "Iter: 16760  \tTraining Loss: -549.3314    \n",
            "    Negative Log Likelihood: 59.0290\tSigma2 Prior: -608.3615\tRegularization: 0.0011\n",
            "Iter: 16770  \tTraining Loss: -527.3779    \n",
            "    Negative Log Likelihood: 59.8770\tSigma2 Prior: -587.2559\tRegularization: 0.0011\n",
            "Iter: 16780  \tTraining Loss: -559.5331    \n",
            "    Negative Log Likelihood: 55.6903\tSigma2 Prior: -615.2245\tRegularization: 0.0011\n",
            "Iter: 16790  \tTraining Loss: -532.0845    \n",
            "    Negative Log Likelihood: 60.8637\tSigma2 Prior: -592.9493\tRegularization: 0.0011\n",
            "Iter: 16800  \tTraining Loss: -554.0684    \n",
            "    Negative Log Likelihood: 57.6187\tSigma2 Prior: -611.6881\tRegularization: 0.0011\n",
            "Iter: 16810  \tTraining Loss: -548.2064    \n",
            "    Negative Log Likelihood: 58.9164\tSigma2 Prior: -607.1239\tRegularization: 0.0011\n",
            "Iter: 16820  \tTraining Loss: -549.3076    \n",
            "    Negative Log Likelihood: 58.6811\tSigma2 Prior: -607.9897\tRegularization: 0.0011\n",
            "Iter: 16830  \tTraining Loss: -546.6672    \n",
            "    Negative Log Likelihood: 58.5410\tSigma2 Prior: -605.2093\tRegularization: 0.0011\n",
            "Iter: 16840  \tTraining Loss: -538.2941    \n",
            "    Negative Log Likelihood: 58.5589\tSigma2 Prior: -596.8541\tRegularization: 0.0011\n",
            "Iter: 16850  \tTraining Loss: -547.7061    \n",
            "    Negative Log Likelihood: 58.5889\tSigma2 Prior: -606.2960\tRegularization: 0.0011\n",
            "Iter: 16860  \tTraining Loss: -536.8492    \n",
            "    Negative Log Likelihood: 60.1347\tSigma2 Prior: -596.9850\tRegularization: 0.0011\n",
            "Iter: 16870  \tTraining Loss: -528.2911    \n",
            "    Negative Log Likelihood: 60.2495\tSigma2 Prior: -588.5417\tRegularization: 0.0011\n",
            "Iter: 16880  \tTraining Loss: -543.9059    \n",
            "    Negative Log Likelihood: 59.3749\tSigma2 Prior: -603.2820\tRegularization: 0.0011\n",
            "Iter: 16890  \tTraining Loss: -539.0392    \n",
            "    Negative Log Likelihood: 59.8210\tSigma2 Prior: -598.8613\tRegularization: 0.0011\n",
            "Iter: 16900  \tTraining Loss: -544.9681    \n",
            "    Negative Log Likelihood: 58.8976\tSigma2 Prior: -603.8668\tRegularization: 0.0011\n",
            "Iter: 16910  \tTraining Loss: -546.1735    \n",
            "    Negative Log Likelihood: 59.0041\tSigma2 Prior: -605.1787\tRegularization: 0.0011\n",
            "Iter: 16920  \tTraining Loss: -527.5777    \n",
            "    Negative Log Likelihood: 60.2614\tSigma2 Prior: -587.8402\tRegularization: 0.0011\n",
            "Iter: 16930  \tTraining Loss: -546.1423    \n",
            "    Negative Log Likelihood: 57.6706\tSigma2 Prior: -603.8140\tRegularization: 0.0011\n",
            "Iter: 16940  \tTraining Loss: -548.4662    \n",
            "    Negative Log Likelihood: 56.6263\tSigma2 Prior: -605.0936\tRegularization: 0.0011\n",
            "Iter: 16950  \tTraining Loss: -554.8663    \n",
            "    Negative Log Likelihood: 57.4797\tSigma2 Prior: -612.3471\tRegularization: 0.0011\n",
            "Iter: 16960  \tTraining Loss: -532.9795    \n",
            "    Negative Log Likelihood: 60.7226\tSigma2 Prior: -593.7032\tRegularization: 0.0011\n",
            "Iter: 16970  \tTraining Loss: -518.5294    \n",
            "    Negative Log Likelihood: 61.3260\tSigma2 Prior: -579.8565\tRegularization: 0.0011\n",
            "Iter: 16980  \tTraining Loss: -541.4463    \n",
            "    Negative Log Likelihood: 57.4752\tSigma2 Prior: -598.9225\tRegularization: 0.0011\n",
            "Iter: 16990  \tTraining Loss: -526.5759    \n",
            "    Negative Log Likelihood: 58.4588\tSigma2 Prior: -585.0359\tRegularization: 0.0011\n",
            "Iter: 17000  \tTraining Loss: -550.6059    \n",
            "    Negative Log Likelihood: 58.3107\tSigma2 Prior: -608.9177\tRegularization: 0.0011\n",
            "Iter: 17010  \tTraining Loss: -534.3541    \n",
            "    Negative Log Likelihood: 59.6278\tSigma2 Prior: -593.9830\tRegularization: 0.0011\n",
            "Iter: 17020  \tTraining Loss: -541.9028    \n",
            "    Negative Log Likelihood: 59.2745\tSigma2 Prior: -601.1785\tRegularization: 0.0011\n",
            "Iter: 17030  \tTraining Loss: -550.3447    \n",
            "    Negative Log Likelihood: 56.0734\tSigma2 Prior: -606.4193\tRegularization: 0.0011\n",
            "Iter: 17040  \tTraining Loss: -546.9542    \n",
            "    Negative Log Likelihood: 55.0544\tSigma2 Prior: -602.0098\tRegularization: 0.0011\n",
            "Iter: 17050  \tTraining Loss: -521.6512    \n",
            "    Negative Log Likelihood: 60.6075\tSigma2 Prior: -582.2598\tRegularization: 0.0011\n",
            "Iter: 17060  \tTraining Loss: -535.1907    \n",
            "    Negative Log Likelihood: 59.4940\tSigma2 Prior: -594.6858\tRegularization: 0.0011\n",
            "Iter: 17070  \tTraining Loss: -529.0903    \n",
            "    Negative Log Likelihood: 59.1298\tSigma2 Prior: -588.2212\tRegularization: 0.0011\n",
            "Iter: 17080  \tTraining Loss: -547.1401    \n",
            "    Negative Log Likelihood: 57.8407\tSigma2 Prior: -604.9819\tRegularization: 0.0011\n",
            "Iter: 17090  \tTraining Loss: -541.9584    \n",
            "    Negative Log Likelihood: 58.8775\tSigma2 Prior: -600.8370\tRegularization: 0.0011\n",
            "Iter: 17100  \tTraining Loss: -549.2922    \n",
            "    Negative Log Likelihood: 57.5129\tSigma2 Prior: -606.8063\tRegularization: 0.0011\n",
            "Iter: 17110  \tTraining Loss: -542.1978    \n",
            "    Negative Log Likelihood: 58.2484\tSigma2 Prior: -600.4473\tRegularization: 0.0011\n",
            "Iter: 17120  \tTraining Loss: -549.7036    \n",
            "    Negative Log Likelihood: 55.6990\tSigma2 Prior: -605.4037\tRegularization: 0.0011\n",
            "Iter: 17130  \tTraining Loss: -553.0810    \n",
            "    Negative Log Likelihood: 55.5468\tSigma2 Prior: -608.6289\tRegularization: 0.0011\n",
            "Iter: 17140  \tTraining Loss: -544.3862    \n",
            "    Negative Log Likelihood: 56.8306\tSigma2 Prior: -601.2180\tRegularization: 0.0011\n",
            "Iter: 17150  \tTraining Loss: -559.4321    \n",
            "    Negative Log Likelihood: 56.7026\tSigma2 Prior: -616.1357\tRegularization: 0.0011\n",
            "Iter: 17160  \tTraining Loss: -541.5353    \n",
            "    Negative Log Likelihood: 58.1078\tSigma2 Prior: -599.6442\tRegularization: 0.0011\n",
            "Iter: 17170  \tTraining Loss: -548.0832    \n",
            "    Negative Log Likelihood: 57.9106\tSigma2 Prior: -605.9949\tRegularization: 0.0011\n",
            "Iter: 17180  \tTraining Loss: -508.5623    \n",
            "    Negative Log Likelihood: 60.0958\tSigma2 Prior: -568.6592\tRegularization: 0.0011\n",
            "Iter: 17190  \tTraining Loss: -543.8322    \n",
            "    Negative Log Likelihood: 57.4225\tSigma2 Prior: -601.2557\tRegularization: 0.0011\n",
            "Iter: 17200  \tTraining Loss: -529.2070    \n",
            "    Negative Log Likelihood: 59.0998\tSigma2 Prior: -588.3079\tRegularization: 0.0011\n",
            "Iter: 17210  \tTraining Loss: -503.2220    \n",
            "    Negative Log Likelihood: 62.5442\tSigma2 Prior: -565.7673\tRegularization: 0.0011\n",
            "Iter: 17220  \tTraining Loss: -557.6423    \n",
            "    Negative Log Likelihood: 56.4807\tSigma2 Prior: -614.1241\tRegularization: 0.0011\n",
            "Iter: 17230  \tTraining Loss: -525.2194    \n",
            "    Negative Log Likelihood: 60.5364\tSigma2 Prior: -585.7570\tRegularization: 0.0011\n",
            "Iter: 17240  \tTraining Loss: -531.5551    \n",
            "    Negative Log Likelihood: 59.0162\tSigma2 Prior: -590.5724\tRegularization: 0.0011\n",
            "Iter: 17250  \tTraining Loss: -535.2016    \n",
            "    Negative Log Likelihood: 58.0144\tSigma2 Prior: -593.2171\tRegularization: 0.0011\n",
            "Iter: 17260  \tTraining Loss: -557.3693    \n",
            "    Negative Log Likelihood: 57.0944\tSigma2 Prior: -614.4648\tRegularization: 0.0011\n",
            "Iter: 17270  \tTraining Loss: -552.8165    \n",
            "    Negative Log Likelihood: 58.3991\tSigma2 Prior: -611.2167\tRegularization: 0.0011\n",
            "Iter: 17280  \tTraining Loss: -561.1765    \n",
            "    Negative Log Likelihood: 57.3663\tSigma2 Prior: -618.5439\tRegularization: 0.0011\n",
            "Iter: 17290  \tTraining Loss: -518.7708    \n",
            "    Negative Log Likelihood: 61.8421\tSigma2 Prior: -580.6140\tRegularization: 0.0011\n",
            "Iter: 17300  \tTraining Loss: -532.8215    \n",
            "    Negative Log Likelihood: 59.2936\tSigma2 Prior: -592.1163\tRegularization: 0.0011\n",
            "Iter: 17310  \tTraining Loss: -537.5837    \n",
            "    Negative Log Likelihood: 59.1829\tSigma2 Prior: -596.7676\tRegularization: 0.0011\n",
            "Iter: 17320  \tTraining Loss: -522.6542    \n",
            "    Negative Log Likelihood: 60.1308\tSigma2 Prior: -582.7862\tRegularization: 0.0011\n",
            "Iter: 17330  \tTraining Loss: -529.3463    \n",
            "    Negative Log Likelihood: 59.9199\tSigma2 Prior: -589.2673\tRegularization: 0.0011\n",
            "Iter: 17340  \tTraining Loss: -520.4179    \n",
            "    Negative Log Likelihood: 60.1192\tSigma2 Prior: -580.5383\tRegularization: 0.0011\n",
            "Iter: 17350  \tTraining Loss: -537.4172    \n",
            "    Negative Log Likelihood: 58.4243\tSigma2 Prior: -595.8426\tRegularization: 0.0011\n",
            "Iter: 17360  \tTraining Loss: -528.6960    \n",
            "    Negative Log Likelihood: 59.0952\tSigma2 Prior: -587.7923\tRegularization: 0.0011\n",
            "Iter: 17370  \tTraining Loss: -556.2014    \n",
            "    Negative Log Likelihood: 55.8812\tSigma2 Prior: -612.0836\tRegularization: 0.0011\n",
            "Iter: 17380  \tTraining Loss: -540.8264    \n",
            "    Negative Log Likelihood: 57.8115\tSigma2 Prior: -598.6390\tRegularization: 0.0011\n",
            "Iter: 17390  \tTraining Loss: -547.6634    \n",
            "    Negative Log Likelihood: 56.9320\tSigma2 Prior: -604.5965\tRegularization: 0.0011\n",
            "Iter: 17400  \tTraining Loss: -548.0306    \n",
            "    Negative Log Likelihood: 58.0382\tSigma2 Prior: -606.0699\tRegularization: 0.0011\n",
            "Iter: 17410  \tTraining Loss: -536.3405    \n",
            "    Negative Log Likelihood: 59.0993\tSigma2 Prior: -595.4409\tRegularization: 0.0011\n",
            "Iter: 17420  \tTraining Loss: -523.8703    \n",
            "    Negative Log Likelihood: 60.1680\tSigma2 Prior: -584.0394\tRegularization: 0.0011\n",
            "Iter: 17430  \tTraining Loss: -539.2553    \n",
            "    Negative Log Likelihood: 56.9435\tSigma2 Prior: -596.1999\tRegularization: 0.0011\n",
            "Iter: 17440  \tTraining Loss: -542.8328    \n",
            "    Negative Log Likelihood: 58.2544\tSigma2 Prior: -601.0883\tRegularization: 0.0011\n",
            "Iter: 17450  \tTraining Loss: -558.4575    \n",
            "    Negative Log Likelihood: 56.2663\tSigma2 Prior: -614.7250\tRegularization: 0.0011\n",
            "Iter: 17460  \tTraining Loss: -541.7505    \n",
            "    Negative Log Likelihood: 56.8158\tSigma2 Prior: -598.5674\tRegularization: 0.0011\n",
            "Iter: 17470  \tTraining Loss: -541.4256    \n",
            "    Negative Log Likelihood: 58.4741\tSigma2 Prior: -599.9008\tRegularization: 0.0011\n",
            "Iter: 17480  \tTraining Loss: -531.0981    \n",
            "    Negative Log Likelihood: 60.1439\tSigma2 Prior: -591.2431\tRegularization: 0.0011\n",
            "Iter: 17490  \tTraining Loss: -535.9229    \n",
            "    Negative Log Likelihood: 58.1668\tSigma2 Prior: -594.0908\tRegularization: 0.0011\n",
            "Iter: 17500  \tTraining Loss: -556.1968    \n",
            "    Negative Log Likelihood: 55.8925\tSigma2 Prior: -612.0905\tRegularization: 0.0011\n",
            "Iter: 17510  \tTraining Loss: -521.0747    \n",
            "    Negative Log Likelihood: 59.6604\tSigma2 Prior: -580.7362\tRegularization: 0.0011\n",
            "Iter: 17520  \tTraining Loss: -559.5271    \n",
            "    Negative Log Likelihood: 54.4895\tSigma2 Prior: -614.0177\tRegularization: 0.0011\n",
            "Iter: 17530  \tTraining Loss: -551.9378    \n",
            "    Negative Log Likelihood: 58.0322\tSigma2 Prior: -609.9711\tRegularization: 0.0011\n",
            "Iter: 17540  \tTraining Loss: -547.4874    \n",
            "    Negative Log Likelihood: 58.3412\tSigma2 Prior: -605.8297\tRegularization: 0.0011\n",
            "Iter: 17550  \tTraining Loss: -538.0499    \n",
            "    Negative Log Likelihood: 59.8303\tSigma2 Prior: -597.8812\tRegularization: 0.0011\n",
            "Iter: 17560  \tTraining Loss: -528.4376    \n",
            "    Negative Log Likelihood: 60.4422\tSigma2 Prior: -588.8809\tRegularization: 0.0011\n",
            "Iter: 17570  \tTraining Loss: -545.3380    \n",
            "    Negative Log Likelihood: 58.1834\tSigma2 Prior: -603.5225\tRegularization: 0.0011\n",
            "Iter: 17580  \tTraining Loss: -533.8988    \n",
            "    Negative Log Likelihood: 60.1614\tSigma2 Prior: -594.0613\tRegularization: 0.0011\n",
            "Iter: 17590  \tTraining Loss: -561.6741    \n",
            "    Negative Log Likelihood: 55.3303\tSigma2 Prior: -617.0056\tRegularization: 0.0011\n",
            "Iter: 17600  \tTraining Loss: -549.3192    \n",
            "    Negative Log Likelihood: 56.8654\tSigma2 Prior: -606.1857\tRegularization: 0.0011\n",
            "Iter: 17610  \tTraining Loss: -528.0533    \n",
            "    Negative Log Likelihood: 58.5584\tSigma2 Prior: -586.6129\tRegularization: 0.0011\n",
            "Iter: 17620  \tTraining Loss: -520.3499    \n",
            "    Negative Log Likelihood: 61.2331\tSigma2 Prior: -581.5842\tRegularization: 0.0011\n",
            "Iter: 17630  \tTraining Loss: -536.0290    \n",
            "    Negative Log Likelihood: 58.8571\tSigma2 Prior: -594.8872\tRegularization: 0.0011\n",
            "Iter: 17640  \tTraining Loss: -558.9221    \n",
            "    Negative Log Likelihood: 57.2976\tSigma2 Prior: -616.2208\tRegularization: 0.0011\n",
            "Iter: 17650  \tTraining Loss: -549.4460    \n",
            "    Negative Log Likelihood: 57.3992\tSigma2 Prior: -606.8464\tRegularization: 0.0011\n",
            "Iter: 17660  \tTraining Loss: -546.4694    \n",
            "    Negative Log Likelihood: 58.4672\tSigma2 Prior: -604.9376\tRegularization: 0.0011\n",
            "Iter: 17670  \tTraining Loss: -529.5056    \n",
            "    Negative Log Likelihood: 58.6210\tSigma2 Prior: -588.1277\tRegularization: 0.0011\n",
            "Iter: 17680  \tTraining Loss: -536.9875    \n",
            "    Negative Log Likelihood: 57.8598\tSigma2 Prior: -594.8484\tRegularization: 0.0011\n",
            "Iter: 17690  \tTraining Loss: -538.8112    \n",
            "    Negative Log Likelihood: 59.4993\tSigma2 Prior: -598.3115\tRegularization: 0.0011\n",
            "Iter: 17700  \tTraining Loss: -536.2124    \n",
            "    Negative Log Likelihood: 60.2710\tSigma2 Prior: -596.4845\tRegularization: 0.0011\n",
            "Iter: 17710  \tTraining Loss: -552.4927    \n",
            "    Negative Log Likelihood: 57.4285\tSigma2 Prior: -609.9223\tRegularization: 0.0011\n",
            "Iter: 17720  \tTraining Loss: -542.2020    \n",
            "    Negative Log Likelihood: 58.4601\tSigma2 Prior: -600.6632\tRegularization: 0.0011\n",
            "Iter: 17730  \tTraining Loss: -522.3573    \n",
            "    Negative Log Likelihood: 61.5783\tSigma2 Prior: -583.9366\tRegularization: 0.0011\n",
            "Iter: 17740  \tTraining Loss: -539.4680    \n",
            "    Negative Log Likelihood: 59.0464\tSigma2 Prior: -598.5155\tRegularization: 0.0011\n",
            "Iter: 17750  \tTraining Loss: -514.3654    \n",
            "    Negative Log Likelihood: 60.9261\tSigma2 Prior: -575.2926\tRegularization: 0.0011\n",
            "Iter: 17760  \tTraining Loss: -547.6670    \n",
            "    Negative Log Likelihood: 57.9848\tSigma2 Prior: -605.6529\tRegularization: 0.0011\n",
            "Iter: 17770  \tTraining Loss: -541.9899    \n",
            "    Negative Log Likelihood: 59.0532\tSigma2 Prior: -601.0442\tRegularization: 0.0011\n",
            "Iter: 17780  \tTraining Loss: -530.0915    \n",
            "    Negative Log Likelihood: 59.3594\tSigma2 Prior: -589.4520\tRegularization: 0.0011\n",
            "Iter: 17790  \tTraining Loss: -550.3588    \n",
            "    Negative Log Likelihood: 57.6664\tSigma2 Prior: -608.0262\tRegularization: 0.0011\n",
            "Iter: 17800  \tTraining Loss: -528.9576    \n",
            "    Negative Log Likelihood: 60.3801\tSigma2 Prior: -589.3388\tRegularization: 0.0011\n",
            "Iter: 17810  \tTraining Loss: -554.8326    \n",
            "    Negative Log Likelihood: 57.2173\tSigma2 Prior: -612.0510\tRegularization: 0.0011\n",
            "Iter: 17820  \tTraining Loss: -549.6493    \n",
            "    Negative Log Likelihood: 56.9502\tSigma2 Prior: -606.6006\tRegularization: 0.0011\n",
            "Iter: 17830  \tTraining Loss: -560.0474    \n",
            "    Negative Log Likelihood: 56.2055\tSigma2 Prior: -616.2540\tRegularization: 0.0011\n",
            "Iter: 17840  \tTraining Loss: -539.5104    \n",
            "    Negative Log Likelihood: 58.6273\tSigma2 Prior: -598.1387\tRegularization: 0.0011\n",
            "Iter: 17850  \tTraining Loss: -537.9720    \n",
            "    Negative Log Likelihood: 60.2132\tSigma2 Prior: -598.1863\tRegularization: 0.0011\n",
            "Iter: 17860  \tTraining Loss: -545.3344    \n",
            "    Negative Log Likelihood: 59.0220\tSigma2 Prior: -604.3575\tRegularization: 0.0011\n",
            "Iter: 17870  \tTraining Loss: -547.5593    \n",
            "    Negative Log Likelihood: 58.4090\tSigma2 Prior: -605.9695\tRegularization: 0.0011\n",
            "Iter: 17880  \tTraining Loss: -525.4921    \n",
            "    Negative Log Likelihood: 60.6587\tSigma2 Prior: -586.1519\tRegularization: 0.0011\n",
            "Iter: 17890  \tTraining Loss: -518.6641    \n",
            "    Negative Log Likelihood: 60.3330\tSigma2 Prior: -578.9982\tRegularization: 0.0011\n",
            "Iter: 17900  \tTraining Loss: -536.2495    \n",
            "    Negative Log Likelihood: 58.6452\tSigma2 Prior: -594.8958\tRegularization: 0.0011\n",
            "Iter: 17910  \tTraining Loss: -529.2198    \n",
            "    Negative Log Likelihood: 60.1350\tSigma2 Prior: -589.3559\tRegularization: 0.0011\n",
            "Iter: 17920  \tTraining Loss: -543.4617    \n",
            "    Negative Log Likelihood: 58.6150\tSigma2 Prior: -602.0779\tRegularization: 0.0011\n",
            "Iter: 17930  \tTraining Loss: -559.3629    \n",
            "    Negative Log Likelihood: 56.1695\tSigma2 Prior: -615.5334\tRegularization: 0.0011\n",
            "Iter: 17940  \tTraining Loss: -548.5089    \n",
            "    Negative Log Likelihood: 57.5894\tSigma2 Prior: -606.0994\tRegularization: 0.0011\n",
            "Iter: 17950  \tTraining Loss: -530.6685    \n",
            "    Negative Log Likelihood: 59.4535\tSigma2 Prior: -590.1230\tRegularization: 0.0011\n",
            "Iter: 17960  \tTraining Loss: -543.6174    \n",
            "    Negative Log Likelihood: 57.7304\tSigma2 Prior: -601.3488\tRegularization: 0.0011\n",
            "Iter: 17970  \tTraining Loss: -551.8889    \n",
            "    Negative Log Likelihood: 57.9372\tSigma2 Prior: -609.8273\tRegularization: 0.0011\n",
            "Iter: 17980  \tTraining Loss: -541.5690    \n",
            "    Negative Log Likelihood: 58.5321\tSigma2 Prior: -600.1022\tRegularization: 0.0011\n",
            "Iter: 17990  \tTraining Loss: -542.9506    \n",
            "    Negative Log Likelihood: 58.9149\tSigma2 Prior: -601.8666\tRegularization: 0.0011\n",
            "Iter: 18000  \tTraining Loss: -529.7424    \n",
            "    Negative Log Likelihood: 59.5334\tSigma2 Prior: -589.2769\tRegularization: 0.0011\n",
            "Iter: 18010  \tTraining Loss: -530.0020    \n",
            "    Negative Log Likelihood: 58.4019\tSigma2 Prior: -588.4050\tRegularization: 0.0011\n",
            "Iter: 18020  \tTraining Loss: -536.6801    \n",
            "    Negative Log Likelihood: 58.5353\tSigma2 Prior: -595.2164\tRegularization: 0.0011\n",
            "Iter: 18030  \tTraining Loss: -529.2891    \n",
            "    Negative Log Likelihood: 59.9459\tSigma2 Prior: -589.2361\tRegularization: 0.0011\n",
            "Iter: 18040  \tTraining Loss: -529.4057    \n",
            "    Negative Log Likelihood: 59.3122\tSigma2 Prior: -588.7190\tRegularization: 0.0011\n",
            "Iter: 18050  \tTraining Loss: -524.3220    \n",
            "    Negative Log Likelihood: 59.7582\tSigma2 Prior: -584.0813\tRegularization: 0.0011\n",
            "Iter: 18060  \tTraining Loss: -520.5482    \n",
            "    Negative Log Likelihood: 61.9560\tSigma2 Prior: -582.5052\tRegularization: 0.0011\n",
            "Iter: 18070  \tTraining Loss: -526.3333    \n",
            "    Negative Log Likelihood: 60.3792\tSigma2 Prior: -586.7136\tRegularization: 0.0011\n",
            "Iter: 18080  \tTraining Loss: -535.9203    \n",
            "    Negative Log Likelihood: 60.6511\tSigma2 Prior: -596.5726\tRegularization: 0.0011\n",
            "Iter: 18090  \tTraining Loss: -527.4220    \n",
            "    Negative Log Likelihood: 60.1754\tSigma2 Prior: -587.5985\tRegularization: 0.0011\n",
            "Iter: 18100  \tTraining Loss: -533.1851    \n",
            "    Negative Log Likelihood: 59.3334\tSigma2 Prior: -592.5196\tRegularization: 0.0011\n",
            "Iter: 18110  \tTraining Loss: -548.4589    \n",
            "    Negative Log Likelihood: 57.1708\tSigma2 Prior: -605.6309\tRegularization: 0.0011\n",
            "Iter: 18120  \tTraining Loss: -544.2251    \n",
            "    Negative Log Likelihood: 57.5553\tSigma2 Prior: -601.7815\tRegularization: 0.0011\n",
            "Iter: 18130  \tTraining Loss: -518.7307    \n",
            "    Negative Log Likelihood: 60.1162\tSigma2 Prior: -578.8480\tRegularization: 0.0011\n",
            "Iter: 18140  \tTraining Loss: -548.8599    \n",
            "    Negative Log Likelihood: 56.2707\tSigma2 Prior: -605.1317\tRegularization: 0.0011\n",
            "Iter: 18150  \tTraining Loss: -553.6828    \n",
            "    Negative Log Likelihood: 56.2580\tSigma2 Prior: -609.9419\tRegularization: 0.0011\n",
            "Iter: 18160  \tTraining Loss: -547.5064    \n",
            "    Negative Log Likelihood: 58.8089\tSigma2 Prior: -606.3164\tRegularization: 0.0011\n",
            "Iter: 18170  \tTraining Loss: -542.6736    \n",
            "    Negative Log Likelihood: 57.5460\tSigma2 Prior: -600.2206\tRegularization: 0.0011\n",
            "Iter: 18180  \tTraining Loss: -528.5657    \n",
            "    Negative Log Likelihood: 59.4362\tSigma2 Prior: -588.0030\tRegularization: 0.0011\n",
            "Iter: 18190  \tTraining Loss: -547.3757    \n",
            "    Negative Log Likelihood: 57.9775\tSigma2 Prior: -605.3543\tRegularization: 0.0011\n",
            "Iter: 18200  \tTraining Loss: -551.6205    \n",
            "    Negative Log Likelihood: 56.6021\tSigma2 Prior: -608.2236\tRegularization: 0.0011\n",
            "Iter: 18210  \tTraining Loss: -561.7791    \n",
            "    Negative Log Likelihood: 55.3375\tSigma2 Prior: -617.1177\tRegularization: 0.0011\n",
            "Iter: 18220  \tTraining Loss: -537.8099    \n",
            "    Negative Log Likelihood: 58.2802\tSigma2 Prior: -596.0912\tRegularization: 0.0011\n",
            "Iter: 18230  \tTraining Loss: -555.6627    \n",
            "    Negative Log Likelihood: 55.8226\tSigma2 Prior: -611.4863\tRegularization: 0.0011\n",
            "Iter: 18240  \tTraining Loss: -554.9623    \n",
            "    Negative Log Likelihood: 57.5402\tSigma2 Prior: -612.5037\tRegularization: 0.0011\n",
            "Iter: 18250  \tTraining Loss: -529.9297    \n",
            "    Negative Log Likelihood: 59.5532\tSigma2 Prior: -589.4840\tRegularization: 0.0011\n",
            "Iter: 18260  \tTraining Loss: -549.9445    \n",
            "    Negative Log Likelihood: 58.7188\tSigma2 Prior: -608.6644\tRegularization: 0.0011\n",
            "Iter: 18270  \tTraining Loss: -519.8738    \n",
            "    Negative Log Likelihood: 61.9829\tSigma2 Prior: -581.8578\tRegularization: 0.0011\n",
            "Iter: 18280  \tTraining Loss: -536.6251    \n",
            "    Negative Log Likelihood: 59.9100\tSigma2 Prior: -596.5361\tRegularization: 0.0011\n",
            "Iter: 18290  \tTraining Loss: -561.4111    \n",
            "    Negative Log Likelihood: 57.0947\tSigma2 Prior: -618.5068\tRegularization: 0.0011\n",
            "Iter: 18300  \tTraining Loss: -558.6872    \n",
            "    Negative Log Likelihood: 56.7545\tSigma2 Prior: -615.4428\tRegularization: 0.0011\n",
            "Iter: 18310  \tTraining Loss: -549.2935    \n",
            "    Negative Log Likelihood: 58.2942\tSigma2 Prior: -607.5888\tRegularization: 0.0011\n",
            "Iter: 18320  \tTraining Loss: -529.8089    \n",
            "    Negative Log Likelihood: 60.7941\tSigma2 Prior: -590.6041\tRegularization: 0.0011\n",
            "Iter: 18330  \tTraining Loss: -553.0569    \n",
            "    Negative Log Likelihood: 57.8360\tSigma2 Prior: -610.8940\tRegularization: 0.0011\n",
            "Iter: 18340  \tTraining Loss: -546.3643    \n",
            "    Negative Log Likelihood: 58.5879\tSigma2 Prior: -604.9533\tRegularization: 0.0011\n",
            "Iter: 18350  \tTraining Loss: -521.8561    \n",
            "    Negative Log Likelihood: 59.8865\tSigma2 Prior: -581.7437\tRegularization: 0.0011\n",
            "Iter: 18360  \tTraining Loss: -551.8356    \n",
            "    Negative Log Likelihood: 57.6335\tSigma2 Prior: -609.4702\tRegularization: 0.0011\n",
            "Iter: 18370  \tTraining Loss: -524.9183    \n",
            "    Negative Log Likelihood: 61.4445\tSigma2 Prior: -586.3640\tRegularization: 0.0011\n",
            "Iter: 18380  \tTraining Loss: -542.9109    \n",
            "    Negative Log Likelihood: 58.0890\tSigma2 Prior: -601.0010\tRegularization: 0.0011\n",
            "Iter: 18390  \tTraining Loss: -527.8981    \n",
            "    Negative Log Likelihood: 60.3813\tSigma2 Prior: -588.2806\tRegularization: 0.0011\n",
            "Iter: 18400  \tTraining Loss: -540.1678    \n",
            "    Negative Log Likelihood: 58.0159\tSigma2 Prior: -598.1849\tRegularization: 0.0011\n",
            "Iter: 18410  \tTraining Loss: -527.7125    \n",
            "    Negative Log Likelihood: 60.1379\tSigma2 Prior: -587.8514\tRegularization: 0.0011\n",
            "Iter: 18420  \tTraining Loss: -562.5626    \n",
            "    Negative Log Likelihood: 55.7655\tSigma2 Prior: -618.3292\tRegularization: 0.0011\n",
            "Iter: 18430  \tTraining Loss: -546.9362    \n",
            "    Negative Log Likelihood: 56.5488\tSigma2 Prior: -603.4860\tRegularization: 0.0011\n",
            "Iter: 18440  \tTraining Loss: -535.8250    \n",
            "    Negative Log Likelihood: 58.5658\tSigma2 Prior: -594.3920\tRegularization: 0.0011\n",
            "Iter: 18450  \tTraining Loss: -549.3862    \n",
            "    Negative Log Likelihood: 57.3678\tSigma2 Prior: -606.7551\tRegularization: 0.0011\n",
            "Iter: 18460  \tTraining Loss: -533.5330    \n",
            "    Negative Log Likelihood: 58.8308\tSigma2 Prior: -592.3649\tRegularization: 0.0011\n",
            "Iter: 18470  \tTraining Loss: -559.0928    \n",
            "    Negative Log Likelihood: 55.2389\tSigma2 Prior: -614.3328\tRegularization: 0.0011\n",
            "Iter: 18480  \tTraining Loss: -560.0619    \n",
            "    Negative Log Likelihood: 55.4929\tSigma2 Prior: -615.5559\tRegularization: 0.0011\n",
            "Iter: 18490  \tTraining Loss: -537.7076    \n",
            "    Negative Log Likelihood: 58.9820\tSigma2 Prior: -596.6908\tRegularization: 0.0011\n",
            "Iter: 18500  \tTraining Loss: -542.6396    \n",
            "    Negative Log Likelihood: 59.1315\tSigma2 Prior: -601.7722\tRegularization: 0.0011\n",
            "Iter: 18510  \tTraining Loss: -551.7525    \n",
            "    Negative Log Likelihood: 57.0502\tSigma2 Prior: -608.8038\tRegularization: 0.0011\n",
            "Iter: 18520  \tTraining Loss: -539.0298    \n",
            "    Negative Log Likelihood: 58.8917\tSigma2 Prior: -597.9225\tRegularization: 0.0011\n",
            "Iter: 18530  \tTraining Loss: -551.9249    \n",
            "    Negative Log Likelihood: 57.4696\tSigma2 Prior: -609.3955\tRegularization: 0.0011\n",
            "Iter: 18540  \tTraining Loss: -542.0775    \n",
            "    Negative Log Likelihood: 58.1283\tSigma2 Prior: -600.2070\tRegularization: 0.0011\n",
            "Iter: 18550  \tTraining Loss: -541.7881    \n",
            "    Negative Log Likelihood: 59.2541\tSigma2 Prior: -601.0433\tRegularization: 0.0011\n",
            "Iter: 18560  \tTraining Loss: -539.2183    \n",
            "    Negative Log Likelihood: 58.4456\tSigma2 Prior: -597.6650\tRegularization: 0.0011\n",
            "Iter: 18570  \tTraining Loss: -516.7531    \n",
            "    Negative Log Likelihood: 61.7822\tSigma2 Prior: -578.5364\tRegularization: 0.0011\n",
            "Iter: 18580  \tTraining Loss: -536.1373    \n",
            "    Negative Log Likelihood: 59.6017\tSigma2 Prior: -595.7401\tRegularization: 0.0011\n",
            "Iter: 18590  \tTraining Loss: -530.5309    \n",
            "    Negative Log Likelihood: 59.6463\tSigma2 Prior: -590.1783\tRegularization: 0.0011\n",
            "Iter: 18600  \tTraining Loss: -545.1489    \n",
            "    Negative Log Likelihood: 58.9328\tSigma2 Prior: -604.0828\tRegularization: 0.0011\n",
            "Iter: 18610  \tTraining Loss: -532.7073    \n",
            "    Negative Log Likelihood: 60.3238\tSigma2 Prior: -593.0322\tRegularization: 0.0011\n",
            "Iter: 18620  \tTraining Loss: -517.2036    \n",
            "    Negative Log Likelihood: 61.9943\tSigma2 Prior: -579.1990\tRegularization: 0.0011\n",
            "Iter: 18630  \tTraining Loss: -511.6526    \n",
            "    Negative Log Likelihood: 60.8756\tSigma2 Prior: -572.5294\tRegularization: 0.0011\n",
            "Iter: 18640  \tTraining Loss: -542.7805    \n",
            "    Negative Log Likelihood: 57.8522\tSigma2 Prior: -600.6338\tRegularization: 0.0011\n",
            "Iter: 18650  \tTraining Loss: -548.9062    \n",
            "    Negative Log Likelihood: 56.9863\tSigma2 Prior: -605.8936\tRegularization: 0.0011\n",
            "Iter: 18660  \tTraining Loss: -549.6055    \n",
            "    Negative Log Likelihood: 57.0949\tSigma2 Prior: -606.7015\tRegularization: 0.0011\n",
            "Iter: 18670  \tTraining Loss: -524.9386    \n",
            "    Negative Log Likelihood: 59.8010\tSigma2 Prior: -584.7407\tRegularization: 0.0011\n",
            "Iter: 18680  \tTraining Loss: -548.7137    \n",
            "    Negative Log Likelihood: 56.9399\tSigma2 Prior: -605.6547\tRegularization: 0.0011\n",
            "Iter: 18690  \tTraining Loss: -537.9468    \n",
            "    Negative Log Likelihood: 57.0601\tSigma2 Prior: -595.0079\tRegularization: 0.0011\n",
            "Iter: 18700  \tTraining Loss: -551.2216    \n",
            "    Negative Log Likelihood: 55.6240\tSigma2 Prior: -606.8467\tRegularization: 0.0011\n",
            "Iter: 18710  \tTraining Loss: -534.5496    \n",
            "    Negative Log Likelihood: 58.5777\tSigma2 Prior: -593.1284\tRegularization: 0.0011\n",
            "Iter: 18720  \tTraining Loss: -530.8948    \n",
            "    Negative Log Likelihood: 57.3607\tSigma2 Prior: -588.2567\tRegularization: 0.0011\n",
            "Iter: 18730  \tTraining Loss: -543.5213    \n",
            "    Negative Log Likelihood: 56.1750\tSigma2 Prior: -599.6974\tRegularization: 0.0011\n",
            "Iter: 18740  \tTraining Loss: -530.4471    \n",
            "    Negative Log Likelihood: 58.2441\tSigma2 Prior: -588.6923\tRegularization: 0.0011\n",
            "Iter: 18750  \tTraining Loss: -565.3599    \n",
            "    Negative Log Likelihood: 53.8297\tSigma2 Prior: -619.1907\tRegularization: 0.0011\n",
            "Iter: 18760  \tTraining Loss: -536.1915    \n",
            "    Negative Log Likelihood: 57.3046\tSigma2 Prior: -593.4972\tRegularization: 0.0011\n",
            "Iter: 18770  \tTraining Loss: -546.0687    \n",
            "    Negative Log Likelihood: 57.3022\tSigma2 Prior: -603.3720\tRegularization: 0.0011\n",
            "Iter: 18780  \tTraining Loss: -526.8879    \n",
            "    Negative Log Likelihood: 59.1725\tSigma2 Prior: -586.0615\tRegularization: 0.0011\n",
            "Iter: 18790  \tTraining Loss: -545.5257    \n",
            "    Negative Log Likelihood: 56.1574\tSigma2 Prior: -601.6842\tRegularization: 0.0011\n",
            "Iter: 18800  \tTraining Loss: -559.1869    \n",
            "    Negative Log Likelihood: 56.3188\tSigma2 Prior: -615.5068\tRegularization: 0.0011\n",
            "Iter: 18810  \tTraining Loss: -553.4288    \n",
            "    Negative Log Likelihood: 56.9357\tSigma2 Prior: -610.3656\tRegularization: 0.0011\n",
            "Iter: 18820  \tTraining Loss: -545.0375    \n",
            "    Negative Log Likelihood: 57.4294\tSigma2 Prior: -602.4680\tRegularization: 0.0011\n",
            "Iter: 18830  \tTraining Loss: -549.9394    \n",
            "    Negative Log Likelihood: 58.1123\tSigma2 Prior: -608.0528\tRegularization: 0.0011\n",
            "Iter: 18840  \tTraining Loss: -535.7122    \n",
            "    Negative Log Likelihood: 59.7373\tSigma2 Prior: -595.4506\tRegularization: 0.0011\n",
            "Iter: 18850  \tTraining Loss: -556.2649    \n",
            "    Negative Log Likelihood: 55.9738\tSigma2 Prior: -612.2397\tRegularization: 0.0011\n",
            "Iter: 18860  \tTraining Loss: -561.6857    \n",
            "    Negative Log Likelihood: 56.0983\tSigma2 Prior: -617.7851\tRegularization: 0.0011\n",
            "Iter: 18870  \tTraining Loss: -549.1280    \n",
            "    Negative Log Likelihood: 58.0988\tSigma2 Prior: -607.2279\tRegularization: 0.0011\n",
            "Iter: 18880  \tTraining Loss: -546.5440    \n",
            "    Negative Log Likelihood: 59.5398\tSigma2 Prior: -606.0848\tRegularization: 0.0011\n",
            "Iter: 18890  \tTraining Loss: -548.8109    \n",
            "    Negative Log Likelihood: 58.3072\tSigma2 Prior: -607.1193\tRegularization: 0.0011\n",
            "Iter: 18900  \tTraining Loss: -525.3785    \n",
            "    Negative Log Likelihood: 60.6745\tSigma2 Prior: -586.0541\tRegularization: 0.0011\n",
            "Iter: 18910  \tTraining Loss: -530.0731    \n",
            "    Negative Log Likelihood: 61.1801\tSigma2 Prior: -591.2543\tRegularization: 0.0011\n",
            "Iter: 18920  \tTraining Loss: -527.7014    \n",
            "    Negative Log Likelihood: 61.1574\tSigma2 Prior: -588.8599\tRegularization: 0.0011\n",
            "Iter: 18930  \tTraining Loss: -560.9979    \n",
            "    Negative Log Likelihood: 57.0921\tSigma2 Prior: -618.0911\tRegularization: 0.0011\n",
            "Iter: 18940  \tTraining Loss: -533.0554    \n",
            "    Negative Log Likelihood: 59.5473\tSigma2 Prior: -592.6038\tRegularization: 0.0011\n",
            "Iter: 18950  \tTraining Loss: -518.3420    \n",
            "    Negative Log Likelihood: 60.9133\tSigma2 Prior: -579.2563\tRegularization: 0.0011\n",
            "Iter: 18960  \tTraining Loss: -528.7899    \n",
            "    Negative Log Likelihood: 60.1651\tSigma2 Prior: -588.9560\tRegularization: 0.0011\n",
            "Iter: 18970  \tTraining Loss: -515.2279    \n",
            "    Negative Log Likelihood: 60.4278\tSigma2 Prior: -575.6567\tRegularization: 0.0011\n",
            "Iter: 18980  \tTraining Loss: -559.1714    \n",
            "    Negative Log Likelihood: 55.5374\tSigma2 Prior: -614.7098\tRegularization: 0.0011\n",
            "Iter: 18990  \tTraining Loss: -542.5378    \n",
            "    Negative Log Likelihood: 56.9590\tSigma2 Prior: -599.4979\tRegularization: 0.0011\n",
            "Iter: 19000  \tTraining Loss: -537.4520    \n",
            "    Negative Log Likelihood: 57.5524\tSigma2 Prior: -595.0055\tRegularization: 0.0011\n",
            "Iter: 19010  \tTraining Loss: -546.4630    \n",
            "    Negative Log Likelihood: 56.1746\tSigma2 Prior: -602.6387\tRegularization: 0.0011\n",
            "Iter: 19020  \tTraining Loss: -541.3020    \n",
            "    Negative Log Likelihood: 56.7441\tSigma2 Prior: -598.0472\tRegularization: 0.0011\n",
            "Iter: 19030  \tTraining Loss: -558.0559    \n",
            "    Negative Log Likelihood: 53.8217\tSigma2 Prior: -611.8787\tRegularization: 0.0011\n",
            "Iter: 19040  \tTraining Loss: -533.4851    \n",
            "    Negative Log Likelihood: 58.5673\tSigma2 Prior: -592.0535\tRegularization: 0.0011\n",
            "Iter: 19050  \tTraining Loss: -525.3657    \n",
            "    Negative Log Likelihood: 58.5236\tSigma2 Prior: -583.8904\tRegularization: 0.0011\n",
            "Iter: 19060  \tTraining Loss: -559.9041    \n",
            "    Negative Log Likelihood: 55.8960\tSigma2 Prior: -615.8012\tRegularization: 0.0011\n",
            "Iter: 19070  \tTraining Loss: -550.0582    \n",
            "    Negative Log Likelihood: 56.5937\tSigma2 Prior: -606.6530\tRegularization: 0.0011\n",
            "Iter: 19080  \tTraining Loss: -557.9184    \n",
            "    Negative Log Likelihood: 57.4634\tSigma2 Prior: -615.3829\tRegularization: 0.0011\n",
            "Iter: 19090  \tTraining Loss: -542.2107    \n",
            "    Negative Log Likelihood: 58.8404\tSigma2 Prior: -601.0521\tRegularization: 0.0011\n",
            "Iter: 19100  \tTraining Loss: -536.6433    \n",
            "    Negative Log Likelihood: 59.7545\tSigma2 Prior: -596.3989\tRegularization: 0.0011\n",
            "Iter: 19110  \tTraining Loss: -532.8900    \n",
            "    Negative Log Likelihood: 59.6304\tSigma2 Prior: -592.5215\tRegularization: 0.0011\n",
            "Iter: 19120  \tTraining Loss: -539.0944    \n",
            "    Negative Log Likelihood: 59.5538\tSigma2 Prior: -598.6493\tRegularization: 0.0011\n",
            "Iter: 19130  \tTraining Loss: -543.6799    \n",
            "    Negative Log Likelihood: 58.9866\tSigma2 Prior: -602.6677\tRegularization: 0.0011\n",
            "Iter: 19140  \tTraining Loss: -531.4475    \n",
            "    Negative Log Likelihood: 59.0890\tSigma2 Prior: -590.5376\tRegularization: 0.0011\n",
            "Iter: 19150  \tTraining Loss: -553.5539    \n",
            "    Negative Log Likelihood: 56.8550\tSigma2 Prior: -610.4100\tRegularization: 0.0011\n",
            "Iter: 19160  \tTraining Loss: -510.6309    \n",
            "    Negative Log Likelihood: 61.4084\tSigma2 Prior: -572.0404\tRegularization: 0.0011\n",
            "Iter: 19170  \tTraining Loss: -531.4313    \n",
            "    Negative Log Likelihood: 59.0869\tSigma2 Prior: -590.5193\tRegularization: 0.0011\n",
            "Iter: 19180  \tTraining Loss: -546.0750    \n",
            "    Negative Log Likelihood: 57.0205\tSigma2 Prior: -603.0966\tRegularization: 0.0011\n",
            "Iter: 19190  \tTraining Loss: -553.0410    \n",
            "    Negative Log Likelihood: 55.6816\tSigma2 Prior: -608.7236\tRegularization: 0.0011\n",
            "Iter: 19200  \tTraining Loss: -542.4680    \n",
            "    Negative Log Likelihood: 58.9127\tSigma2 Prior: -601.3818\tRegularization: 0.0011\n",
            "Iter: 19210  \tTraining Loss: -517.7207    \n",
            "    Negative Log Likelihood: 60.5813\tSigma2 Prior: -578.3031\tRegularization: 0.0011\n",
            "Iter: 19220  \tTraining Loss: -543.9081    \n",
            "    Negative Log Likelihood: 57.3087\tSigma2 Prior: -601.2179\tRegularization: 0.0011\n",
            "Iter: 19230  \tTraining Loss: -550.5868    \n",
            "    Negative Log Likelihood: 56.9958\tSigma2 Prior: -607.5837\tRegularization: 0.0011\n",
            "Iter: 19240  \tTraining Loss: -533.4954    \n",
            "    Negative Log Likelihood: 59.7943\tSigma2 Prior: -593.2908\tRegularization: 0.0011\n",
            "Iter: 19250  \tTraining Loss: -547.6938    \n",
            "    Negative Log Likelihood: 58.2237\tSigma2 Prior: -605.9186\tRegularization: 0.0011\n",
            "Iter: 19260  \tTraining Loss: -557.8807    \n",
            "    Negative Log Likelihood: 54.8976\tSigma2 Prior: -612.7794\tRegularization: 0.0011\n",
            "Iter: 19270  \tTraining Loss: -538.7089    \n",
            "    Negative Log Likelihood: 57.2478\tSigma2 Prior: -595.9578\tRegularization: 0.0011\n",
            "Iter: 19280  \tTraining Loss: -550.7944    \n",
            "    Negative Log Likelihood: 56.0534\tSigma2 Prior: -606.8489\tRegularization: 0.0011\n",
            "Iter: 19290  \tTraining Loss: -548.3169    \n",
            "    Negative Log Likelihood: 56.2795\tSigma2 Prior: -604.5975\tRegularization: 0.0011\n",
            "Iter: 19300  \tTraining Loss: -547.1728    \n",
            "    Negative Log Likelihood: 57.0831\tSigma2 Prior: -604.2570\tRegularization: 0.0011\n",
            "Iter: 19310  \tTraining Loss: -535.1179    \n",
            "    Negative Log Likelihood: 59.0562\tSigma2 Prior: -594.1751\tRegularization: 0.0011\n",
            "Iter: 19320  \tTraining Loss: -545.6009    \n",
            "    Negative Log Likelihood: 57.2313\tSigma2 Prior: -602.8333\tRegularization: 0.0011\n",
            "Iter: 19330  \tTraining Loss: -541.1675    \n",
            "    Negative Log Likelihood: 57.3703\tSigma2 Prior: -598.5389\tRegularization: 0.0011\n",
            "Iter: 19340  \tTraining Loss: -563.3997    \n",
            "    Negative Log Likelihood: 55.5164\tSigma2 Prior: -618.9171\tRegularization: 0.0011\n",
            "Iter: 19350  \tTraining Loss: -534.3019    \n",
            "    Negative Log Likelihood: 57.9620\tSigma2 Prior: -592.2650\tRegularization: 0.0011\n",
            "Iter: 19360  \tTraining Loss: -545.1573    \n",
            "    Negative Log Likelihood: 56.6617\tSigma2 Prior: -601.8202\tRegularization: 0.0011\n",
            "Iter: 19370  \tTraining Loss: -547.4020    \n",
            "    Negative Log Likelihood: 57.0258\tSigma2 Prior: -604.4289\tRegularization: 0.0011\n",
            "Iter: 19380  \tTraining Loss: -524.8319    \n",
            "    Negative Log Likelihood: 60.0826\tSigma2 Prior: -584.9156\tRegularization: 0.0011\n",
            "Iter: 19390  \tTraining Loss: -550.0274    \n",
            "    Negative Log Likelihood: 57.3253\tSigma2 Prior: -607.3538\tRegularization: 0.0011\n",
            "Iter: 19400  \tTraining Loss: -535.5696    \n",
            "    Negative Log Likelihood: 58.7670\tSigma2 Prior: -594.3377\tRegularization: 0.0011\n",
            "Iter: 19410  \tTraining Loss: -546.1702    \n",
            "    Negative Log Likelihood: 58.4157\tSigma2 Prior: -604.5870\tRegularization: 0.0011\n",
            "Iter: 19420  \tTraining Loss: -526.2721    \n",
            "    Negative Log Likelihood: 59.7023\tSigma2 Prior: -585.9755\tRegularization: 0.0011\n",
            "Iter: 19430  \tTraining Loss: -542.3915    \n",
            "    Negative Log Likelihood: 59.2821\tSigma2 Prior: -601.6747\tRegularization: 0.0011\n",
            "Iter: 19440  \tTraining Loss: -546.9941    \n",
            "    Negative Log Likelihood: 58.3158\tSigma2 Prior: -605.3110\tRegularization: 0.0011\n",
            "Iter: 19450  \tTraining Loss: -543.1334    \n",
            "    Negative Log Likelihood: 57.6776\tSigma2 Prior: -600.8121\tRegularization: 0.0011\n",
            "Iter: 19460  \tTraining Loss: -545.8186    \n",
            "    Negative Log Likelihood: 58.1495\tSigma2 Prior: -603.9692\tRegularization: 0.0011\n",
            "Iter: 19470  \tTraining Loss: -538.1257    \n",
            "    Negative Log Likelihood: 59.1872\tSigma2 Prior: -597.3140\tRegularization: 0.0011\n",
            "Iter: 19480  \tTraining Loss: -551.6293    \n",
            "    Negative Log Likelihood: 57.6060\tSigma2 Prior: -609.2364\tRegularization: 0.0011\n",
            "Iter: 19490  \tTraining Loss: -547.8669    \n",
            "    Negative Log Likelihood: 57.0532\tSigma2 Prior: -604.9213\tRegularization: 0.0011\n",
            "Iter: 19500  \tTraining Loss: -537.0574    \n",
            "    Negative Log Likelihood: 57.7803\tSigma2 Prior: -594.8389\tRegularization: 0.0011\n",
            "Iter: 19510  \tTraining Loss: -545.4941    \n",
            "    Negative Log Likelihood: 57.3354\tSigma2 Prior: -602.8306\tRegularization: 0.0011\n",
            "Iter: 19520  \tTraining Loss: -534.3721    \n",
            "    Negative Log Likelihood: 58.3695\tSigma2 Prior: -592.7427\tRegularization: 0.0011\n",
            "Iter: 19530  \tTraining Loss: -542.5759    \n",
            "    Negative Log Likelihood: 56.8717\tSigma2 Prior: -599.4487\tRegularization: 0.0011\n",
            "Iter: 19540  \tTraining Loss: -543.9757    \n",
            "    Negative Log Likelihood: 57.7020\tSigma2 Prior: -601.6788\tRegularization: 0.0011\n",
            "Iter: 19550  \tTraining Loss: -544.8822    \n",
            "    Negative Log Likelihood: 57.3097\tSigma2 Prior: -602.1931\tRegularization: 0.0011\n",
            "Iter: 19560  \tTraining Loss: -549.5206    \n",
            "    Negative Log Likelihood: 57.0659\tSigma2 Prior: -606.5876\tRegularization: 0.0011\n",
            "Iter: 19570  \tTraining Loss: -531.3514    \n",
            "    Negative Log Likelihood: 58.7155\tSigma2 Prior: -590.0679\tRegularization: 0.0011\n",
            "Iter: 19580  \tTraining Loss: -540.8344    \n",
            "    Negative Log Likelihood: 57.0266\tSigma2 Prior: -597.8621\tRegularization: 0.0011\n",
            "Iter: 19590  \tTraining Loss: -524.2498    \n",
            "    Negative Log Likelihood: 59.8092\tSigma2 Prior: -584.0601\tRegularization: 0.0011\n",
            "Iter: 19600  \tTraining Loss: -571.8790    \n",
            "    Negative Log Likelihood: 54.1855\tSigma2 Prior: -626.0656\tRegularization: 0.0011\n",
            "Iter: 19610  \tTraining Loss: -546.3384    \n",
            "    Negative Log Likelihood: 57.3528\tSigma2 Prior: -603.6923\tRegularization: 0.0011\n",
            "Iter: 19620  \tTraining Loss: -554.0712    \n",
            "    Negative Log Likelihood: 55.7277\tSigma2 Prior: -609.8000\tRegularization: 0.0011\n",
            "Iter: 19630  \tTraining Loss: -556.8612    \n",
            "    Negative Log Likelihood: 56.3152\tSigma2 Prior: -613.1775\tRegularization: 0.0011\n",
            "Iter: 19640  \tTraining Loss: -550.1740    \n",
            "    Negative Log Likelihood: 56.4382\tSigma2 Prior: -606.6133\tRegularization: 0.0011\n",
            "Iter: 19650  \tTraining Loss: -541.6457    \n",
            "    Negative Log Likelihood: 59.7457\tSigma2 Prior: -601.3925\tRegularization: 0.0011\n",
            "Iter: 19660  \tTraining Loss: -539.9552    \n",
            "    Negative Log Likelihood: 58.0141\tSigma2 Prior: -597.9705\tRegularization: 0.0011\n",
            "Iter: 19670  \tTraining Loss: -540.7438    \n",
            "    Negative Log Likelihood: 57.6228\tSigma2 Prior: -598.3677\tRegularization: 0.0011\n",
            "Iter: 19680  \tTraining Loss: -552.9816    \n",
            "    Negative Log Likelihood: 56.3941\tSigma2 Prior: -609.3768\tRegularization: 0.0011\n",
            "Iter: 19690  \tTraining Loss: -541.6066    \n",
            "    Negative Log Likelihood: 58.3847\tSigma2 Prior: -599.9924\tRegularization: 0.0011\n",
            "Iter: 19700  \tTraining Loss: -544.8873    \n",
            "    Negative Log Likelihood: 59.7093\tSigma2 Prior: -604.5977\tRegularization: 0.0011\n",
            "Iter: 19710  \tTraining Loss: -557.3196    \n",
            "    Negative Log Likelihood: 56.5014\tSigma2 Prior: -613.8221\tRegularization: 0.0011\n",
            "Iter: 19720  \tTraining Loss: -542.1568    \n",
            "    Negative Log Likelihood: 57.7431\tSigma2 Prior: -599.9009\tRegularization: 0.0011\n",
            "Iter: 19730  \tTraining Loss: -561.7640    \n",
            "    Negative Log Likelihood: 56.3211\tSigma2 Prior: -618.0862\tRegularization: 0.0011\n",
            "Iter: 19740  \tTraining Loss: -555.4114    \n",
            "    Negative Log Likelihood: 57.1691\tSigma2 Prior: -612.5816\tRegularization: 0.0011\n",
            "Iter: 19750  \tTraining Loss: -523.9529    \n",
            "    Negative Log Likelihood: 59.9185\tSigma2 Prior: -583.8725\tRegularization: 0.0011\n",
            "Iter: 19760  \tTraining Loss: -545.4415    \n",
            "    Negative Log Likelihood: 57.7076\tSigma2 Prior: -603.1502\tRegularization: 0.0011\n",
            "Iter: 19770  \tTraining Loss: -547.6845    \n",
            "    Negative Log Likelihood: 57.9144\tSigma2 Prior: -605.6000\tRegularization: 0.0011\n",
            "Iter: 19780  \tTraining Loss: -534.9337    \n",
            "    Negative Log Likelihood: 58.9811\tSigma2 Prior: -593.9160\tRegularization: 0.0011\n",
            "Iter: 19790  \tTraining Loss: -535.4991    \n",
            "    Negative Log Likelihood: 58.0461\tSigma2 Prior: -593.5464\tRegularization: 0.0011\n",
            "Iter: 19800  \tTraining Loss: -567.1545    \n",
            "    Negative Log Likelihood: 55.0373\tSigma2 Prior: -622.1929\tRegularization: 0.0011\n",
            "Iter: 19810  \tTraining Loss: -538.1132    \n",
            "    Negative Log Likelihood: 58.1431\tSigma2 Prior: -596.2574\tRegularization: 0.0011\n",
            "Iter: 19820  \tTraining Loss: -538.5789    \n",
            "    Negative Log Likelihood: 58.4233\tSigma2 Prior: -597.0033\tRegularization: 0.0011\n",
            "Iter: 19830  \tTraining Loss: -517.1646    \n",
            "    Negative Log Likelihood: 59.8195\tSigma2 Prior: -576.9852\tRegularization: 0.0011\n",
            "Iter: 19840  \tTraining Loss: -523.0607    \n",
            "    Negative Log Likelihood: 58.0577\tSigma2 Prior: -581.1196\tRegularization: 0.0011\n",
            "Iter: 19850  \tTraining Loss: -541.7941    \n",
            "    Negative Log Likelihood: 57.5312\tSigma2 Prior: -599.3265\tRegularization: 0.0011\n",
            "Iter: 19860  \tTraining Loss: -529.7696    \n",
            "    Negative Log Likelihood: 57.4193\tSigma2 Prior: -587.1901\tRegularization: 0.0011\n",
            "Iter: 19870  \tTraining Loss: -549.1673    \n",
            "    Negative Log Likelihood: 56.0354\tSigma2 Prior: -605.2039\tRegularization: 0.0011\n",
            "Iter: 19880  \tTraining Loss: -529.9077    \n",
            "    Negative Log Likelihood: 58.0262\tSigma2 Prior: -587.9351\tRegularization: 0.0011\n",
            "Iter: 19890  \tTraining Loss: -534.5588    \n",
            "    Negative Log Likelihood: 58.2193\tSigma2 Prior: -592.7792\tRegularization: 0.0011\n",
            "Iter: 19900  \tTraining Loss: -544.1024    \n",
            "    Negative Log Likelihood: 57.9176\tSigma2 Prior: -602.0211\tRegularization: 0.0011\n",
            "Iter: 19910  \tTraining Loss: -550.7177    \n",
            "    Negative Log Likelihood: 57.3873\tSigma2 Prior: -608.1062\tRegularization: 0.0011\n",
            "Iter: 19920  \tTraining Loss: -554.5164    \n",
            "    Negative Log Likelihood: 57.0163\tSigma2 Prior: -611.5338\tRegularization: 0.0011\n",
            "Iter: 19930  \tTraining Loss: -544.6744    \n",
            "    Negative Log Likelihood: 57.8999\tSigma2 Prior: -602.5755\tRegularization: 0.0011\n",
            "Iter: 19940  \tTraining Loss: -530.3334    \n",
            "    Negative Log Likelihood: 58.6442\tSigma2 Prior: -588.9788\tRegularization: 0.0011\n",
            "Iter: 19950  \tTraining Loss: -551.0704    \n",
            "    Negative Log Likelihood: 55.7023\tSigma2 Prior: -606.7739\tRegularization: 0.0011\n",
            "Iter: 19960  \tTraining Loss: -550.1782    \n",
            "    Negative Log Likelihood: 57.0711\tSigma2 Prior: -607.2505\tRegularization: 0.0011\n",
            "Iter: 19970  \tTraining Loss: -539.0183    \n",
            "    Negative Log Likelihood: 57.5170\tSigma2 Prior: -596.5365\tRegularization: 0.0011\n",
            "Iter: 19980  \tTraining Loss: -524.1216    \n",
            "    Negative Log Likelihood: 59.6738\tSigma2 Prior: -583.7966\tRegularization: 0.0011\n",
            "Iter: 19990  \tTraining Loss: -553.1625    \n",
            "    Negative Log Likelihood: 55.8226\tSigma2 Prior: -608.9863\tRegularization: 0.0011\n",
            "Iter: 19999  \tTraining Loss: -520.3616    \n",
            "    Negative Log Likelihood: 59.2010\tSigma2 Prior: -579.5638\tRegularization: 0.0011\n",
            "Done training with 20000 iterations\n",
            "<class 'list'>\n",
            "['15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_0', '15_0', '15_0', '15_0', '15_0', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1']\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "{'15_0': 0, '15_1': 1, '15_2': 2}\n",
            "{0: 0, 1: 1, 2: 2}\n",
            "Ground truth labels:\n",
            "['15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_0', '15_0', '15_0', '15_0', '15_0', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1']\n",
            "Predicted labels:\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_1']\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
            "{'30_0': 0, '30_1': 1}\n",
            "{0: 0, 1: 1}\n",
            "Ground truth labels:\n",
            "['30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_1']\n",
            "Predicted labels:\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1']\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "{'52_0': 0, '52_1': 1, '52_2': 2}\n",
            "{0: 0, 1: 1, 2: 2}\n",
            "Ground truth labels:\n",
            "['52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1']\n",
            "Predicted labels:\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1']\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2]\n",
            "{'71_0': 0, '71_1': 1, '71_2': 2}\n",
            "{0: 0, 1: 1, 2: 2}\n",
            "Ground truth labels:\n",
            "['71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1']\n",
            "Predicted labels:\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_2', '75_2', '75_2', '75_2', '75_2', '75_2', '75_2', '75_2', '75_2', '75_2', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_3', '75_3', '75_3', '75_3', '75_3', '75_3', '75_3', '75_3', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_2', '75_2', '75_2', '75_2', '75_2', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_3', '75_3', '75_3', '75_3', '75_3', '75_1', '75_1', '75_1', '75_1', '75_1', '75_1', '75_1', '75_1', '75_2', '75_2', '75_2']\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 1, 1, 1]\n",
            "{'75_0': 0, '75_1': 1, '75_2': 2, '75_3': 3, '75_4': 4}\n",
            "{0: 0, 1: 1, 2: 2, 3: 3, 4: 4}\n",
            "Ground truth labels:\n",
            "['75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_2', '75_2', '75_2', '75_2', '75_2', '75_2', '75_2', '75_2', '75_2', '75_2', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_3', '75_3', '75_3', '75_3', '75_3', '75_3', '75_3', '75_3', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_2', '75_2', '75_2', '75_2', '75_2', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_3', '75_3', '75_3', '75_3', '75_3', '75_1', '75_1', '75_1', '75_1', '75_1', '75_1', '75_1', '75_1', '75_2', '75_2', '75_2']\n",
            "Predicted labels:\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 1, 1, 1]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4']\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
            "{'80_0': 0, '80_3': 1, '80_4': 2, '80_5': 3}\n",
            "{0: 0, 1: 1, 2: 2, 3: 3}\n",
            "Ground truth labels:\n",
            "['80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4']\n",
            "Predicted labels:\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_2', '140_2', '140_2', '140_2', '140_2', '140_5', '140_5', '140_5', '140_5', '140_5', '140_5', '140_5', '140_5', '140_5', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4']\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "{'140_0': 0, '140_1': 1, '140_2': 2, '140_4': 3, '140_5': 4}\n",
            "{0: 0, 1: 1, 2: 2, 3: 3, 4: 4}\n",
            "Ground truth labels:\n",
            "['140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_2', '140_2', '140_2', '140_2', '140_2', '140_5', '140_5', '140_5', '140_5', '140_5', '140_5', '140_5', '140_5', '140_5', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4']\n",
            "Predicted labels:\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1']\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "{'146_0': 0, '146_1': 1}\n",
            "{0: 0, 1: 1}\n",
            "Ground truth labels:\n",
            "['146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1']\n",
            "Predicted labels:\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_2', '152_2', '152_2', '152_2']\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3]\n",
            "{'152_0': 0, '152_1': 1, '152_2': 2, '152_4': 3}\n",
            "{0: 0, 1: 1, 2: 2, 3: 3}\n",
            "Ground truth labels:\n",
            "['152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_2', '152_2', '152_2', '152_2']\n",
            "Predicted labels:\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['175_0', '175_0', '175_0', '175_0', '175_0', '175_0', '175_0', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_0', '175_0', '175_0', '175_0', '175_0', '175_0', '175_0', '175_0', '175_0', '175_0', '175_1', '175_1', '175_1', '175_1']\n",
            "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2]\n",
            "{'175_0': 0, '175_1': 1, '175_2': 2}\n",
            "{0: 0, 1: 1, 2: 2}\n",
            "Ground truth labels:\n",
            "['175_0', '175_0', '175_0', '175_0', '175_0', '175_0', '175_0', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_0', '175_0', '175_0', '175_0', '175_0', '175_0', '175_0', '175_0', '175_0', '175_0', '175_1', '175_1', '175_1', '175_1']\n",
            "Predicted labels:\n",
            "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_0', '204_0', '204_0', '204_0', '204_0', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_2', '204_2', '204_2', '204_2', '204_2', '204_2', '204_2', '204_2', '204_2', '204_2', '204_2', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0']\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "{'204_0': 0, '204_1': 1, '204_2': 2, '204_3': 3}\n",
            "{0: 0, 1: 1, 2: 2, 3: 3, 4: 4}\n",
            "Ground truth labels:\n",
            "['204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_0', '204_0', '204_0', '204_0', '204_0', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_2', '204_2', '204_2', '204_2', '204_2', '204_2', '204_2', '204_2', '204_2', '204_2', '204_2', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0']\n",
            "Predicted labels:\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['315_0', '315_0', '315_0', '315_0', '315_0', '315_0', '315_0', '315_0', '315_0', '315_0', '315_0', '315_0', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_4', '315_4', '315_4', '315_4', '315_4', '315_4', '315_4', '315_4', '315_4', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_5', '315_5', '315_5', '315_5']\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4]\n",
            "{'315_0': 0, '315_1': 1, '315_2': 2, '315_4': 3, '315_5': 4}\n",
            "{0: 0, 1: 1, 2: 2, 3: 3, 4: 4}\n",
            "Ground truth labels:\n",
            "['315_0', '315_0', '315_0', '315_0', '315_0', '315_0', '315_0', '315_0', '315_0', '315_0', '315_0', '315_0', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_4', '315_4', '315_4', '315_4', '315_4', '315_4', '315_4', '315_4', '315_4', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_5', '315_5', '315_5', '315_5']\n",
            "Predicted labels:\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1']\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "{'333_0': 0, '333_1': 1}\n",
            "{0: 0, 1: 1}\n",
            "Ground truth labels:\n",
            "['333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1']\n",
            "Predicted labels:\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0']\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "{'347_0': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0']\n",
            "Predicted labels:\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1']\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "{'361_0': 0, '361_1': 1, '361_2': 2, '361_3': 3}\n",
            "{0: 0, 1: 1, 2: 2, 3: 3}\n",
            "Ground truth labels:\n",
            "['361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1']\n",
            "Predicted labels:\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_1', '374_1', '374_1', '374_1', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_0', '374_0', '374_0']\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
            "{'374_0': 0, '374_1': 1}\n",
            "{0: 0, 1: 1}\n",
            "Ground truth labels:\n",
            "['374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_1', '374_1', '374_1', '374_1', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_0', '374_0', '374_0']\n",
            "Predicted labels:\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_2', '405_2', '405_2', '405_2', '405_2', '405_2', '405_2', '405_2', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_2', '405_2', '405_2', '405_2', '405_2', '405_2', '405_2', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_1', '405_1', '405_1', '405_1']\n",
            "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n",
            "{'405_0': 0, '405_1': 1, '405_2': 2}\n",
            "{0: 0, 1: 1, 2: 2}\n",
            "Ground truth labels:\n",
            "['405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_2', '405_2', '405_2', '405_2', '405_2', '405_2', '405_2', '405_2', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_2', '405_2', '405_2', '405_2', '405_2', '405_2', '405_2', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_1', '405_1', '405_1', '405_1']\n",
            "Predicted labels:\n",
            "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['419_0', '419_0', '419_0', '419_0', '419_0', '419_0', '419_0', '419_0', '419_0', '419_0', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_4', '419_4', '419_4', '419_4', '419_4', '419_4', '419_4', '419_4', '419_4', '419_4', '419_4', '419_3', '419_3', '419_3', '419_3', '419_3', '419_3', '419_3', '419_3', '419_3', '419_3', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_2', '419_2', '419_2', '419_2', '419_2', '419_2', '419_2', '419_0', '419_0', '419_0', '419_0', '419_0', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_4', '419_4', '419_4', '419_4', '419_4', '419_4']\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 2, 2, 2, 2, 2]\n",
            "{'419_0': 0, '419_1': 1, '419_2': 2, '419_3': 3, '419_4': 4, '419_6': 5}\n",
            "{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}\n",
            "Ground truth labels:\n",
            "['419_0', '419_0', '419_0', '419_0', '419_0', '419_0', '419_0', '419_0', '419_0', '419_0', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_4', '419_4', '419_4', '419_4', '419_4', '419_4', '419_4', '419_4', '419_4', '419_4', '419_4', '419_3', '419_3', '419_3', '419_3', '419_3', '419_3', '419_3', '419_3', '419_3', '419_3', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_2', '419_2', '419_2', '419_2', '419_2', '419_2', '419_2', '419_0', '419_0', '419_0', '419_0', '419_0', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_4', '419_4', '419_4', '419_4', '419_4', '419_4']\n",
            "Predicted labels:\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 2, 2, 2, 2, 2]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0']\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "{'430_0': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0']\n",
            "Predicted labels:\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['439_0', '439_0', '439_0', '439_0', '439_0', '439_0', '439_0', '439_0', '439_0', '439_2', '439_2', '439_2', '439_2', '439_2', '439_2', '439_2', '439_2', '439_2', '439_2', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_2', '439_2', '439_2', '439_2', '439_2', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_2', '439_2', '439_2', '439_2', '439_2', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4']\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "{'439_0': 0, '439_1': 1, '439_2': 2, '439_3': 3, '439_4': 4}\n",
            "{0: 0, 1: 1, 2: 2, 3: 3, 4: 4}\n",
            "Ground truth labels:\n",
            "['439_0', '439_0', '439_0', '439_0', '439_0', '439_0', '439_0', '439_0', '439_0', '439_2', '439_2', '439_2', '439_2', '439_2', '439_2', '439_2', '439_2', '439_2', '439_2', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_2', '439_2', '439_2', '439_2', '439_2', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_2', '439_2', '439_2', '439_2', '439_2', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4']\n",
            "Predicted labels:\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0']\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "{'441_0': 0, '441_1': 1}\n",
            "{0: 0, 1: 1}\n",
            "Ground truth labels:\n",
            "['441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0']\n",
            "Predicted labels:\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0']\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "{'442_0': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0']\n",
            "Predicted labels:\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1']\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "{'459_0': 0, '459_1': 1}\n",
            "{0: 0, 1: 1}\n",
            "Ground truth labels:\n",
            "['459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1']\n",
            "Predicted labels:\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1']\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "{'491_0': 0, '491_1': 1, '491_3': 2}\n",
            "{0: 0, 1: 1, 2: 2}\n",
            "Ground truth labels:\n",
            "['491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1']\n",
            "Predicted labels:\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_2', '494_2', '494_2', '494_2', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_2', '494_2', '494_2', '494_2', '494_2', '494_2', '494_2', '494_2', '494_2', '494_2', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_0', '494_0']\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n",
            "{'494_0': 0, '494_1': 1, '494_2': 2}\n",
            "{0: 0, 1: 1, 2: 2}\n",
            "Ground truth labels:\n",
            "['494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_2', '494_2', '494_2', '494_2', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_2', '494_2', '494_2', '494_2', '494_2', '494_2', '494_2', '494_2', '494_2', '494_2', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_0', '494_0']\n",
            "Predicted labels:\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n",
            "--------------------------------------------------------------------------------\n",
            "Finished diarization experiment\n",
            "Config:\n",
            "  sigma_alpha: 1\n",
            "  sigma_beta: 1\n",
            "  crp_alpha: 1.0\n",
            "  learning rate: 1e-05\n",
            "  regularization: 1e-05\n",
            "  batch size: 20\n",
            "Performance:\n",
            "  averaged accuracy: 0.999574\n",
            "  accuracy numbers for all testing sequences:\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    0.989362\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#test toy set\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import pickle\n",
        "# training model on toy dataset\n",
        "SAVED_MODEL_NAME = '/content/saved_model_toy.uisrnn'\n",
        "test=np.load('/content/toy_testing_data.npz',allow_pickle=True)\n",
        "train=np.load('/content/toy_training_data.npz',allow_pickle=True)\n",
        "def diarization_experiment(args,train,test):\n",
        "  \"\"\"Experiment pipeline.\n",
        "  Load data --> train model --> test model --> output result\n",
        "  Args:\n",
        "    model_args: model configurations\n",
        "    training_args: training configurations\n",
        "    inference_args: inference configurations\n",
        "  \"\"\"\n",
        "\n",
        "  predicted_cluster_ids = []\n",
        "  test_record = []\n",
        "\n",
        "  train_sequence = train[train.files[0]]\n",
        "  test_sequence= test[test.files[0]]\n",
        "  train_cluster_id= train[train.files[1]]\n",
        "  test_cluster_id= test[test.files[1]]\n",
        "  #test_cluster_id=test_cluster_id.reshape(-1,1)\n",
        "  model = UISRNN(args)\n",
        "  # training\n",
        "  model.fit(train_sequence, train_cluster_id, args)\n",
        "  model.save(SAVED_MODEL_NAME)\n",
        "  # we can also skip training by calling：\n",
        "  #model.load(SAVED_MODEL_NAME)\n",
        "  # testing\n",
        "  for (test_sequence, test_cluster_id) in zip(test_sequence, test_cluster_id):\n",
        "    predicted_cluster_id = model.predict(test_sequence, args)\n",
        "    predicted_cluster_ids.append(predicted_cluster_id)\n",
        "    print(type(predicted_cluster_id))\n",
        "    accuracy = compute_sequence_match_accuracy(\n",
        "        test_cluster_id, predicted_cluster_id)\n",
        "    test_record.append((accuracy, len(test_cluster_id)))\n",
        "    print('Ground truth labels:')\n",
        "    print(test_cluster_id)\n",
        "    print('Predicted labels:')\n",
        "    print(predicted_cluster_id)\n",
        "    print('-' * 80)\n",
        "  output_string = output_result(args,args,test_record)\n",
        "\n",
        "  print('Finished diarization experiment')\n",
        "  print(output_string)\n",
        "args=arguments()\n",
        "diarization_experiment(args,train,test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRcQCDOEcaey"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "def save_files_on_drive(filename, is_model):\n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    drive = GoogleDrive(gauth)\n",
        "\n",
        "    if is_model:\n",
        "        model = UISRNN(args)\n",
        "        model.load(filename)\n",
        "        # Save the model locally (in Colab directory)\n",
        "        model.save(filename)\n",
        "        # Create a file on Google Drive\n",
        "        files = drive.CreateFile({'title': filename})\n",
        "        files.SetContentFile(filename)\n",
        "        files.Upload()\n",
        "        # Download the file to Google Drive\n",
        "        drive.CreateFile({'id': files.get('id')})\n",
        "    else:\n",
        "        d = np.load(filename)\n",
        "        # Save the NumPy array as a temporary file\n",
        "        temp_filename = \"temp.npy\"\n",
        "        np.save(temp_filename, d)\n",
        "        # Create a file on Google Drive\n",
        "        files = drive.CreateFile({'title': filename})\n",
        "        # Set the content file with the temporary file\n",
        "        files.SetContentFile(temp_filename)\n",
        "        files.Upload()\n",
        "        # Download the file to Google Drive\n",
        "        drive.CreateFile({'id': files.get('id')})\n",
        "\n",
        "# Assuming these files exist in the current directory\n",
        "save_files_on_drive(\"saved_model_toy.uisrnn\", True)\n",
        "save_files_on_drive(\"saved_model_timit.uisrnn\", True)\n",
        "save_files_on_drive(\"train_sequence.npy\", False)\n",
        "save_files_on_drive(\"test_sequence.npy\", False)\n",
        "save_files_on_drive(\"train_cluster_id.npy\", False)\n",
        "save_files_on_drive(\"test_cluster_id.npy\", False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pz3JJ0lycauP",
        "outputId": "63043511-72c1-41b9-d0a1-3a9ab9f0b589"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['453']\n",
            "[0]\n",
            "{'453': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['453']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['453']\n",
            "[0]\n",
            "{'453': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['453']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['453']\n",
            "[0]\n",
            "{'453': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['453']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['453']\n",
            "[0]\n",
            "{'453': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['453']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['453']\n",
            "[0]\n",
            "{'453': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['453']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['453']\n",
            "[0]\n",
            "{'453': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['453']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['453']\n",
            "[0]\n",
            "{'453': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['453']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['454']\n",
            "[0]\n",
            "{'454': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['454']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['455']\n",
            "[0]\n",
            "{'455': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['455']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['456']\n",
            "[0]\n",
            "{'456': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['456']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['457']\n",
            "[0]\n",
            "{'457': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['457']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['458']\n",
            "[0]\n",
            "{'458': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['458']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['459']\n",
            "[0]\n",
            "{'459': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['459']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['460']\n",
            "[0]\n",
            "{'460': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['460']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['461']\n",
            "[0]\n",
            "{'461': 0}\n",
            "{0: 0}\n",
            "Ground truth labels:\n",
            "['461']\n",
            "Predicted labels:\n",
            "[0]\n",
            "--------------------------------------------------------------------------------\n",
            "Finished diarization experiment\n",
            "Config:\n",
            "  sigma_alpha: 1\n",
            "  sigma_beta: 1\n",
            "  crp_alpha: 1.0\n",
            "  learning rate: 1e-05\n",
            "  regularization: 1e-05\n",
            "  batch size: 20\n",
            "Performance:\n",
            "  averaged accuracy: 1.000000\n",
            "  accuracy numbers for all testing sequences:\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "    1.000000\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "SAVED_MODEL_NAME = '/content/saved_model_toy.uisrnn'\n",
        "#model trained on toy data set produced same results as the model trained on timit dataset\n",
        "\n",
        "def diarization_experiment(args):\n",
        "  \"\"\"Experiment pipeline.\n",
        "  Load data --> train model --> test model --> output result\n",
        "  Args:\n",
        "    model_args: model configurations\n",
        "    training_args: training configurations\n",
        "    inference_args: inference configurations\n",
        "  \"\"\"\n",
        "\n",
        "  predicted_cluster_ids = []\n",
        "  test_record = []\n",
        "\n",
        "  train_sequence = np.load('/content/drive/My Drive/train_sequence.npy')\n",
        "  test_sequence= np.load('/content/drive/My Drive/test_sequence.npy')\n",
        "  train_cluster_id= np.load('/content/drive/My Drive/train_cluster_id.npy')\n",
        "  test_cluster_id= np.load('/content/drive/My Drive/test_cluster_id.npy')\n",
        "  test_cluster_id=test_cluster_id.reshape(-1,1)\n",
        "  model = UISRNN(args)\n",
        "  # training\n",
        "  #model.fit(train_sequence, train_cluster_id, args)\n",
        "  #model.save(SAVED_MODEL_NAME)\n",
        "  # we can also skip training by calling：\n",
        "  model.load(SAVED_MODEL_NAME)\n",
        "\n",
        "  # testing\n",
        "  for (test_sequence, test_cluster_id) in zip(test_sequence, test_cluster_id):\n",
        "    predicted_cluster_id = model.predict(test_sequence.reshape(1,-1), args)\n",
        "    predicted_cluster_ids.append(predicted_cluster_id)\n",
        "    print(type(predicted_cluster_id))\n",
        "    accuracy = compute_sequence_match_accuracy(\n",
        "        test_cluster_id, predicted_cluster_id)\n",
        "    test_record.append((accuracy, len(test_cluster_id)))\n",
        "    print('Ground truth labels:')\n",
        "    print(test_cluster_id)\n",
        "    print('Predicted labels:')\n",
        "    print(predicted_cluster_id)\n",
        "    print('-' * 80)\n",
        "  output_string = output_result(args,args,test_record)\n",
        "\n",
        "  print('Finished diarization experiment')\n",
        "  print(output_string)\n",
        "args=arguments()\n",
        "diarization_experiment(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQwFVLCPcarh",
        "outputId": "1e8b7e49-c839-43c3-f74a-957a2649962c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "['15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_0', '15_0', '15_0', '15_0', '15_0', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1']\n",
            "[95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189]\n",
            "{'15_0': 0, '15_1': 1, '15_2': 2}\n",
            "{95: 0, 96: 1, 97: 2, 98: 3, 99: 4, 100: 5, 101: 6, 102: 7, 103: 8, 104: 9, 105: 10, 106: 11, 107: 12, 108: 13, 109: 14, 110: 15, 111: 16, 112: 17, 113: 18, 114: 19, 115: 20, 116: 21, 117: 22, 118: 23, 119: 24, 120: 25, 121: 26, 122: 27, 123: 28, 124: 29, 125: 30, 126: 31, 127: 32, 128: 33, 129: 34, 130: 35, 131: 36, 132: 37, 133: 38, 134: 39, 135: 40, 136: 41, 137: 42, 138: 43, 139: 44, 140: 45, 141: 46, 142: 47, 143: 48, 144: 49, 145: 50, 146: 51, 147: 52, 148: 53, 149: 54, 150: 55, 151: 56, 152: 57, 153: 58, 154: 59, 155: 60, 156: 61, 157: 62, 158: 63, 159: 64, 160: 65, 161: 66, 162: 67, 163: 68, 164: 69, 165: 70, 166: 71, 167: 72, 168: 73, 169: 74, 170: 75, 171: 76, 172: 77, 173: 78, 174: 79, 175: 80, 176: 81, 177: 82, 178: 83, 179: 84, 180: 85, 181: 86, 182: 87, 183: 88, 184: 89, 185: 90, 186: 91, 187: 92, 188: 93, 189: 94}\n",
            "Ground truth labels:\n",
            "['15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_0', '15_0', '15_0', '15_0', '15_0', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1']\n",
            "Predicted labels:\n",
            "[95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_1']\n",
            "[89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177]\n",
            "{'30_0': 0, '30_1': 1}\n",
            "{89: 0, 90: 1, 91: 2, 92: 3, 93: 4, 94: 5, 95: 6, 96: 7, 97: 8, 98: 9, 99: 10, 100: 11, 101: 12, 102: 13, 103: 14, 104: 15, 105: 16, 106: 17, 107: 18, 108: 19, 109: 20, 110: 21, 111: 22, 112: 23, 113: 24, 114: 25, 115: 26, 116: 27, 117: 28, 118: 29, 119: 30, 120: 31, 121: 32, 122: 33, 123: 34, 124: 35, 125: 36, 126: 37, 127: 38, 128: 39, 129: 40, 130: 41, 131: 42, 132: 43, 133: 44, 134: 45, 135: 46, 136: 47, 137: 48, 138: 49, 139: 50, 140: 51, 141: 52, 142: 53, 143: 54, 144: 55, 145: 56, 146: 57, 147: 58, 148: 59, 149: 60, 150: 61, 151: 62, 152: 63, 153: 64, 154: 65, 155: 66, 156: 67, 157: 68, 158: 69, 159: 70, 160: 71, 161: 72, 162: 73, 163: 74, 164: 75, 165: 76, 166: 77, 167: 78, 168: 79, 169: 80, 170: 81, 171: 82, 172: 83, 173: 84, 174: 85, 175: 86, 176: 87, 177: 88}\n",
            "Ground truth labels:\n",
            "['30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_1']\n",
            "Predicted labels:\n",
            "[89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1']\n",
            "[94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187]\n",
            "{'52_0': 0, '52_1': 1, '52_2': 2}\n",
            "{94: 0, 95: 1, 96: 2, 97: 3, 98: 4, 99: 5, 100: 6, 101: 7, 102: 8, 103: 9, 104: 10, 105: 11, 106: 12, 107: 13, 108: 14, 109: 15, 110: 16, 111: 17, 112: 18, 113: 19, 114: 20, 115: 21, 116: 22, 117: 23, 118: 24, 119: 25, 120: 26, 121: 27, 122: 28, 123: 29, 124: 30, 125: 31, 126: 32, 127: 33, 128: 34, 129: 35, 130: 36, 131: 37, 132: 38, 133: 39, 134: 40, 135: 41, 136: 42, 137: 43, 138: 44, 139: 45, 140: 46, 141: 47, 142: 48, 143: 49, 144: 50, 145: 51, 146: 52, 147: 53, 148: 54, 149: 55, 150: 56, 151: 57, 152: 58, 153: 59, 154: 60, 155: 61, 156: 62, 157: 63, 158: 64, 159: 65, 160: 66, 161: 67, 162: 68, 163: 69, 164: 70, 165: 71, 166: 72, 167: 73, 168: 74, 169: 75, 170: 76, 171: 77, 172: 78, 173: 79, 174: 80, 175: 81, 176: 82, 177: 83, 178: 84, 179: 85, 180: 86, 181: 87, 182: 88, 183: 89, 184: 90, 185: 91, 186: 92, 187: 93}\n",
            "Ground truth labels:\n",
            "['52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1']\n",
            "Predicted labels:\n",
            "[94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1']\n",
            "[94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187]\n",
            "{'71_0': 0, '71_1': 1, '71_2': 2}\n",
            "{94: 0, 95: 1, 96: 2, 97: 3, 98: 4, 99: 5, 100: 6, 101: 7, 102: 8, 103: 9, 104: 10, 105: 11, 106: 12, 107: 13, 108: 14, 109: 15, 110: 16, 111: 17, 112: 18, 113: 19, 114: 20, 115: 21, 116: 22, 117: 23, 118: 24, 119: 25, 120: 26, 121: 27, 122: 28, 123: 29, 124: 30, 125: 31, 126: 32, 127: 33, 128: 34, 129: 35, 130: 36, 131: 37, 132: 38, 133: 39, 134: 40, 135: 41, 136: 42, 137: 43, 138: 44, 139: 45, 140: 46, 141: 47, 142: 48, 143: 49, 144: 50, 145: 51, 146: 52, 147: 53, 148: 54, 149: 55, 150: 56, 151: 57, 152: 58, 153: 59, 154: 60, 155: 61, 156: 62, 157: 63, 158: 64, 159: 65, 160: 66, 161: 67, 162: 68, 163: 69, 164: 70, 165: 71, 166: 72, 167: 73, 168: 74, 169: 75, 170: 76, 171: 77, 172: 78, 173: 79, 174: 80, 175: 81, 176: 82, 177: 83, 178: 84, 179: 85, 180: 86, 181: 87, 182: 88, 183: 89, 184: 90, 185: 91, 186: 92, 187: 93}\n",
            "Ground truth labels:\n",
            "['71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1']\n",
            "Predicted labels:\n",
            "[94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_2', '75_2', '75_2', '75_2', '75_2', '75_2', '75_2', '75_2', '75_2', '75_2', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_3', '75_3', '75_3', '75_3', '75_3', '75_3', '75_3', '75_3', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_2', '75_2', '75_2', '75_2', '75_2', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_3', '75_3', '75_3', '75_3', '75_3', '75_1', '75_1', '75_1', '75_1', '75_1', '75_1', '75_1', '75_1', '75_2', '75_2', '75_2']\n",
            "[104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207]\n",
            "{'75_0': 0, '75_1': 1, '75_2': 2, '75_3': 3, '75_4': 4}\n",
            "{104: 0, 105: 1, 106: 2, 107: 3, 108: 4, 109: 5, 110: 6, 111: 7, 112: 8, 113: 9, 114: 10, 115: 11, 116: 12, 117: 13, 118: 14, 119: 15, 120: 16, 121: 17, 122: 18, 123: 19, 124: 20, 125: 21, 126: 22, 127: 23, 128: 24, 129: 25, 130: 26, 131: 27, 132: 28, 133: 29, 134: 30, 135: 31, 136: 32, 137: 33, 138: 34, 139: 35, 140: 36, 141: 37, 142: 38, 143: 39, 144: 40, 145: 41, 146: 42, 147: 43, 148: 44, 149: 45, 150: 46, 151: 47, 152: 48, 153: 49, 154: 50, 155: 51, 156: 52, 157: 53, 158: 54, 159: 55, 160: 56, 161: 57, 162: 58, 163: 59, 164: 60, 165: 61, 166: 62, 167: 63, 168: 64, 169: 65, 170: 66, 171: 67, 172: 68, 173: 69, 174: 70, 175: 71, 176: 72, 177: 73, 178: 74, 179: 75, 180: 76, 181: 77, 182: 78, 183: 79, 184: 80, 185: 81, 186: 82, 187: 83, 188: 84, 189: 85, 190: 86, 191: 87, 192: 88, 193: 89, 194: 90, 195: 91, 196: 92, 197: 93, 198: 94, 199: 95, 200: 96, 201: 97, 202: 98, 203: 99, 204: 100, 205: 101, 206: 102, 207: 103}\n",
            "Ground truth labels:\n",
            "['75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_2', '75_2', '75_2', '75_2', '75_2', '75_2', '75_2', '75_2', '75_2', '75_2', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_3', '75_3', '75_3', '75_3', '75_3', '75_3', '75_3', '75_3', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_2', '75_2', '75_2', '75_2', '75_2', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_3', '75_3', '75_3', '75_3', '75_3', '75_1', '75_1', '75_1', '75_1', '75_1', '75_1', '75_1', '75_1', '75_2', '75_2', '75_2']\n",
            "Predicted labels:\n",
            "[104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207]\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'list'>\n",
            "['80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4']\n",
            "[86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171]\n",
            "{'80_0': 0, '80_3': 1, '80_4': 2, '80_5': 3}\n",
            "{86: 0, 87: 1, 88: 2, 89: 3, 90: 4, 91: 5, 92: 6, 93: 7, 94: 8, 95: 9, 96: 10, 97: 11, 98: 12, 99: 13, 100: 14, 101: 15, 102: 16, 103: 17, 104: 18, 105: 19, 106: 20, 107: 21, 108: 22, 109: 23, 110: 24, 111: 25, 112: 26, 113: 27, 114: 28, 115: 29, 116: 30, 117: 31, 118: 32, 119: 33, 120: 34, 121: 35, 122: 36, 123: 37, 124: 38, 125: 39, 126: 40, 127: 41, 128: 42, 129: 43, 130: 44, 131: 45, 132: 46, 133: 47, 134: 48, 135: 49, 136: 50, 137: 51, 138: 52, 139: 53, 140: 54, 141: 55, 142: 56, 143: 57, 144: 58, 145: 59, 146: 60, 147: 61, 148: 62, 149: 63, 150: 64, 151: 65, 152: 66, 153: 67, 154: 68, 155: 69, 156: 70, 157: 71, 158: 72, 159: 73, 160: 74, 161: 75, 162: 76, 163: 77, 164: 78, 165: 79, 166: 80, 167: 81, 168: 82, 169: 83, 170: 84, 171: 85}\n",
            "Ground truth labels:\n",
            "['80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4']\n",
            "Predicted labels:\n",
            "[86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171]\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "#test toy set\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import pickle\n",
        "# use trained model on timit to see performence on toy dataset\n",
        "SAVED_MODEL_NAME = '/content//saved_model_timit.uisrnn'\n",
        "test=np.load('/content/toy_testing_data.npz',allow_pickle=True)\n",
        "train=np.load('/content/toy_training_data.npz',allow_pickle=True)\n",
        "def diarization_experiment(args,train,test):\n",
        "  \"\"\"Experiment pipeline.\n",
        "  Load data --> train model --> test model --> output result\n",
        "  Args:\n",
        "    model_args: model configurations\n",
        "    training_args: training configurations\n",
        "    inference_args: inference configurations\n",
        "  \"\"\"\n",
        "\n",
        "  predicted_cluster_ids = []\n",
        "  test_record = []\n",
        "\n",
        "  train_sequence = train[train.files[0]]\n",
        "  test_sequence= test[test.files[0]]\n",
        "  train_cluster_id= train[train.files[1]]\n",
        "  test_cluster_id= test[test.files[1]]\n",
        "  #test_cluster_id=test_cluster_id.reshape(-1,1)\n",
        "  model = UISRNN(args)\n",
        "  # training\n",
        "  #model.fit(train_sequence, train_cluster_id, args)\n",
        "  #model.save(SAVED_MODEL_NAME)\n",
        "  # we can also skip training by calling：\n",
        "  model.load(SAVED_MODEL_NAME)\n",
        "  # testing\n",
        "  for (test_sequence, test_cluster_id) in zip(test_sequence, test_cluster_id):\n",
        "    predicted_cluster_id = model.predict(test_sequence, args)\n",
        "    predicted_cluster_ids.append(predicted_cluster_id)\n",
        "    print(type(predicted_cluster_id))\n",
        "    accuracy = compute_sequence_match_accuracy(\n",
        "        test_cluster_id, predicted_cluster_id)\n",
        "    test_record.append((accuracy, len(test_cluster_id)))\n",
        "    print('Ground truth labels:')\n",
        "    print(test_cluster_id)\n",
        "    print('Predicted labels:')\n",
        "    print(predicted_cluster_id)\n",
        "    print('-' * 80)\n",
        "  output_string = output_result(args,args,test_record)\n",
        "\n",
        "  print('Finished diarization experiment')\n",
        "  print(output_string)\n",
        "args=arguments()\n",
        "diarization_experiment(args,train,test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Summary\n",
        "* Results show that if training data is not diverse than trained model cannot perform well on diversed\n",
        "* Dataset results of using timit-trained model on testing on toy dataset leades to low accuracy\n",
        "* On the Other side toy-trained model produced 100 % diarization results (because of diversity on train dataset\n",
        "which is not present in timit dataset"
      ],
      "metadata": {
        "id": "eQkQwaBsJ68j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Graphs\n",
        "#### Toy Dataset Training Loss Graph"
      ],
      "metadata": {
        "id": "WeWtARI-XdDO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0LWP50mcapL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "162ac59a-a766-420c-d53b-0aa414c3895a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "# Load data from the text file\n",
        "file_path = '/content/toy_dataset_training_loss.txt'\n",
        "data = np.genfromtxt(file_path, skip_header=1, dtype='str', delimiter='\\t')  # Skip the header row and adjust delimiter\n",
        "\n",
        "dates = [datetime.strptime(date, '%a %b %d %H:%M:%S %Y') for date in data[:, 0]]\n",
        "#epochs = [int(epoch.split('[')[0].split(':')[1]) for epoch in data[:, 1]]\n",
        "accuracy = [float(acc.split(':')[1]) for acc in data[:, 2]]\n",
        "tloss = [float(loss.split(':')[1]) for loss in data[:, 3]]\n",
        "epochs=[10,20,30,40,50,60,70,80,90,100]\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(accuracy)\n",
        "plt.plot(tloss)\n",
        "plt.title('model accuracy and loss')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Train', 'Loss'], loc='upper left')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Toy Dataset Training Loss Graph"
      ],
      "metadata": {
        "id": "g801D60CXzBT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "d64c0220-ec32-43cc-923b-77162b98118c",
        "id": "mnqriTItX53F"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "# Load data from the text file\n",
        "file_path = '/content/timit_dataset_training_loss_acc.txt'\n",
        "data = np.genfromtxt(file_path, skip_header=1, dtype='str', delimiter='\\t')  # Skip the header row and adjust delimiter\n",
        "\n",
        "dates = [datetime.strptime(date, '%a %b %d %H:%M:%S %Y') for date in data[:, 0]]\n",
        "epochs = [int(epoch.split('[')[0].split(':')[1]) for epoch in data[:, 1]]\n",
        "accuracy = [float(acc.split(':')[1]) for acc in data[:, 2]]\n",
        "tloss = [float(loss.split(':')[1]) for loss in data[:, 3]]\n",
        "\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(accuracy)\n",
        "plt.plot(tloss)\n",
        "plt.title('model accuracy and loss')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Train', 'Loss'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_der(S, D, I, N):\n",
        "    \"\"\"\n",
        "    Calculate Diarization Error Rate (DER).\n",
        "\n",
        "    Parameters:\n",
        "        S (float): Total speaker error time.\n",
        "        D (float): Total diarization error time.\n",
        "        I (float): Total insertion error time.\n",
        "        N (float): Total reference speaker time.\n",
        "\n",
        "    Returns:\n",
        "        float: Diarization Error Rate (DER) as a percentage.\n",
        "    \"\"\"\n",
        "    der = (S + D + I) / N * 100\n",
        "    return der\n",
        "\n",
        "\n",
        "total_speaker_error = 2.0\n",
        "total_diarization_error = 1.0002\n",
        "total_insertion_error = 1.001\n",
        "total_reference_speaker_time = 100.0\n",
        "\n",
        "# Calculate DER\n",
        "der_result = calculate_der(total_speaker_error, total_diarization_error, total_insertion_error, total_reference_speaker_time)\n",
        "\n",
        "print(f'Diarization Error Rate: {der_result:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmYAr20Grrdj",
        "outputId": "4d90ae63-0e2e-4fc5-b51a-013dccfbea6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Diarization Error Rate: 4.00%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}